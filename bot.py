#!/usr/bin/env python3
"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π Konspekt Helper Bot - –ê–≥—Ä–µ–≥–∞—Ü–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
–ë–æ—Ç —Å–≤—è–∑—ã–≤–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, —É–±–∏—Ä–∞–µ—Ç –¥—É–±–ª–∏ –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö
"""

import os
import logging
import json
import requests
from datetime import datetime
from http.server import HTTPServer, BaseHTTPRequestHandler
import threading
import random
import re
import html
from urllib.parse import urlparse
from collections import Counter, defaultdict
import hashlib
from difflib import SequenceMatcher

# ==================== –ù–ê–°–¢–†–û–ô–ö–ê ====================
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
GOOGLE_CSE_ID = os.getenv("GOOGLE_CSE_ID", "13aac457275834df9")
RENDER_EXTERNAL_URL = os.getenv("RENDER_EXTERNAL_URL", "")
PORT = int(os.getenv("PORT", 10000))

if not TELEGRAM_TOKEN:
    logger.error("‚ùå TELEGRAM_TOKEN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")
    exit(1)

# ==================== –°–¢–ê–¢–ò–°–¢–ò–ö–ê ====================
stats = {
    "total_users": 0,
    "total_messages": 0,
    "conspects_created": 0,
    "google_searches": 0,
    "reliable_sources": 0,
    "filtered_sources": 0,
    "aggregated_facts": 0,
    "duplicates_removed": 0,
    "start_time": datetime.now().isoformat(),
    "user_states": {}
}

# ==================== –°–ò–°–¢–ï–ú–ê –ê–ì–†–ï–ì–ê–¶–ò–ò –ò –§–ò–õ–¨–¢–†–ê–¶–ò–ò ====================
class InformationAggregator:
    """–ê–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –∏ —Å–≤—è–∑—ã–≤–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
    
    def __init__(self):
        self.source_checker = SourceChecker()
        
    def aggregate_information(self, items, query):
        """–ê–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
        all_facts = []
        all_definitions = []
        all_statistics = []
        sources_by_domain = defaultdict(list)
        content_hashes = set()  # –î–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        
        for item in items[:15]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–æ–ª—å—à–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            title = item.get("title", "")
            snippet = item.get("snippet", "")
            link = item.get("link", "")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            source_check = self.source_checker.check_source_quality(link, title, snippet)
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            if source_check["quality"] == "low":
                continue
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            processed_data = self._process_source_item(title, snippet, link, query, source_check)
            
            if processed_data:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —Ö–µ—à—É —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
                content_hash = self._generate_content_hash(processed_data["fact"])
                if content_hash in content_hashes:
                    stats["duplicates_removed"] += 1
                    continue
                
                content_hashes.add(content_hash)
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∞–∫—Ç—ã
                if processed_data["fact"]:
                    all_facts.append({
                        "text": processed_data["fact"],
                        "source": link,
                        "domain": urlparse(link).netloc,
                        "quality": source_check["quality"],
                        "type": self._classify_fact_type(processed_data["fact"])
                    })
                
                # –î–æ–±–∞–≤–ª—è–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
                if processed_data["definition"]:
                    all_definitions.append(processed_data["definition"])
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                all_statistics.extend(processed_data["statistics"])
                
                # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–æ –¥–æ–º–µ–Ω–∞–º
                domain = urlparse(link).netloc
                sources_by_domain[domain].append({
                    "url": link,
                    "quality": source_check["quality"]
                })
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏ —Å–≤—è–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        analyzed_info = self._analyze_and_link_facts(all_facts, query)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        result = {
            "linked_facts": analyzed_info["linked_facts"],
            "fact_clusters": analyzed_info["fact_clusters"],
            "definitions": self._merge_definitions(all_definitions)[:6],
            "statistics": self._merge_statistics(all_statistics)[:10],
            "timeline_data": analyzed_info["timeline_data"][:5],
            "comparison_data": analyzed_info["comparison_data"][:5],
            "key_entities": analyzed_info["key_entities"][:12],
            "controversial_points": analyzed_info["controversial_points"][:3],
            "source_coverage": self._calculate_source_coverage(sources_by_domain),
            "total_unique_facts": len(analyzed_info["linked_facts"]),
            "domains_used": list(sources_by_domain.keys())[:8]
        }
        
        stats["aggregated_facts"] += len(result["linked_facts"])
        return result
    
    def _process_source_item(self, title, snippet, link, query, source_check):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        full_text = f"{title}. {snippet}"
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∞–∫—Ç
        fact = self._extract_comprehensive_fact(full_text, query)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        definition = self._extract_enhanced_definition(full_text)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        statistics = self._extract_detailed_statistics(full_text)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—ã –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
        dates = self._extract_dates(full_text)
        if dates and fact:
            fact = f"{fact} ({dates[0]})"
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º–µ–Ω–∞ –∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
        entities = self._extract_entities(full_text)
        
        return {
            "fact": fact,
            "definition": definition,
            "statistics": statistics,
            "dates": dates,
            "entities": entities,
            "quality": source_check["quality"]
        }
    
    def _extract_comprehensive_fact(self, text, query):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π —Ñ–∞–∫—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
        sentences = re.split(r'[.!?]+', text)
        
        relevant_sentences = []
        for sentence in sentences:
            sentence = sentence.strip()
            if 40 < len(sentence) < 250:
                if self._is_comprehensive_sentence(sentence, query):
                    relevant_sentences.append(sentence)
        
        if not relevant_sentences:
            return None
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        if len(relevant_sentences) > 1:
            # –ù–∞—Ö–æ–¥–∏–º –Ω–∞–∏–±–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
            best_sentence = max(relevant_sentences, key=lambda s: len(s.split()))
            return best_sentence[:220]
        
        return relevant_sentences[0][:200]
    
    def _is_comprehensive_sentence(self, sentence, query):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º"""
        sentence_lower = sentence.lower()
        query_words = [w.lower() for w in query.split() if len(w) > 3]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
        relevance_score = sum(1 for word in query_words if word in sentence_lower)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
        has_specifics = bool(re.search(r'\d{4}|\d+%|\d+\.\d+', sentence))
        has_entities = bool(re.search(r'[–ê-–Ø–Å][–∞-—è—ë]+\s+[–ê-–Ø–Å][–∞-—è—ë]+', sentence))
        has_verbs = len([w for w in sentence.split() if w.endswith(('—Å—è', '—Ç—å', '–ª', '–ª–∞'))]) > 1
        
        return (relevance_score > 0 or has_specifics) and (has_verbs or has_entities)
    
    def _extract_enhanced_definition(self, text):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
        patterns = [
            r'—ç—Ç–æ\s+[^.!?]{10,150}(?:[.!?]|\s+‚Äî\s+[^.!?]{5,50})',
            r'–æ–ø—Ä–µ–¥–µ–ª[—è—é]–µ—Ç—Å—è\s+–∫–∞–∫\s+[^.!?]{10,150}[.!?]',
            r'—è–≤–ª—è–µ—Ç—Å—è\s+[^.!?]{10,150}(?:[.!?]|\s+‚Äî\s+[^.!?]{5,50})',
            r'–ø–æ–¥\s+[^.!?]{3,20}\s+–ø–æ–Ω–∏–º–∞[—é—è]—Ç\s+[^.!?]{10,150}[.!?]'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                definition = match.group(0).strip()
                if 30 < len(definition) < 180:
                    # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –µ—Å–ª–∏ –µ—Å—Ç—å
                    context_match = re.search(r'[^.!?]{10,80}\s+‚Äî\s+', definition)
                    if context_match:
                        return definition[:160] + "..."
                    else:
                        return definition[:140] + "..."
        
        return None
    
    def _extract_detailed_statistics(self, text):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        patterns = [
            # –ü—Ä–æ—Ü–µ–Ω—Ç—ã —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
            r'\d+\.?\d*%\s+(?:[^.!?]{5,40})',
            # –ë–æ–ª—å—à–∏–µ —á–∏—Å–ª–∞ —Å –ø–æ—è—Å–Ω–µ–Ω–∏–µ–º
            r'\d+[,.]?\d*\s*(?:–º–ª–Ω|–º–ª—Ä–¥|—Ç—ã—Å|–º–∏–ª–ª–∏–æ–Ω|–º–∏–ª–ª–∏–∞—Ä–¥|—Ç—ã—Å—è—á)[^.!?]{5,40}',
            # –î–µ–Ω—å–≥–∏ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
            r'\$\d+[,.]?\d*\s+(?:[^.!?]{5,30})',
            # –î–∞—Ç—ã –∏ –ø–µ—Ä–∏–æ–¥—ã
            r'\d{4}\s*(?:–≥–æ–¥[—É–∞]?|–≥\.?)\s+(?:[^.!?]{5,30})',
            # –î–∏–∞–ø–∞–∑–æ–Ω—ã
            r'–æ—Ç\s+\d+\s+–¥–æ\s+\d+\s+(?:[^.!?]{5,20})',
            # –°—Ä–∞–≤–Ω–µ–Ω–∏—è
            r'–≤\s+\d+[,.]?\d*\s+—Ä–∞–∑–∞\s+(?:[^.!?]{5,30})'
        ]
        
        statistics = []
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                if 10 < len(match) < 100:
                    statistics.append(match.strip())
        
        return list(set(statistics))[:8]
    
    def _extract_dates(self, text):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞—Ç—ã"""
        patterns = [
            r'\d{1,2}\s+[–∞-—è—ë]+\s+\d{4}',
            r'\d{4}\s+–≥–æ–¥[–∞—É]?',
            r'–≤\s+\d{4}\s+–≥–æ–¥—É',
            r'\d{1,2}\.\d{1,2}\.\d{4}'
        ]
        
        dates = []
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            dates.extend(matches)
        
        return dates[:3]
    
    def _extract_entities(self, text):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º–µ–Ω–∞ –∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏"""
        # –ò–º–µ–Ω–∞ (–§–∞–º–∏–ª–∏—è –ò–º—è)
        names = re.findall(r'[–ê-–Ø–Å][–∞-—è—ë]+\s+[–ê-–Ø–Å][–∞-—è—ë]+', text)
        
        # –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ (—Å –∑–∞–≥–ª–∞–≤–Ω—ã—Ö –±—É–∫–≤, –¥–ª–∏–Ω–Ω—ã–µ)
        orgs = re.findall(r'[–ê-–Ø–Å][–ê-–Ø–Å–∞-—è—ë]+\s+(?:—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç|–∏–Ω—Å—Ç–∏—Ç—É—Ç|–∞–∫–∞–¥–µ–º–∏—è|–∫–æ–º–ø–∞–Ω–∏—è|–∫–æ—Ä–ø–æ—Ä–∞—Ü–∏—è)', text)
        
        return names[:3] + orgs[:2]
    
    def _generate_content_hash(self, text):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ö–µ—à –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
        if not text:
            return ""
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç (—É–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã, –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É)
        normalized = re.sub(r'\s+', ' ', text.lower()).strip()
        return hashlib.md5(normalized.encode()).hexdigest()[:16]
    
    def _classify_fact_type(self, fact):
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ç–∏–ø —Ñ–∞–∫—Ç–∞"""
        fact_lower = fact.lower()
        
        if re.search(r'\d{4}|\d+\.\d+|\d+%', fact):
            return "statistical"
        elif any(word in fact_lower for word in ['–æ–±–Ω–∞—Ä—É–∂–µ–Ω', '–æ—Ç–∫—Ä—ã—Ç', '–∏–∑–æ–±—Ä–µ—Ç—ë–Ω']):
            return "discovery"
        elif any(word in fact_lower for word in ['–≤—ã–∑–≤–∞–ª', '–ø—Ä–∏–≤–µ–ª', '–ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏–µ–º']):
            return "consequence"
        elif any(word in fact_lower for word in ['—Å–æ–≥–ª–∞—Å–Ω–æ', '–ø–æ –¥–∞–Ω–Ω—ã–º', '–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ']):
            return "research"
        elif any(word in fact_lower for word in ['–≤–∞–∂–Ω', '–∑–Ω–∞—á–µ–Ω–∏', '–≤–ª–∏—è–Ω–∏']):
            return "significance"
        
        return "general"
    
    def _analyze_and_link_facts(self, facts, query):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏ —Å–≤—è–∑—ã–≤–∞–µ—Ç —Ñ–∞–∫—Ç—ã –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        if not facts:
            return {
                "linked_facts": [],
                "fact_clusters": [],
                "timeline_data": [],
                "comparison_data": [],
                "key_entities": [],
                "controversial_points": []
            }
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ñ–∞–∫—Ç—ã –ø–æ —Ç–∏–ø–∞–º –∏ —Ç–µ–º–∞–º
        fact_clusters = defaultdict(list)
        
        for fact in facts:
            fact_type = fact["type"]
            fact_clusters[fact_type].append(fact)
        
        # –°–æ–∑–¥–∞–µ–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ñ–∞–∫—Ç—ã
        linked_facts = []
        
        # 1. –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∞–∫—Ç—ã —Å —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        if "statistical" in fact_clusters:
            stats_facts = fact_clusters["statistical"]
            if len(stats_facts) >= 2:
                linked = self._link_statistical_facts(stats_facts)
                linked_facts.extend(linked)
        
        # 2. –§–∞–∫—Ç—ã –æ–± –æ—Ç–∫—Ä—ã—Ç–∏—è—Ö –∏ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏—è—Ö
        if "discovery" in fact_clusters:
            disc_facts = fact_clusters["discovery"]
            linked_facts.extend([f["text"] for f in disc_facts[:3]])
        
        # 3. –§–∞–∫—Ç—ã –æ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è—Ö
        if "consequence" in fact_clusters:
            cons_facts = fact_clusters["consequence"]
            if cons_facts:
                linked_facts.append(f"üîó –ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è: {cons_facts[0]['text']}")
        
        # 4. –û–±—â–∏–µ —Ñ–∞–∫—Ç—ã
        if "general" in fact_clusters:
            gen_facts = fact_clusters["general"]
            linked_facts.extend([f["text"] for f in gen_facts[:5]])
        
        # 5. –ï—Å–ª–∏ –º–∞–ª–æ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤, –¥–æ–±–∞–≤–ª—è–µ–º –ª—É—á—à–∏–µ –∏–∑ –≤—Å–µ—Ö
        if len(linked_facts) < 8:
            all_facts_sorted = sorted(facts, key=lambda x: len(x["text"]), reverse=True)
            additional_facts = [f["text"] for f in all_facts_sorted[:10] 
                              if f["text"] not in linked_facts]
            linked_facts.extend(additional_facts[:8-len(linked_facts)])
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
        key_entities = self._extract_key_entities_from_facts(facts)
        
        # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —Å–ø–æ—Ä–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã
        controversial = self._find_controversial_points(facts)
        
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        timeline_data = self._create_timeline_data(facts)
        
        # –î–∞–Ω–Ω—ã–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        comparison_data = self._create_comparison_data(facts)
        
        return {
            "linked_facts": linked_facts[:15],  # –ë–æ–ª—å—à–µ —Ñ–∞–∫—Ç–æ–≤
            "fact_clusters": [{k: len(v)} for k, v in fact_clusters.items()],
            "timeline_data": timeline_data,
            "comparison_data": comparison_data,
            "key_entities": key_entities,
            "controversial_points": controversial
        }
    
    def _link_statistical_facts(self, stats_facts):
        """–°–≤—è–∑—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∞–∫—Ç—ã"""
        if len(stats_facts) < 2:
            return [f["text"] for f in stats_facts]
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Å—Ö–æ–∂–∏–º —á–∏—Å–ª–∞–º
        number_groups = defaultdict(list)
        
        for fact in stats_facts:
            numbers = re.findall(r'\d+\.?\d*', fact["text"])
            for num in numbers[:2]:
                if float(num) > 1:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –º–∞–ª–µ–Ω—å–∫–∏–µ —á–∏—Å–ª–∞
                    key = f"{float(num):.1f}"
                    number_groups[key].append(fact)
        
        linked = []
        for num, facts in number_groups.items():
            if len(facts) >= 2:
                # –ù–∞—Ö–æ–¥–∏–º –æ–±—â—É—é —Ç–µ–º—É
                domains = set(f["domain"] for f in facts)
                sources_info = f"({len(facts)} –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {', '.join(list(domains)[:2])})"
                best_fact = max(facts, key=lambda f: len(f["text"]))
                linked.append(f"üìä {best_fact['text']} {sources_info}")
            else:
                linked.append(f"üìä {facts[0]['text']}")
        
        return linked[:5]
    
    def _extract_key_entities_from_facts(self, facts):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ —Ñ–∞–∫—Ç–æ–≤"""
        all_text = " ".join([f["text"] for f in facts])
        
        # –ò–º–µ–Ω–∞ –∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
        entities = re.findall(r'[–ê-–Ø–Å][–∞-—è—ë]+\s+[–ê-–Ø–Å][–∞-—è—ë]+|[–ê-–Ø–Å][–ê-–Ø–Å–∞-—è—ë]+\s+(?:—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç|–∏–Ω—Å—Ç–∏—Ç—É—Ç)', all_text)
        
        # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ —á–∞—Å—Ç–æ—Ç–µ
        entity_counter = Counter(entities)
        return [entity for entity, count in entity_counter.most_common(15)]
    
    def _find_controversial_points(self, facts):
        """–ù–∞—Ö–æ–¥–∏—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —Å–ø–æ—Ä–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã"""
        if len(facts) < 3:
            return []
        
        # –ò—â–µ–º —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è —Å –º–æ–¥–∞–ª—å–Ω—ã–º–∏ –≥–ª–∞–≥–æ–ª–∞–º–∏ –∏ –æ—Ü–µ–Ω–æ—á–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
        controversial_patterns = [
            r'–≤–µ—Ä–æ—è—Ç–Ω–æ', r'–≤–æ–∑–º–æ–∂–Ω–æ', r'–ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ',
            r'—Å–ø–æ—Ä–Ω–æ', r'–¥–∏—Å–∫—É—Å—Å–∏–æ–Ω–Ω–æ', r'–ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤–æ',
            r'–Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Å—á–∏—Ç–∞—é—Ç', r'–ø–æ –º–Ω–µ–Ω–∏—é'
        ]
        
        controversial = []
        for fact in facts:
            fact_lower = fact["text"].lower()
            for pattern in controversial_patterns:
                if re.search(pattern, fact_lower):
                    controversial.append(f"üí¨ {fact['text'][:120]}...")
                    break
        
        return controversial[:4]
    
    def _create_timeline_data(self, facts):
        """–°–æ–∑–¥–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–∏–Ω–∏–∏"""
        timeline = []
        
        for fact in facts:
            # –ò—â–µ–º –≥–æ–¥—ã –≤ —Ñ–∞–∫—Ç–∞—Ö
            years = re.findall(r'\b\d{4}\b', fact["text"])
            for year in years[:2]:
                if 1000 < int(year) < 2100:
                    # –£–ø—Ä–æ—â–∞–µ–º —Ñ–∞–∫—Ç –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–∏–Ω–∏–∏
                    clean_fact = re.sub(r'\([^)]*\)', '', fact["text"])
                    clean_fact = clean_fact[:80] + ("..." if len(clean_fact) > 80 else "")
                    timeline.append(f"{year}: {clean_fact}")
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≥–æ–¥—É
        timeline.sort(key=lambda x: int(re.search(r'\d{4}', x).group()))
        return timeline[:8]
    
    def _create_comparison_data(self, facts):
        """–°–æ–∑–¥–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        if len(facts) < 2:
            return []
        
        # –ò—â–µ–º —Ñ–∞–∫—Ç—ã —Å —á–∏—Å–ª–∞–º–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        comparison = []
        for i in range(min(3, len(facts))):
            for j in range(i+1, min(4, len(facts))):
                fact1, fact2 = facts[i], facts[j]
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–∞ –∏–∑ —Ñ–∞–∫—Ç–æ–≤
                nums1 = re.findall(r'\d+\.?\d*', fact1["text"])
                nums2 = re.findall(r'\d+\.?\d*', fact2["text"])
                
                if nums1 and nums2:
                    try:
                        num1 = float(nums1[0])
                        num2 = float(nums2[0])
                        if num1 > 0 and num2 > 0 and abs(num1 - num2) > 0.1:
                            ratio = max(num1, num2) / min(num1, num2)
                            if 1.5 < ratio < 10:  # –†–∞–∑—É–º–Ω–æ–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ
                                comparison.append(f"üìà –°—Ä–∞–≤–Ω–µ–Ω–∏–µ: {num1:.1f} vs {num2:.1f} (–≤ {ratio:.1f} —Ä–∞–∑)")
                    except ValueError:
                        continue
        
        return comparison[:4]
    
    def _merge_definitions(self, definitions):
        """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è"""
        if not definitions:
            return []
        
        # –£–±–∏—Ä–∞–µ–º –ø–æ—Ö–æ–∂–∏–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        unique_defs = []
        for def1 in definitions:
            is_duplicate = False
            for def2 in unique_defs:
                similarity = SequenceMatcher(None, def1.lower(), def2.lower()).ratio()
                if similarity > 0.7:  # –ë–æ–ª–µ–µ 70% —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                unique_defs.append(def1)
        
        return unique_defs
    
    def _merge_statistics(self, statistics):
        """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        if not statistics:
            return []
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø–∞–º
        percent_stats = [s for s in statistics if '%' in s]
        money_stats = [s for s in statistics if any(w in s.lower() for w in ['$', '–¥–æ–ª–ª–∞—Ä', '—Ä—É–±–ª', '–µ–≤—Ä–æ'])]
        other_stats = [s for s in statistics if s not in percent_stats and s not in money_stats]
        
        merged = []
        if percent_stats:
            merged.append(f"üìä –ü—Ä–æ—Ü–µ–Ω—Ç—ã: {', '.join(percent_stats[:3])}")
        if money_stats:
            merged.append(f"üí∞ –§–∏–Ω–∞–Ω—Å—ã: {', '.join(money_stats[:3])}")
        if other_stats:
            merged.extend(other_stats[:5])
        
        return merged
    
    def _calculate_source_coverage(self, sources_by_domain):
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ—Ö–≤–∞—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        total_sources = sum(len(sources) for sources in sources_by_domain.values())
        unique_domains = len(sources_by_domain)
        
        coverage = {
            "total_sources": total_sources,
            "unique_domains": unique_domains,
            "domain_distribution": {domain: len(sources) 
                                  for domain, sources in list(sources_by_domain.items())[:5]}
        }
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ
        if total_sources > 0:
            diversity_score = unique_domains / total_sources
            coverage["diversity_score"] = f"{diversity_score:.2f}"
            if diversity_score > 0.4:
                coverage["assessment"] = "‚úÖ –í—ã—Å–æ–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"
            elif diversity_score > 0.2:
                coverage["assessment"] = "‚ö†Ô∏è –°—Ä–µ–¥–Ω–µ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"
            else:
                coverage["assessment"] = "‚ùå –ù–∏–∑–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"
        
        return coverage

# ==================== –û–°–ù–û–í–ù–û–ô –ê–ù–ê–õ–ò–ó–ê–¢–û–† ====================
class InformationAnalyzer:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å –∞–≥—Ä–µ–≥–∞—Ü–∏–µ–π"""
    
    def __init__(self):
        self.aggregator = InformationAggregator()
        self.source_checker = SourceChecker()
    
    def analyze_topic(self, query, search_results):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–º—É —Å –∞–≥—Ä–µ–≥–∞—Ü–∏–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        items = search_results.get("items", [])
        
        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        aggregated_data = self.aggregator.aggregate_information(items, query)
        
        return {
            "topic": query,
            "type": self._determine_topic_type(query),
            "aggregated_data": aggregated_data,
            "timestamp": datetime.now().isoformat(),
            "quality_report": self._generate_quality_report(aggregated_data)
        }
    
    def _determine_topic_type(self, query):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø —Ç–µ–º—ã"""
        query_lower = query.lower()
        
        science_terms = ["—Ñ–∏–∑–∏–∫–∞", "—Ö–∏–º–∏—è", "–±–∏–æ–ª–æ–≥–∏—è", "–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞", "–Ω–∞—É–∫–∞", "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ"]
        tech_terms = ["—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è", "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "–∫–æ–º–ø—å—é—Ç–µ—Ä"]
        history_terms = ["–∏—Å—Ç–æ—Ä–∏—è", "–≤–æ–π–Ω–∞", "—Ä–µ–≤–æ–ª—é—Ü–∏—è", "–¥—Ä–µ–≤–Ω–∏–π", "—Å—Ä–µ–¥–Ω–µ–≤–µ–∫–æ–≤—å–µ"]
        
        if any(term in query_lower for term in science_terms):
            return "–Ω–∞—É—á–Ω–∞—è"
        elif any(term in query_lower for term in tech_terms):
            return "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è"
        elif any(term in query_lower for term in history_terms):
            return "–∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∞—è"
        
        return "–æ–±—â–∞—è"
    
    def _generate_quality_report(self, aggregated_data):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –æ –∫–∞—á–µ—Å—Ç–≤–µ"""
        coverage = aggregated_data.get("source_coverage", {})
        total_facts = aggregated_data.get("total_unique_facts", 0)
        
        report = []
        
        if total_facts > 0:
            report.append(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–∫—Ç–æ–≤: {total_facts}")
        
        if "total_sources" in coverage:
            report.append(f"üìö –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {coverage['total_sources']}")
        
        if "unique_domains" in coverage:
            report.append(f"üåê –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤: {coverage['unique_domains']}")
        
        if "assessment" in coverage:
            report.append(coverage["assessment"])
        
        if aggregated_data.get("controversial_points"):
            report.append(f"üí¨ –°–ø–æ—Ä–Ω—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤: {len(aggregated_data['controversial_points'])}")
        
        return "\n".join(report)

# ==================== –ì–ï–ù–ï–†–ê–¢–û–† –ö–û–ù–°–ü–ï–ö–¢–û–í ====================
class SmartConspectGenerator:
    def __init__(self):
        self.searcher = SmartGoogleSearch()
        logger.info("‚úÖ –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≥–æ—Ç–æ–≤ —Å –∞–≥—Ä–µ–≥–∞—Ü–∏–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏")
    
    def generate(self, topic, volume="extended"):  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω—Å–ø–µ–∫—Ç —Å –∞–≥—Ä–µ–≥–∞—Ü–∏–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        if self._is_easter_egg(topic):
            return self._create_easter_egg_response()
        
        search_results = self.searcher.search_and_analyze(topic)
        structured_info = search_results.get("structured_info", {})
        aggregated_data = structured_info.get("aggregated_data", {})
        quality_report = structured_info.get("quality_report", "")
        
        if volume == "detailed":
            return self._generate_detailed(topic, aggregated_data, quality_report)
        elif volume == "short":
            return self._generate_short(topic, aggregated_data, quality_report)
        else:
            return self._generate_extended(topic, aggregated_data, quality_report)
    
    def _is_easter_egg(self, text):
        text_lower = text.lower()
        return "–ø–∞—Å—Ö–∞–ª–∫–∞" in text_lower
    
    def _create_easter_egg_response(self):
        return "ü•ö *–ü–∞—Å—Ö–∞–ª–∫–∞ –Ω–∞–π–¥–µ–Ω–∞!* –ë–æ—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—É—é –∞–≥—Ä–µ–≥–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö."
    
    def _generate_short(self, topic, data, quality_report):
        """–ö—Ä–∞—Ç–∫–æ - –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã"""
        facts = data.get("linked_facts", [])
        
        if not facts:
            return f"üìå *{topic}*\n\n{quality_report}\n\n–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
        
        conspect = f"üìå *{topic}*\n\n{quality_report}\n\n"
        
        # –õ—É—á—à–∏–µ —Ñ–∞–∫—Ç—ã
        for i, fact in enumerate(facts[:6], 1):
            conspect += f"{i}. {fact}\n"
        
        # –ö–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        entities = data.get("key_entities", [])
        if entities:
            conspect += f"\nüîë *–ö–ª—é—á–µ–≤—ã–µ –ø–æ–Ω—è—Ç–∏—è:* {', '.join(entities[:4])}\n"
        
        return conspect
    
    def _generate_detailed(self, topic, data, quality_report):
        """–ü–æ–¥—Ä–æ–±–Ω–æ - —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"""
        conspect = f"üìö *{topic}*\n\n{quality_report}\n\n"
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∞–∫—Ç—ã
        facts = data.get("linked_facts", [])
        if facts:
            conspect += "üéØ *–û—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∞–∫—Ç—ã:*\n\n"
            for i, fact in enumerate(facts[:10], 1):
                conspect += f"{i}. {fact}\n"
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        definitions = data.get("definitions", [])
        if definitions:
            conspect += f"\nüìñ *–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è:*\n\n"
            for definition in definitions[:4]:
                conspect += f"‚Ä¢ {definition}\n"
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        statistics = data.get("statistics", [])
        if statistics:
            conspect += f"\nüìä *–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:*\n\n"
            for stat in statistics[:6]:
                conspect += f"‚Ä¢ {stat}\n"
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        timeline = data.get("timeline_data", [])
        if timeline:
            conspect += f"\nüïí *–•—Ä–æ–Ω–æ–ª–æ–≥–∏—è:*\n\n"
            for event in timeline[:4]:
                conspect += f"‚Ä¢ {event}\n"
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö
        coverage = data.get("source_coverage", {})
        if "total_sources" in coverage:
            conspect += f"\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
            conspect += f"üìö –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {coverage['total_sources']} | "
            conspect += f"üåê –î–æ–º–µ–Ω–æ–≤: {coverage.get('unique_domains', 0)}"
        
        return conspect
    
    def _generate_extended(self, topic, data, quality_report):
        """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ - –≤—Å—è –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"""
        conspect = f"üî¨ *–ü–û–õ–ù–´–ô –ê–ù–ê–õ–ò–ó: {topic}*\n\n{quality_report}\n\n"
        
        # –í–í–ï–î–ï–ù–ò–ï
        conspect += "="*50 + "\n"
        conspect += "–í–í–ï–î–ï–ù–ò–ï –ò –ú–ï–¢–û–î–û–õ–û–ì–ò–Ø\n"
        conspect += "="*50 + "\n\n"
        
        coverage = data.get("source_coverage", {})
        conspect += f"*–ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è:* –ê–≥—Ä–µ–≥–∞—Ü–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ {coverage.get('total_sources', 0)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤\n"
        conspect += f"*–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–∫—Ç–æ–≤:* {data.get('total_unique_facts', 0)}\n"
        conspect += f"*–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤:* {stats.get('duplicates_removed', 0)}\n"
        conspect += f"*–í—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞:* {datetime.now().strftime('%H:%M')}\n\n"
        
        # –û–°–ù–û–í–ù–´–ï –§–ê–ö–¢–´
        conspect += "="*50 + "\n"
        conspect += "–û–°–ù–û–í–ù–´–ï –§–ê–ö–¢–´ –ò –î–ê–ù–ù–´–ï\n"
        conspect += "="*50 + "\n\n"
        
        facts = data.get("linked_facts", [])
        if facts:
            for i, fact in enumerate(facts[:15], 1):
                conspect += f"{i}. {fact}\n\n"
        else:
            conspect += "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ —Ç–µ–º–µ —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è\n\n"
        
        # –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ò –¶–ò–§–†–´
        statistics = data.get("statistics", [])
        if statistics:
            conspect += "="*50 + "\n"
            conspect += "–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ò –ß–ò–°–õ–û–í–´–ï –î–ê–ù–ù–´–ï\n"
            conspect += "="*50 + "\n\n"
            
            for stat in statistics:
                conspect += f"‚Ä¢ {stat}\n"
            conspect += "\n"
        
        # –û–ü–†–ï–î–ï–õ–ï–ù–ò–Ø –ò –ü–û–ù–Ø–¢–ò–Ø
        definitions = data.get("definitions", [])
        if definitions:
            conspect += "="*50 + "\n"
            conspect += "–û–ü–†–ï–î–ï–õ–ï–ù–ò–Ø –ò –ö–õ–Æ–ß–ï–í–´–ï –ü–û–ù–Ø–¢–ò–Ø\n"
            conspect += "="*50 + "\n\n"
            
            for i, definition in enumerate(definitions, 1):
                conspect += f"{i}. {definition}\n\n"
        
        # –•–†–û–ù–û–õ–û–ì–ò–Ø
        timeline = data.get("timeline_data", [])
        if timeline:
            conspect += "="*50 + "\n"
            conspect += "–•–†–û–ù–û–õ–û–ì–ò–ß–ï–°–ö–ò–ï –î–ê–ù–ù–´–ï\n"
            conspect += "="*50 + "\n\n"
            
            for event in timeline:
                conspect += f"‚Ä¢ {event}\n"
            conspect += "\n"
        
        # –ö–õ–Æ–ß–ï–í–´–ï –°–£–©–ù–û–°–¢–ò
        entities = data.get("key_entities", [])
        if entities:
            conspect += "="*50 + "\n"
            conspect += "–ö–õ–Æ–ß–ï–í–´–ï –°–£–©–ù–û–°–¢–ò –ò –û–†–ì–ê–ù–ò–ó–ê–¶–ò–ò\n"
            conspect += "="*50 + "\n\n"
            
            for i, entity in enumerate(entities[:12], 1):
                conspect += f"{i}. {entity}\n"
            conspect += "\n"
        
        # –°–†–ê–í–ù–ï–ù–ò–Ø –ò –ê–ù–ê–õ–ò–ó
        comparison = data.get("comparison_data", [])
        if comparison:
            conspect += "="*50 + "\n"
            conspect += "–°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó\n"
            conspect += "="*50 + "\n\n"
            
            for comp in comparison:
                conspect += f"‚Ä¢ {comp}\n"
            conspect += "\n"
        
        # –°–ü–û–†–ù–´–ï –ú–û–ú–ï–ù–¢–´
        controversial = data.get("controversial_points", [])
        if controversial:
            conspect += "="*50 + "\n"
            conspect += "–°–ü–û–†–ù–´–ï –ò –î–ò–°–ö–£–°–°–ò–û–ù–ù–´–ï –ú–û–ú–ï–ù–¢–´\n"
            conspect += "="*50 + "\n\n"
            
            for point in controversial:
                conspect += f"‚Ä¢ {point}\n"
            conspect += "\n"
        
        # –ê–ù–ê–õ–ò–ó –ò–°–¢–û–ß–ù–ò–ö–û–í
        conspect += "="*50 + "\n"
        conspect += "–ê–ù–ê–õ–ò–ó –ò–°–¢–û–ß–ù–ò–ö–û–í –ò –î–û–°–¢–û–í–ï–†–ù–û–°–¢–ò\n"
        conspect += "="*50 + "\n\n"
        
        conspect += f"*–í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:* {coverage.get('total_sources', 0)}\n"
        conspect += f"*–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤:* {coverage.get('unique_domains', 0)}\n"
        conspect += f"*–°–≤—è–∑–µ–π –º–µ–∂–¥—É —Ñ–∞–∫—Ç–∞–º–∏:* {len(facts)}\n"
        conspect += f"*–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤:* {stats.get('duplicates_removed', 0)}\n\n"
        
        if "domain_distribution" in coverage:
            conspect += "*–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –¥–æ–º–µ–Ω–∞–º:*\n"
            for domain, count in coverage["domain_distribution"].items():
                conspect += f"‚Ä¢ {domain}: {count} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤\n"
        
        # –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï
        conspect += "\n" + "="*50 + "\n"
        conspect += "–ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï –ò –í–´–í–û–î–´\n"
        conspect += "="*50 + "\n\n"
        
        total_facts = data.get("total_unique_facts", 0)
        if total_facts >= 10:
            conspect += "‚úÖ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Ö–æ—Ä–æ—à–æ –æ—Å–≤–µ—â–µ–Ω–∞ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö\n"
            conspect += "‚úÖ –ù–∞–π–¥–µ–Ω—ã —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ñ–∞–∫—Ç—ã\n"
            conspect += "‚úÖ –í—ã—è–≤–ª–µ–Ω—ã –∫–ª—é—á–µ–≤—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ –∏ —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—è\n"
        elif total_facts >= 5:
            conspect += "‚ö†Ô∏è –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ\n"
            conspect += "‚ö†Ô∏è –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º\n"
        else:
            conspect += "‚ùå –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞\n"
            conspect += "‚ùå –¢—Ä–µ–±—É—é—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è\n"
        
        conspect += f"\nü§ñ *@Konspekt_help_bot* | üß† *–ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö* | üïí {datetime.now().strftime('%d.%m.%Y')}"
        
        return conspect

# ==================== –û–°–¢–ê–õ–¨–ù–û–ô –ö–û–î (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ====================
# [SourceChecker, SmartGoogleSearch, TelegramBot, BotHTTPServer –æ—Å—Ç–∞—é—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π]
# –ü—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤—å—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥ –ø–æ—Å–ª–µ –∫–ª–∞—Å—Å–∞ InformationAggregator

# ==================== –ó–ê–ü–£–°–ö ====================
def main():
    logger.info("=" * 60)
    logger.info("üöÄ –ó–ê–ü–£–°–ö –ë–û–¢–ê –° –ê–ì–†–ï–ì–ê–¶–ò–ï–ô –ò–ù–§–û–†–ú–ê–¶–ò–ò")
    logger.info("=" * 60)
    logger.info(f"‚úÖ –†–µ–∂–∏–º: –ê–≥—Ä–µ–≥–∞—Ü–∏—è –∏–∑ 15+ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
    logger.info(f"‚úÖ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
    logger.info(f"‚úÖ –°–≤—è–∑—ã–≤–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏")
    logger.info(f"‚úÖ –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π –æ–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö")
    logger.info("=" * 60)
    
    server = HTTPServer(('', PORT), BotHTTPServer)
    logger.info(f"‚úÖ HTTP —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω –Ω–∞ –ø–æ—Ä—Ç—É {PORT}")
    
    try:
        server.serve_forever()
    except KeyboardInterrupt:
        logger.info("‚èπÔ∏è –°–µ—Ä–≤–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    main()
