#!/usr/bin/env python3
"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π Konspekt Helper Bot —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
–ë–æ—Ç –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –Ω–µ–Ω–∞—É—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
"""

import os
import logging
import json
import requests
from datetime import datetime
from http.server import HTTPServer, BaseHTTPRequestHandler
import threading
import random
import re
import html
from urllib.parse import urlparse
from collections import Counter

# ==================== –ù–ê–°–¢–†–û–ô–ö–ê ====================
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
GOOGLE_CSE_ID = os.getenv("GOOGLE_CSE_ID", "13aac457275834df9")
RENDER_EXTERNAL_URL = os.getenv("RENDER_EXTERNAL_URL", "")
PORT = int(os.getenv("PORT", 10000))

if not TELEGRAM_TOKEN:
    logger.error("‚ùå TELEGRAM_TOKEN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")
    exit(1)

# ==================== –°–¢–ê–¢–ò–°–¢–ò–ö–ê ====================
stats = {
    "total_users": 0,
    "total_messages": 0,
    "conspects_created": 0,
    "google_searches": 0,
    "reliable_sources": 0,
    "filtered_sources": 0,
    "start_time": datetime.now().isoformat(),
    "user_states": {}
}

# ==================== –°–ò–°–¢–ï–ú–ê –ü–†–û–í–ï–†–ö–ò –ò–°–¢–û–ß–ù–ò–ö–û–í ====================
class SourceChecker:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
    
    # –ù–∞–¥–µ–∂–Ω—ã–µ –¥–æ–º–µ–Ω—ã (–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ, –Ω–∞—É–∫–∞, –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏)
    RELIABLE_DOMAINS = [
        '.edu', '.ac.', '.gov', '.org', 
        'wikipedia.org', 'arxiv.org', 'sciencedirect.com',
        'nature.com', 'sciencemag.org', 'researchgate.net',
        'springer.com', 'ieee.org', 'ncbi.nlm.nih.gov',
        'who.int', 'unesco.org', 'bbc.com', 'reuters.com',
        'theguardian.com', 'nytimes.com', 'meduza.io'
    ]
    
    # –ù–µ–Ω–∞–¥–µ–∂–Ω—ã–µ –¥–æ–º–µ–Ω—ã (–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç, —Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏—è)
    UNRELIABLE_DOMAINS = [
        'reddit.com', '4chan.org', 'tiktok.com', 
        'twitter.com', 'x.com', 'instagram.com',
        'facebook.com', 'pikabu.ru', 'vk.com',
        'livejournal.com', '9gag.com', 'buzzfeed.com'
    ]
    
    # –°–ª–æ–≤–∞-–º–∞—Ä–∫–µ—Ä—ã –Ω–µ–Ω–∞—É—á–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    PSEUDOSCIENCE_KEYWORDS = [
        '–ª–∂–µ–Ω–∞—É–∫–∞', '–ø—Å–µ–≤–¥–æ–Ω–∞—É–∫–∞', '–∫–æ–Ω—Å–ø–∏—Ä–æ–ª–æ–≥–∏—è', '—Ç–µ–æ—Ä–∏—è –∑–∞–≥–æ–≤–æ—Ä–∞',
        '—á—É–¥–µ—Å–Ω–æ–µ –∏—Å—Ü–µ–ª–µ–Ω–∏–µ', '–º–∞–≥–∏—á–µ—Å–∫–∞—è —Å–∏–ª–∞', '—ç–∫—Å—Ç—Ä–∞—Å–µ–Ω—Å', '—è—Å–Ω–æ–≤–∏–¥—è—â–∏–π',
        '–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –º–µ–¥–∏—Ü–∏–Ω–∞', '–±–∏–æ—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞', '—Ç–æ—Ä—Å–∏–æ–Ω–Ω—ã–µ –ø–æ–ª—è',
        '—Ö–æ–ª–æ–¥–Ω—ã–π —è–¥–µ—Ä–Ω—ã–π —Å–∏–Ω—Ç–µ–∑', '–≤–µ—á–Ω—ã–π –¥–≤–∏–≥–∞—Ç–µ–ª—å', '–ø–∞–º—è—Ç—å –≤–æ–¥—ã'
    ]
    
    # –ü—Ä–∏–∑–Ω–∞–∫–∏ –Ω–µ–Ω–∞–¥–µ–∂–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    UNRELIABLE_PATTERNS = [
        r'—à–æ–∫[!.]?', r'—Å–µ–Ω—Å–∞—Ü[–∏—è][!.]?', r'–≤—ã –Ω–µ –ø–æ–≤–µ—Ä–∏—Ç–µ', r'–≤—Å–µ–º –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ',
        r'—É—á[—ë–µ]–Ω—ã–µ —Å–∫—Ä—ã–≤–∞—é—Ç', r'–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –º–æ–ª—á–∏—Ç', r'100% –¥–æ–∫–∞–∑–∞–Ω–æ',
        r'–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ –æ–ø—Ä–æ–≤–µ—Ä–≥–Ω—É—Ç–æ', r'—ç—Ç–æ —Å–∫—Ä—ã–≤–∞—é—Ç', r'—Ç–∞–π–Ω–æ–µ –∑–Ω–∞–Ω–∏–µ',
        r'—Å–µ–∫—Ä–µ—Ç–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã', r'–∑–∞–ø—Ä–µ—â[–µ—ë]–Ω–Ω–∞—è –ø—Ä–∞–≤–¥–∞'
    ]
    
    # –ü—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞—É—á–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    SCIENTIFIC_PATTERNS = [
        r'–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏[–µ—è] –ø–æ–∫–∞–∑–∞–ª[–∏–æ]', r'—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç[—ã]? –ø–æ–¥—Ç–≤–µ—Ä–¥–∏–ª[–∏]',
        r'–ø–æ –¥–∞–Ω–Ω—ã–º', r'—Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º', r'–º–µ—Ç–∞-–∞–Ω–∞–ª–∏–∑',
        r'—Ä–µ—Ü–µ–Ω–∑–∏—Ä—É–µ–º–æ–µ –∏–∑–¥–∞–Ω–∏–µ', r'–∫–ª–∏–Ω–∏—á–µ—Å–∫–æ–µ –∏—Å–ø—ã—Ç–∞–Ω–∏–µ',
        r'–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ', r'–¥–≤–æ–π–Ω–æ–π —Å–ª–µ–ø–æ–π –º–µ—Ç–æ–¥'
    ]
    
    def check_source_quality(self, url, title, snippet):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º"""
        score = 0
        reasons = []
        
        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ–º–µ–Ω–∞
        domain_quality = self._check_domain(url)
        if domain_quality == "reliable":
            score += 3
            reasons.append("‚úÖ –ù–∞–¥–µ–∂–Ω—ã–π –¥–æ–º–µ–Ω")
        elif domain_quality == "unreliable":
            score -= 2
            reasons.append("‚ö†Ô∏è –ù–µ–Ω–∞–¥–µ–∂–Ω—ã–π –¥–æ–º–µ–Ω")
        
        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –Ω–∞ —Å–µ–Ω—Å–∞—Ü–∏–æ–Ω–Ω–æ—Å—Ç—å
        title_score = self._check_sensationalism(title)
        score += title_score
        if title_score < 0:
            reasons.append("‚ö†Ô∏è –°–µ–Ω—Å–∞—Ü–∏–æ–Ω–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫")
        
        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –Ω–∞ –Ω–∞—É—á–Ω–æ—Å—Ç—å
        content_score = self._check_content_quality(snippet)
        score += content_score
        if content_score > 0:
            reasons.append("‚úÖ –ù–∞—É—á–Ω—ã–π —Å—Ç–∏–ª—å")
        
        # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Å–µ–≤–¥–æ–Ω–∞—É–∫—É
        if self._check_pseudoscience(title + " " + snippet):
            score -= 3
            reasons.append("‚ùå –ü—Ä–∏–∑–Ω–∞–∫–∏ –ø—Å–µ–≤–¥–æ–Ω–∞—É–∫–∏")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏
        if score >= 3:
            quality = "high"
        elif score >= 0:
            quality = "medium"
        else:
            quality = "low"
        
        return {
            "quality": quality,
            "score": score,
            "reasons": reasons,
            "domain": urlparse(url).netloc if url else "unknown"
        }
    
    def _check_domain(self, url):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ–º–µ–Ω –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        if not url:
            return "neutral"
        
        url_lower = url.lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–¥–µ–∂–Ω—ã–µ –¥–æ–º–µ–Ω—ã
        for domain in self.RELIABLE_DOMAINS:
            if domain in url_lower:
                return "reliable"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ–Ω–∞–¥–µ–∂–Ω—ã–µ –¥–æ–º–µ–Ω—ã
        for domain in self.UNRELIABLE_DOMAINS:
            if domain in url_lower:
                return "unreliable"
        
        return "neutral"
    
    def _check_sensationalism(self, text):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Å–µ–Ω—Å–∞—Ü–∏–æ–Ω–Ω–æ—Å—Ç—å"""
        if not text:
            return 0
        
        text_lower = text.lower()
        
        # –°—á–µ—Ç—á–∏–∫ —Å–µ–Ω—Å–∞—Ü–∏–æ–Ω–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤
        sensational_count = 0
        for pattern in self.UNRELIABLE_PATTERNS:
            if re.search(pattern, text_lower):
                sensational_count += 1
        
        if sensational_count >= 2:
            return -2  # –û—á–µ–Ω—å —Å–µ–Ω—Å–∞—Ü–∏–æ–Ω–Ω—ã–π
        elif sensational_count == 1:
            return -1  # –ù–µ–º–Ω–æ–≥–æ —Å–µ–Ω—Å–∞—Ü–∏–æ–Ω–Ω—ã–π
        
        return 0  # –ù–æ—Ä–º–∞–ª—å–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
    
    def _check_content_quality(self, text):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è"""
        if not text:
            return 0
        
        text_lower = text.lower()
        
        # –°—á–µ—Ç—á–∏–∫ –Ω–∞—É—á–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤
        scientific_count = 0
        for pattern in self.SCIENTIFIC_PATTERNS:
            if re.search(pattern, text_lower):
                scientific_count += 1
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ü–∏—Ñ—Ä –∏ –¥–∞–Ω–Ω—ã—Ö
        has_numbers = bool(re.search(r'\d+[%‚Ä∞¬∞]|\d+\.\d+', text))
        has_references = bool(re.search(r'–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏[–µ—è]|—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç|–¥–∞–Ω–Ω—ã–µ', text_lower))
        
        score = 0
        if scientific_count >= 2:
            score += 2
        elif scientific_count == 1:
            score += 1
        
        if has_numbers:
            score += 1
        if has_references:
            score += 1
        
        return score
    
    def _check_pseudoscience(self, text):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø—Å–µ–≤–¥–æ–Ω–∞—É–∫–∏"""
        text_lower = text.lower()
        
        for keyword in self.PSEUDOSCIENCE_KEYWORDS:
            if keyword in text_lower:
                return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∑–∞–≥–æ–≤–æ—Ä–æ–≤
        conspiracy_patterns = [
            r'—Ç–∞–π–Ω–æ–µ –ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ', r'–º–∏—Ä–æ–≤–∞—è –∑–∞–∫—É–ª–∏—Å–∞',
            r'—Å–∫—Ä—ã–≤–∞[—é]?—Ç –ø—Ä–∞–≤–¥—É', r'–Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ',
            r'–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –Ω–∞—É–∫–∞ –æ—à–∏–±–∞–µ—Ç—Å—è'
        ]
        
        for pattern in conspiracy_patterns:
            if re.search(pattern, text_lower):
                return True
        
        return False
    
    def filter_content(self, text):
        """–§–∏–ª—å—Ç—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç, —É–¥–∞–ª—è—è –Ω–µ–Ω–∞–¥–µ–∂–Ω—ã–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è"""
        if not text:
            return text
        
        sentences = re.split(r'[.!?]+', text)
        filtered_sentences = []
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –Ω–∞ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å
            if self._is_reliable_sentence(sentence):
                filtered_sentences.append(sentence)
            else:
                logger.debug(f"–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: {sentence[:50]}...")
                stats["filtered_sources"] += 1
        
        return ". ".join(filtered_sentences) + ("." if filtered_sentences else "")
    
    def _is_reliable_sentence(self, sentence):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω—ã–º"""
        if len(sentence) < 10:
            return False
        
        sentence_lower = sentence.lower()
        
        # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Å–µ–Ω—Å–∞—Ü–∏–æ–Ω–Ω–æ—Å—Ç—å
        for pattern in self.UNRELIABLE_PATTERNS:
            if re.search(pattern, sentence_lower):
                return False
        
        # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø—Å–µ–≤–¥–æ–Ω–∞—É–∫—É
        if self._check_pseudoscience(sentence):
            return False
        
        # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∏–∑–ª–∏—à–Ω—é—é –∫–∞—Ç–µ–≥–æ—Ä–∏—á–Ω–æ—Å—Ç—å –±–µ–∑ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤
        categorical_without_proof = [
            r'—Ç–æ—á–Ω–æ –∏–∑–≤–µ—Å—Ç–Ω–æ', r'–Ω–µ—Å–æ–º–Ω–µ–Ω–Ω–æ', r'–∞–±—Å–æ–ª—é—Ç–Ω–æ —Ç–æ—á–Ω–æ',
            r'–¥–æ–∫–∞–∑–∞–Ω–æ —Ä–∞–∑ –∏ –Ω–∞–≤—Å–µ–≥–¥–∞', r'—ç—Ç–æ —Ñ–∞–∫—Ç'
        ]
        
        for pattern in categorical_without_proof:
            if re.search(pattern, sentence_lower):
                # –†–∞–∑—Ä–µ—à–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
                if not re.search(r'–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏[–µ—è]|—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç|–¥–∞–Ω–Ω—ã–µ', sentence_lower):
                    return False
        
        # 4. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫–∏
        has_specifics = bool(re.search(r'\d{4}|\d+[%]|\d+\.\d+', sentence))
        has_clear_subject = len(sentence.split()) > 5
        
        return has_clear_subject and (has_specifics or len(sentence) > 30)

# ==================== –£–õ–£–ß–®–ï–ù–ù–´–ô –ê–ù–ê–õ–ò–ó–ê–¢–û–† ====================
class InformationAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏"""
    
    def __init__(self):
        self.source_checker = SourceChecker()
        self.cache = {}
    
    def analyze_topic(self, query, search_results):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–º—É —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ç–µ–º—ã
        topic_type = self._determine_topic_type(query)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞
        analysis = self._analyze_with_quality_check(search_results, query)
        
        return {
            "topic": query,
            "type": topic_type,
            "analysis": analysis,
            "timestamp": datetime.now().isoformat(),
            "source_quality_report": self._generate_quality_report(analysis)
        }
    
    def _determine_topic_type(self, query):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø —Ç–µ–º—ã"""
        query_lower = query.lower()
        
        if any(word in query_lower for word in ["–∏—Å—Ç–æ—Ä–∏—è", "–≤–æ–π–Ω–∞", "—Ä–µ–≤–æ–ª—é—Ü–∏—è"]):
            return "–∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∞—è"
        elif any(word in query_lower for word in ["—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è", "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç"]):
            return "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è"
        elif any(word in query_lower for word in ["–º–µ–¥–∏—Ü–∏–Ω–∞", "–∑–¥–æ—Ä–æ–≤—å–µ", "–±–æ–ª–µ–∑–Ω—å"]):
            return "–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è"
        elif any(word in query_lower for word in ["—ç–∫–æ–Ω–æ–º–∏–∫–∞", "—Ñ–∏–Ω–∞–Ω—Å—ã", "—Ä—ã–Ω–æ–∫"]):
            return "—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∞—è"
        elif any(word in query_lower for word in ["—Ñ–∏–∑–∏–∫–∞", "—Ö–∏–º–∏—è", "–±–∏–æ–ª–æ–≥–∏—è", "–Ω–∞—É–∫–∞"]):
            return "–Ω–∞—É—á–Ω–∞—è"
        return "–æ–±—â–∞—è"
    
    def _analyze_with_quality_check(self, results, query):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞"""
        items = results.get("items", [])
        
        reliable_points = []
        questionable_points = []
        statistics = []
        definitions = []
        sources_checked = []
        
        for item in items[:10]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–æ–ª—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            title = item.get("title", "")
            snippet = item.get("snippet", "")
            link = item.get("link", "")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            source_check = self.source_checker.check_source_quality(link, title, snippet)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
            filtered_text = self.source_checker.filter_content(f"{title}. {snippet}")
            if not filtered_text:
                continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∞–∫—Ç—ã —Ç–æ–ª—å–∫–æ –∏–∑ –Ω–∞–¥–µ–∂–Ω—ã—Ö —á–∞—Å—Ç–µ–π
            fact = self._extract_reliable_fact(filtered_text, query, source_check["quality"])
            if fact:
                if source_check["quality"] == "high":
                    reliable_points.append({
                        "text": fact,
                        "source": link,
                        "quality": "high",
                        "reasons": source_check["reasons"]
                    })
                    stats["reliable_sources"] += 1
                elif source_check["quality"] == "medium":
                    reliable_points.append({
                        "text": fact,
                        "source": link,
                        "quality": "medium",
                        "reasons": source_check["reasons"]
                    })
                else:
                    questionable_points.append({
                        "text": f"‚ö†Ô∏è {fact}",
                        "source": link,
                        "quality": "low",
                        "reasons": source_check["reasons"]
                    })
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Ç–æ–ª—å–∫–æ –∏–∑ –Ω–∞–¥–µ–∂–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            if source_check["quality"] in ["high", "medium"]:
                numbers = self._extract_numbers(filtered_text)
                statistics.extend(numbers)
                
                definition = self._extract_definition(filtered_text)
                if definition:
                    definitions.append(definition)
            
            sources_checked.append({
                "url": link,
                "quality": source_check["quality"],
                "score": source_check["score"],
                "domain": source_check["domain"]
            })
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–æ—á–∫–∏ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
        reliable_points.sort(key=lambda x: 0 if x["quality"] == "high" else 1)
        all_points = [p["text"] for p in reliable_points] + [p["text"] for p in questionable_points]
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ—Ä–º–∏–Ω—ã —Ç–æ–ª—å–∫–æ –∏–∑ –Ω–∞–¥–µ–∂–Ω—ã—Ö —Ç–æ—á–µ–∫
        key_terms = self._extract_terms_from_reliable_points([p["text"] for p in reliable_points])
        
        # –ù–∞—Ö–æ–¥–∏–º –∫–æ–Ω—Å–µ–Ω—Å—É—Å–Ω—ã–µ —Ñ–∞–∫—Ç—ã
        consensus_terms = self._find_consensus_facts([p["text"] for p in reliable_points])
        
        return {
            "reliable_points": [p["text"] for p in reliable_points[:8]],
            "questionable_points": [p["text"] for p in questionable_points[:3]],
            "statistics": statistics[:6],
            "definitions": definitions[:4],
            "key_terms": key_terms[:8],
            "consensus_terms": consensus_terms[:5],
            "total_sources": len(items),
            "reliable_sources_count": len([s for s in sources_checked if s["quality"] in ["high", "medium"]]),
            "sources_quality": sources_checked[:5],
            "all_points": all_points[:10]
        }
    
    def _extract_reliable_fact(self, text, query, quality):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ñ–∞–∫—Ç —Å —É—á–µ—Ç–æ–º –∫–∞—á–µ—Å—Ç–≤–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        sentences = re.split(r'[.!?]+', text)
        
        for sentence in sentences:
            sentence = sentence.strip()
            if 30 < len(sentence) < 200:
                if self._is_relevant_fact(sentence, query):
                    # –î–ª—è –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Ç—Ä–µ–±—É–µ–º –±–æ–ª—å—à–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤
                    if quality == "low":
                        if re.search(r'\d{4}|\d+%|–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏[–µ—è]', sentence.lower()):
                            return sentence[:180]
                    else:
                        return sentence[:180]
        
        return None
    
    def _is_relevant_fact(self, sentence, query):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Ñ–∞–∫—Ç–∞"""
        query_words = [word.lower() for word in query.split() if len(word) > 3]
        sentence_lower = sentence.lower()
        
        matches = sum(1 for word in query_words if word in sentence_lower)
        return matches > 0 and len(sentence.split()) > 5
    
    def _extract_numbers(self, text):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —á–∏—Å–ª–∞ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        patterns = [
            r'\d+\.?\d*%',  # –ü—Ä–æ—Ü–µ–Ω—Ç—ã
            r'\d+\.?\d*\s*(?:–º–ª–Ω|–º–ª—Ä–¥|—Ç—ã—Å|–º–∏–ª–ª–∏–æ–Ω|–º–∏–ª–ª–∏–∞—Ä–¥|—Ç—ã—Å—è—á)',  # –ë–æ–ª—å—à–∏–µ —á–∏—Å–ª–∞
            r'\$\d+\.?\d*',  # –î–æ–ª–ª–∞—Ä—ã
            r'\d+\.?\d*\s*(?:–¥–æ–ª–ª–∞—Ä–æ–≤|—Ä—É–±–ª–µ–π|–µ–≤—Ä–æ|–¥–æ–ª–ª\.|—Ä—É–±\.)',
            r'\d{4}\s*–≥–æ–¥—É?',  # –ì–æ–¥–∞
            r'\d+\.?\d*\s*(?:–ª–µ—Ç|–≥–æ–¥|–º–µ—Å—è—Ü|–¥–µ–Ω—å)'  # –ü–µ—Ä–∏–æ–¥—ã
        ]
        
        numbers = []
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            numbers.extend(matches)
        
        return list(set(numbers))[:8]
    
    def _extract_definition(self, text):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è"""
        patterns = [
            r'—ç—Ç–æ\s+[^.!?]{10,120}[.!?]',
            r'—è–≤–ª—è–µ—Ç—Å—è\s+[^.!?]{10,120}[.!?]',
            r'–æ–ø—Ä–µ–¥–µ–ª[—è—é]–µ—Ç—Å—è\s+–∫–∞–∫\s+[^.!?]{10,120}[.!?]',
            r'–ø–æ–¥\s+[^.!?]{5,20}\s+–ø–æ–Ω–∏–º–∞[—é—è]—Ç\s+[^.!?]{10,120}[.!?]'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                definition = match.group(0).strip()
                if 30 < len(definition) < 150:
                    return definition[:120] + "..."
        
        return None
    
    def _extract_terms_from_reliable_points(self, points):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ –Ω–∞–¥–µ–∂–Ω—ã—Ö —Ç–æ—á–µ–∫"""
        all_text = " ".join(points)
        words = re.findall(r'\b[–∞-—è—ëa-z]{4,}\b', all_text.lower())
        
        stop_words = {"—ç—Ç–æ—Ç", "—Ç–∞–∫–æ–π", "–∫–∞–∫–æ–π", "–∫–æ—Ç–æ—Ä—ã–π", "–æ—á–µ–Ω—å", "–º–æ–∂–µ—Ç", "–±—É–¥–µ—Ç"}
        freq = {}
        
        for word in words:
            if word not in stop_words and len(word) > 3:
                freq[word] = freq.get(word, 0) + 1
        
        sorted_terms = sorted(freq.items(), key=lambda x: x[1], reverse=True)
        return [term.capitalize() for term, count in sorted_terms[:15]]
    
    def _find_consensus_facts(self, points):
        """–ù–∞—Ö–æ–¥–∏—Ç —Ñ–∞–∫—Ç—ã, —É–ø–æ–º–∏–Ω–∞–µ–º—ã–µ –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö"""
        if not points:
            return []
        
        # –£–ø—Ä–æ—â–∞–µ–º —Ç–æ—á–∫–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        simplified = []
        for point in points:
            clean = re.sub(r'[^\w\s]', '', point.lower())
            words = clean.split()
            keywords = [w for w in words if len(w) > 4][:6]
            simplified.append(" ".join(keywords))
        
        # –°—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—É —Å–ª–æ–≤
        word_counter = Counter()
        for point in simplified:
            word_counter.update(point.split())
        
        # –ë–µ—Ä–µ–º —Å–ª–æ–≤–∞, –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è –≤ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö
        common_terms = [word for word, count in word_counter.items() if count > 1]
        return common_terms[:8]
    
    def _generate_quality_report(self, analysis):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –æ –∫–∞—á–µ—Å—Ç–≤–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        total = analysis["total_sources"]
        reliable = analysis["reliable_sources_count"]
        
        if total == 0:
            return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"
        
        reliability_percent = (reliable / total) * 100
        
        if reliability_percent >= 70:
            rating = "‚úÖ –í—ã—Å–æ–∫–æ–µ"
        elif reliability_percent >= 40:
            rating = "‚ö†Ô∏è –°—Ä–µ–¥–Ω–µ–µ"
        else:
            rating = "‚ùå –ù–∏–∑–∫–æ–µ"
        
        return f"{rating} –∫–∞—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ ({reliable}/{total} –Ω–∞–¥–µ–∂–Ω—ã—Ö)"
    
    def _is_junk(self, text):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –º—É—Å–æ—Ä–æ–º"""
        junk_phrases = [
            "–∫–ª–∏–∫–Ω–∏—Ç–µ", "–Ω–∞–∂–º–∏—Ç–µ", "–ø–æ–¥—Ä–æ–±–Ω–µ–µ", "—á–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ",
            "—É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ", "—Ä–µ–∫–ª–∞–º–∞", "sponsored", "advertisement",
            "–∫—É–ø–∏—Ç—å", "–∑–∞–∫–∞–∑–∞—Ç—å", "—Ü–µ–Ω–∞", "–∞–∫—Ü–∏—è", "—Å–∫–∏–¥–∫–∞"
        ]
        
        text_lower = text.lower()
        return any(phrase in text_lower for phrase in junk_phrases)

# ==================== –£–ú–ù–´–ô –ü–û–ò–°–ö ====================
class SmartGoogleSearch:
    def __init__(self):
        self.api_key = GOOGLE_API_KEY
        self.cse_id = GOOGLE_CSE_ID
        self.base_url = "https://www.googleapis.com/customsearch/v1"
        self.analyzer = InformationAnalyzer()
        
    def search_and_analyze(self, query):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞"""
        if not query or len(query.strip()) < 2:
            return {"error": "–ö–æ—Ä–æ—Ç–∫–∏–π –∑–∞–ø—Ä–æ—Å"}
        
        stats["google_searches"] += 1
        
        params = {
            "key": self.api_key,
            "cx": self.cse_id,
            "q": query,
            "num": 10,
            "hl": "ru",
            "lr": "lang_ru",
            "gl": "ru"
        }
        
        try:
            response = requests.get(self.base_url, params=params, timeout=15)
            
            if response.status_code != 200:
                return self._create_fallback(query)
            
            data = response.json()
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞
            structured_info = self.analyzer.analyze_topic(query, data)
            
            return {
                "success": True,
                "query": query,
                "structured_info": structured_info,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return self._create_fallback(query)
    
    def _create_fallback(self, query):
        """–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π fallback"""
        return {
            "success": False,
            "query": query,
            "structured_info": {
                "topic": query,
                "type": "–æ–±—â–∞—è",
                "analysis": {
                    "reliable_points": ["–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Ç—Ä–µ–±—É–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ –Ω–∞–¥–µ–∂–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º"],
                    "questionable_points": [],
                    "statistics": [],
                    "definitions": [],
                    "key_terms": [query.capitalize()],
                    "consensus_terms": [],
                    "total_sources": 0,
                    "reliable_sources_count": 0,
                    "sources_quality": []
                },
                "source_quality_report": "‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ—Ç –ø–æ–∏—Å–∫–æ–≤–∏–∫–∞"
            },
            "fallback": True
        }

# ==================== –ì–ï–ù–ï–†–ê–¢–û–† –ö–û–ù–°–ü–ï–ö–¢–û–í ====================
class SmartConspectGenerator:
    def __init__(self):
        self.searcher = SmartGoogleSearch()
        logger.info("‚úÖ –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≥–æ—Ç–æ–≤ —Å —Å–∏—Å—Ç–µ–º–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞")
    
    def generate(self, topic, volume="short"):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω—Å–ø–µ–∫—Ç —Å —É—á–µ—Ç–æ–º –∫–∞—á–µ—Å—Ç–≤–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        # –ü–∞—Å—Ö–∞–ª–∫–∞
        if self._is_easter_egg(topic):
            return self._create_easter_egg_response()
        
        # –ü–æ–∏—Å–∫ –∏ –∞–Ω–∞–ª–∏–∑ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞
        search_results = self.searcher.search_and_analyze(topic)
        structured_info = search_results.get("structured_info", {})
        analysis = structured_info.get("analysis", {})
        quality_report = structured_info.get("source_quality_report", "")
        
        # –í –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –æ–±—ä–µ–º–∞
        if volume == "detailed":
            return self._generate_detailed(topic, analysis, quality_report)
        elif volume == "extended":
            return self._generate_extended(topic, analysis, quality_report)
        else:
            return self._generate_short(topic, analysis, quality_report)
    
    def _is_easter_egg(self, text):
        text_lower = text.lower()
        return any(phrase in text_lower for phrase in [
            "–ø–ª–∞–Ω –∑–∞—Ö–≤–∞—Ç–∞ –ø–æ–ª—å—à–∏", "–∑–∞—Ö–≤–∞—Ç –ø–æ–ª—å—à–∏", "—á–∞–π–Ω–∞—è –ø–∞—Å—Ö–∞–ª–∫–∞"
        ])
    
    def _create_easter_egg_response(self):
        return "üçµ *–ü–∞—Å—Ö–∞–ª–∫–∞ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–∞!* –ß–∞–π–Ω—ã–µ —Ü–µ—Ä–µ–º–æ–Ω–∏–∏ ‚Äî –≤–∞–∂–Ω—ã–π –∫—É–ª—å—Ç—É—Ä–Ω—ã–π —Ñ–µ–Ω–æ–º–µ–Ω."
    
    def _generate_short(self, topic, analysis, quality_report):
        """–ö—Ä–∞—Ç–∫–æ - —Ç–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"""
        reliable_points = analysis.get("reliable_points", [])
        
        if not reliable_points:
            return f"üìå *{topic}*\n\nüîç *{quality_report}*\n\n–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –Ω–∞–¥–µ–∂–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö"
        
        conspect = f"üìå *{topic}*\n\nüîç *{quality_report}*\n\n"
        
        # –¢–æ–ª—å–∫–æ –Ω–∞–¥–µ–∂–Ω—ã–µ —Ç–æ—á–∫–∏
        for i, point in enumerate(reliable_points[:4], 1):
            conspect += f"‚Ä¢ {point}\n"
        
        # –ö–æ–Ω—Å–µ–Ω—Å—É—Å–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        consensus = analysis.get("consensus_terms", [])
        if consensus:
            conspect += f"\nüîë –ö–ª—é—á–µ–≤—ã–µ –ø–æ–Ω—è—Ç–∏—è: {', '.join(consensus[:3])}\n"
        
        return conspect
    
    def _generate_detailed(self, topic, analysis, quality_report):
        """–ü–æ–¥—Ä–æ–±–Ω–æ - –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è + –¥–∞–Ω–Ω—ã–µ"""
        reliable_points = analysis.get("reliable_points", [])
        
        if not reliable_points:
            return f"üìö *{topic}*\n\nüîç *{quality_report}*\n\n–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"
        
        conspect = f"üìö *{topic}*\n\nüîç *{quality_report}*\n\n"
        
        # –í—Å–µ –Ω–∞–¥–µ–∂–Ω—ã–µ —Ç–æ—á–∫–∏
        for i, point in enumerate(reliable_points[:8], 1):
            conspect += f"{i}. {point}\n"
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        statistics = analysis.get("statistics", [])
        if statistics:
            conspect += f"\nüìä *–î–∞–Ω–Ω—ã–µ:*\n"
            for stat in statistics[:4]:
                conspect += f"‚Ä¢ {stat}\n"
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        definitions = analysis.get("definitions", [])
        if definitions:
            conspect += f"\nüìñ *–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è:*\n"
            for definition in definitions[:3]:
                conspect += f"‚Ä¢ {definition}\n"
        
        # –¢–µ—Ä–º–∏–Ω—ã
        terms = analysis.get("key_terms", [])
        if terms:
            conspect += f"\nüîë *–¢–µ—Ä–º–∏–Ω—ã:* {', '.join(terms[:6])}\n"
        
        # –°–æ–º–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        questionable = analysis.get("questionable_points", [])
        if questionable:
            conspect += f"\n‚ö†Ô∏è *–°–ø–æ—Ä–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:*\n"
            for point in questionable[:2]:
                conspect += f"‚Ä¢ {point}\n"
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–∏—Å–∫–µ
        conspect += f"\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
        conspect += f"üìà –ù–∞–¥–µ–∂–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {analysis.get('reliable_sources_count', 0)}/{analysis.get('total_sources', 0)}"
        
        return conspect
    
    def _generate_extended(self, topic, analysis, quality_report):
        """–ü–æ–ª–Ω–æ - –≤—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å –º–∞—Ä–∫–∏—Ä–æ–≤–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞"""
        all_points = analysis.get("all_points", [])
        
        if not all_points:
            return f"üî¨ *{topic}*\n\nüîç *{quality_report}*\n\n–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
        
        conspect = f"üî¨ *{topic}*\n\nüîç *{quality_report}*\n\n"
        
        # –í—Å–µ —Ç–æ—á–∫–∏ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –∫–∞—á–µ—Å—Ç–≤–∞
        reliable_count = 0
        for i, point in enumerate(all_points, 1):
            if point.startswith("‚ö†Ô∏è"):
                conspect += f"{i}. {point}\n"
            else:
                conspect += f"{i}. ‚úÖ {point}\n"
                reliable_count += 1
        
        # –í—Å—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        statistics = analysis.get("statistics", [])
        if statistics:
            conspect += f"\nüìä *–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ —Ü–∏—Ñ—Ä—ã:*\n\n"
            for stat in statistics:
                conspect += f"‚Ä¢ {stat}\n"
        
        # –í—Å–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        definitions = analysis.get("definitions", [])
        if definitions:
            conspect += f"\nüìñ *–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è:*\n\n"
            for definition in definitions:
                conspect += f"‚Ä¢ {definition}\n"
        
        # –í—Å—è —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è
        terms = analysis.get("key_terms", [])
        consensus = analysis.get("consensus_terms", [])
        
        if terms:
            conspect += f"\nüî§ *–¢–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è:*\n\n"
            for i, term in enumerate(terms[:12], 1):
                conspect += f"{i}. {term}\n"
        
        if consensus:
            conspect += f"\nüéØ *–ö–æ–Ω—Å–µ–Ω—Å—É—Å–Ω—ã–µ –ø–æ–Ω—è—Ç–∏—è:*\n"
            conspect += f"{', '.join(consensus)}\n"
        
        # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –æ –∫–∞—á–µ—Å—Ç–≤–µ
        conspect += f"\n{'='*50}\n"
        conspect += f"üìã *–û–¢–ß–ï–¢ –û –ö–ê–ß–ï–°–¢–í–ï –ò–ù–§–û–†–ú–ê–¶–ò–ò*\n"
        conspect += f"{'='*50}\n\n"
        
        conspect += f"‚Ä¢ –í—Å–µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {analysis.get('total_sources', 0)}\n"
        conspect += f"‚Ä¢ –ù–∞–¥–µ–∂–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {analysis.get('reliable_sources_count', 0)}\n"
        conspect += f"‚Ä¢ –ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤: {reliable_count}\n"
        conspect += f"‚Ä¢ –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: {stats.get('filtered_sources', 0)}\n\n"
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–æ–º–µ–Ω–∞—Ö
        sources_quality = analysis.get("sources_quality", [])
        if sources_quality:
            conspect += f"*–ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:*\n"
            for source in sources_quality[:5]:
                quality_icon = "‚úÖ" if source["quality"] == "high" else "‚ö†Ô∏è" if source["quality"] == "medium" else "‚ùå"
                conspect += f"{quality_icon} {source['domain']} (–æ—Ü–µ–Ω–∫–∞: {source['score']})\n"
        
        conspect += f"\nüïí –ê–Ω–∞–ª–∏–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω: {datetime.now().strftime('%d.%m.%Y %H:%M')}"
        conspect += f"\nü§ñ @Konspekt_help_bot —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞"
        
        return conspect

# ==================== TELEGRAM BOT ====================
class TelegramBot:
    def __init__(self):
        if not TELEGRAM_TOKEN:
            raise ValueError("TELEGRAM_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        self.token = TELEGRAM_TOKEN
        self.bot_url = f"https://api.telegram.org/bot{self.token}"
        self.generator = SmartConspectGenerator()
        
        if RENDER_EXTERNAL_URL:
            self._setup_webhook()
        
        logger.info("‚úÖ Telegram –±–æ—Ç –≥–æ—Ç–æ–≤ —Å —Å–∏—Å—Ç–µ–º–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞")
    
    def _setup_webhook(self):
        webhook_url = f"{RENDER_EXTERNAL_URL}/webhook"
        try:
            response = requests.post(
                f"{self.bot_url}/setWebhook",
                json={"url": webhook_url},
                timeout=10
            )
            if response.json().get("ok"):
                logger.info(f"‚úÖ –í–µ–±—Ö—É–∫: {webhook_url}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–µ–±—Ö—É–∫–∞: {e}")
    
    def process_message(self, chat_id, text):
        text = text.strip()
        self._update_stats(chat_id)
        
        if text.startswith("/"):
            if text == "/start":
                return self._send_welcome(chat_id)
            elif text == "/help":
                return self._send_help(chat_id)
            elif text == "/stats":
                return self._send_stats(chat_id)
            elif text == "/quality":
                return self._send_quality_info(chat_id)
            else:
                return self._send_message(chat_id, "‚ùì –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞")
        
        if text in ["1", "2", "3"]:
            return self._handle_volume(chat_id, text)
        
        return self._handle_topic(chat_id, text)
    
    def _send_welcome(self, chat_id):
        welcome = (
            "üîç *–ë–æ—Ç —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏*\n\n"
            "ü§ñ *–ß—Ç–æ –Ω–æ–≤–æ–≥–æ:*\n"
            "‚Ä¢ ‚úÖ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–µ–Ω–∞–¥–µ–∂–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (Reddit –∏ –¥—Ä.)\n"
            "‚Ä¢ ‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Å–µ–≤–¥–æ–Ω–∞—É–∫—É –∏ —Å–µ–Ω—Å–∞—Ü–∏–æ–Ω–Ω–æ—Å—Ç—å\n"
            "‚Ä¢ ‚úÖ –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –Ω–∞—É—á–Ω—ã–º –∏ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º\n"
            "‚Ä¢ ‚úÖ –ú–∞—Ä–∫–∏—Ä–æ–≤–∫–∞ —Å–ø–æ—Ä–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏\n\n"
            "üìä *–£—Ä–æ–≤–Ω–∏:*\n"
            "‚Ä¢ 1 ‚Äî –ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ —Ç–µ–∑–∏—Å—ã\n"
            "‚Ä¢ 2 ‚Äî –§–∞–∫—Ç—ã + –¥–∞–Ω–Ω—ã–µ + –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è\n"
            "‚Ä¢ 3 ‚Äî –ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç —Å –æ—Ü–µ–Ω–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞\n\n"
            "üìå –ü—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—à–∏—Ç–µ —Ç–µ–º—É"
        )
        return self._send_message(chat_id, welcome)
    
    def _send_help(self, chat_id):
        help_text = (
            "üîç *–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞:*\n\n"
            "1. *–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–æ–º–µ–Ω–æ–≤:*\n"
            "   ‚úÖ –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: .edu, .gov, –Ω–∞—É—á–Ω—ã–µ –∂—É—Ä–Ω–∞–ª—ã\n"
            "   ‚ùå –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: Reddit, —Å–æ—Ü—Å–µ—Ç–∏, —Ä–∞–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∞–π—Ç—ã\n\n"
            "2. *–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞:*\n"
            "   ‚Ä¢ –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Å–µ–Ω—Å–∞—Ü–∏–æ–Ω–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤\n"
            "   ‚Ä¢ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø—Å–µ–≤–¥–æ–Ω–∞—É—á–Ω—ã—Ö —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π\n"
            "   ‚Ä¢ –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫—É –∏ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è\n\n"
            "3. *–û—Ü–µ–Ω–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:*\n"
            "   ‚Ä¢ –ö–∞–∂–¥–æ–º—É –∏—Å—Ç–æ—á–Ω–∏–∫—É –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç—Å—è –æ—Ü–µ–Ω–∫–∞\n"
            "   ‚Ä¢ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –º–∞—Ä–∫–∏—Ä—É–µ—Ç—Å—è ‚úÖ/‚ö†Ô∏è/‚ùå\n"
            "   ‚Ä¢ –í –æ—Ç—á–µ—Ç–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–æ—Ü–µ–Ω—Ç –Ω–∞–¥–µ–∂–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n\n"
            "üìå *–ü—Ä–∏–º–µ—Ä:*\n"
            "–û—Ç–ø—Ä–∞–≤—å—Ç–µ '–ö–≤–∞–Ω—Ç–æ–≤–∞—è —Ñ–∏–∑–∏–∫–∞' –∏–ª–∏ '–ò—Å—Ç–æ—Ä–∏—è –†–∏–º–∞'"
        )
        return self._send_message(chat_id, help_text)
    
    def _send_stats(self, chat_id):
        stat_text = (
            f"üìä *–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞:*\n\n"
            f"üë• –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {stats['total_users']}\n"
            f"üí¨ –°–æ–æ–±—â–µ–Ω–∏–π: {stats['total_messages']}\n"
            f"üìÑ –ö–æ–Ω—Å–ø–µ–∫—Ç–æ–≤: {stats['conspects_created']}\n"
            f"üîç –ü–æ–∏—Å–∫–æ–≤: {stats['google_searches']}\n"
            f"‚úÖ –ù–∞–¥–µ–∂–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {stats['reliable_sources']}\n"
            f"üö´ –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: {stats['filtered_sources']}\n"
            f"üìà –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {stats['filtered_sources']/(stats['google_searches']*10+1)*100:.1f}%"
        )
        return self._send_message(chat_id, stat_text)
    
    def _send_quality_info(self, chat_id):
        info = (
            "üî¨ *–°–∏—Å—Ç–µ–º–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞:*\n\n"
            "*–ü—Ä–æ–≤–µ—Ä—è–µ–º—ã–µ –¥–æ–º–µ–Ω—ã:*\n"
            "‚úÖ –ù–∞–¥–µ–∂–Ω—ã–µ: .edu, .gov, .org, –Ω–∞—É—á–Ω—ã–µ –∂—É—Ä–Ω–∞–ª—ã\n"
            "‚ö†Ô∏è –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ: –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ —Å–∞–π—Ç—ã, –±–ª–æ–≥–∏\n"
            "‚ùå –ù–µ–Ω–∞–¥–µ–∂–Ω—ã–µ: Reddit, —Å–æ—Ü—Å–µ—Ç–∏, —Ñ–æ—Ä—É–º—ã\n\n"
            "*–ú–∞—Ä–∫–µ—Ä—ã –Ω–µ–Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏:*\n"
            "‚Ä¢ –°–µ–Ω—Å–∞—Ü–∏–æ–Ω–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ ('–®–æ–∫!', '–°–µ–Ω—Å–∞—Ü–∏—è!')\n"
            "‚Ä¢ –ü—Å–µ–≤–¥–æ–Ω–∞—É—á–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã\n"
            "‚Ä¢ –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n"
            "‚Ä¢ –£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –±–µ–∑ —Å—Å—ã–ª–æ–∫ –Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è\n\n"
            "üìå –ë–æ—Ç –ø–æ–º–µ—á–∞–µ—Ç ‚ö†Ô∏è —Å–ø–æ—Ä–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é"
        )
        return self._send_message(chat_id, info)
    
    def _handle_topic(self, chat_id, topic):
        user_id = str(chat_id)
        if user_id not in stats["user_states"]:
            stats["user_states"][user_id] = {}
        
        stats["user_states"][user_id]["pending_topic"] = topic
        
        response = (
            f"üéØ *–¢–µ–º–∞: {topic}*\n\n"
            f"üîç *–ë—É–¥–µ—Ç –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ –∫–∞—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤*\n\n"
            f"üìä *–£—Ä–æ–≤–µ–Ω—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏:*\n\n"
            f"1Ô∏è‚É£ –¢–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ —Ç–µ–∑–∏—Å—ã\n"
            f"2Ô∏è‚É£ –§–∞–∫—Ç—ã + –¥–∞–Ω–Ω—ã–µ + –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è\n"
            f"3Ô∏è‚É£ –ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç —Å –æ—Ü–µ–Ω–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞\n\n"
            f"–û—Ç–ø—Ä–∞–≤—å—Ç–µ 1, 2 –∏–ª–∏ 3"
        )
        return self._send_message(chat_id, response)
    
    def _handle_volume(self, chat_id, volume_choice):
        user_id = str(chat_id)
        user_state = stats["user_states"].get(user_id, {})
        topic = user_state.get("pending_topic", "")
        
        if not topic:
            return self._send_message(chat_id, "‚ùå –°–Ω–∞—á–∞–ª–∞ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ —Ç–µ–º—É")
        
        volume_map = {"1": "short", "2": "detailed", "3": "extended"}
        volume = volume_map.get(volume_choice, "short")
        
        # –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ
        self._send_message(chat_id, f"üîç *–ü—Ä–æ–≤–µ—Ä—è—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ —Ç–µ–º–µ:* {topic}\nüìä –£—Ä–æ–≤–µ–Ω—å: {volume_choice}/3")
        
        try:
            conspect = self.generator.generate(topic, volume)
            stats["conspects_created"] += 1
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º
            self._send_conspect_safely(chat_id, conspect)
            
            # –ö–æ—Ä–æ—Ç–∫–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ
            return self._send_message(chat_id, "‚úÖ *–ì–æ—Ç–æ–≤–æ!*\n\n–ù–æ–≤–∞—è —Ç–µ–º–∞? –ü—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—à–∏—Ç–µ –µ—ë")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            return self._send_message(
                chat_id,
                f"‚ùå *–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏*\n\n–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥—É—é —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É"
            )
    
    def _send_conspect_safely(self, chat_id, conspect):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Å–ø–µ–∫—Ç"""
        max_length = 4000
        
        if len(conspect) <= max_length:
            self._send_message(chat_id, conspect)
            return
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º
        sections = re.split(r'(={10,}|\n‚îÅ‚îÅ[‚îÅ]+\n)', conspect)
        
        current = ""
        for section in sections:
            if re.match(r'(={10,}|\n‚îÅ‚îÅ[‚îÅ]+\n)', section):
                if current and len(current) > 1000:
                    self._send_message(chat_id, current.strip())
                    current = section + "\n\n"
                else:
                    current += section + "\n\n"
            else:
                if len(current + section) > max_length and current:
                    self._send_message(chat_id, current.strip())
                    current = section
                else:
                    current += section
        
        if current.strip():
            self._send_message(chat_id, current.strip())
    
    def _update_stats(self, chat_id):
        user_id = str(chat_id)
        
        if user_id not in stats["user_states"]:
            stats["total_users"] += 1
            stats["user_states"][user_id] = {
                "first_seen": datetime.now().isoformat(),
                "message_count": 0
            }
        
        stats["user_states"][user_id]["last_seen"] = datetime.now().isoformat()
        stats["user_states"][user_id]["message_count"] += 1
        stats["total_messages"] += 1
    
    def _send_message(self, chat_id, text):
        try:
            response = requests.post(
                f"{self.bot_url}/sendMessage",
                json={
                    "chat_id": chat_id,
                    "text": text,
                    "parse_mode": "Markdown",
                    "disable_web_page_preview": True
                },
                timeout=15
            )
            return response.json().get("ok", False)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {e}")
            return False

# ==================== HTTP –°–ï–†–í–ï–† ====================
class BotHTTPServer(BaseHTTPRequestHandler):
    def do_GET(self):
        path = self.path.split('?')[0]
        
        if path == "/":
            self._send_html(INDEX_HTML)
        elif path == "/health":
            self._send_json({"status": "ok", "time": datetime.now().isoformat()})
        elif path == "/stats":
            self._send_json(stats)
        elif path == "/quality_info":
            info = {
                "reliable_domains": SourceChecker.RELIABLE_DOMAINS[:10],
                "unreliable_domains": SourceChecker.UNRELIABLE_DOMAINS,
                "filtered_sources": stats.get("filtered_sources", 0),
                "reliable_sources": stats.get("reliable_sources", 0)
            }
            self._send_json(info)
        else:
            self.send_response(404)
            self.end_headers()
    
    def do_POST(self):
        if self.path == "/webhook":
            content_length = int(self.headers.get('Content-Length', 0))
            if content_length:
                try:
                    data = self.rfile.read(content_length)
                    update = json.loads(data.decode('utf-8'))
                    
                    threading.Thread(
                        target=self._handle_update,
                        args=(update,),
                        daemon=True
                    ).start()
                    
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–µ–±—Ö—É–∫–∞: {e}")
            
            self.send_response(200)
            self.end_headers()
            self.wfile.write(b'OK')
        else:
            self.send_response(404)
            self.end_headers()
    
    def _handle_update(self, update):
        try:
            if "message" in update and "text" in update["message"]:
                chat_id = update["message"]["chat"]["id"]
                text = update["message"]["text"]
                
                bot = TelegramBot()
                bot.process_message(chat_id, text)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
    
    def _send_html(self, content):
        self.send_response(200)
        self.send_header('Content-Type', 'text/html; charset=utf-8')
        self.end_headers()
        self.wfile.write(content.encode('utf-8'))
    
    def _send_json(self, data):
        self.send_response(200)
        self.send_header('Content-Type', 'application/json; charset=utf-8')
        self.end_headers()
        self.wfile.write(json.dumps(data, ensure_ascii=False, indent=2).encode('utf-8'))
    
    def log_message(self, format, *args):
        pass

INDEX_HTML = """<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>ü§ñ –ë–æ—Ç —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; background: #f0f2f5; }
        .container { max-width: 800px; margin: 0 auto; background: white; padding: 25px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
        .status { color: green; font-weight: bold; padding: 10px; background: #e8f5e8; border-radius: 5px; }
        .quality-badges { display: flex; gap: 10px; margin: 15px 0; }
        .badge { padding: 5px 10px; border-radius: 4px; font-size: 14px; }
        .badge-reliable { background: #d4edda; color: #155724; }
        .badge-unreliable { background: #f8d7da; color: #721c24; }
        .badge-filtered { background: #fff3cd; color: #856404; }
    </style>
</head>
<body>
    <div class="container">
        <h2>ü§ñ –ë–æ—Ç —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏</h2>
        <p class="status">‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç —Å —Å–∏—Å—Ç–µ–º–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –Ω–µ–Ω–∞–¥–µ–∂–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤</p>
        
        <div class="quality-badges">
            <div class="badge badge-reliable">‚úÖ –ù–∞–¥–µ–∂–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏</div>
            <div class="badge badge-unreliable">‚ùå –ù–µ–Ω–∞–¥–µ–∂–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏</div>
            <div class="badge badge-filtered">‚ö†Ô∏è –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ</div>
        </div>
        
        <h3>üîç –ß—Ç–æ —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –±–æ—Ç:</h3>
        <ul>
            <li>‚ùå Reddit, —Å–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏, –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ —Ñ–æ—Ä—É–º—ã</li>
            <li>‚ùå –°–µ–Ω—Å–∞—Ü–∏–æ–Ω–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ –ø—Å–µ–≤–¥–æ–Ω–∞—É—á–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç</li>
            <li>‚ùå –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –±–µ–∑ —Å—Å—ã–ª–æ–∫ –Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏ –¥–∞–Ω–Ω—ã–µ</li>
            <li>‚úÖ –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: .edu, .gov, –Ω–∞—É—á–Ω—ã–µ –∂—É—Ä–Ω–∞–ª—ã, –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏</li>
        </ul>
        
        <h3>üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã:</h3>
        <div id="stats">–ó–∞–≥—Ä—É–∑–∫–∞...</div>
        
        <h3>üîó –ë—ã—Å—Ç—Ä—ã–µ —Å—Å—ã–ª–∫–∏:</h3>
        <p><a href="https://t.me/Konspekt_help_bot" target="_blank">ü§ñ –û—Ç–∫—Ä—ã—Ç—å –±–æ—Ç–∞</a></p>
        <p><a href="/stats" target="_blank">üìà –ü–æ–ª–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ (JSON)</a></p>
        <p><a href="/quality_info" target="_blank">üî¨ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ –ø—Ä–æ–≤–µ—Ä–∫–∏</a></p>
        
        <p style="color: #666; margin-top: 20px;">
            –û–±–Ω–æ–≤–ª–µ–Ω–æ: <span id="time"></span>
        </p>
    </div>
    
    <script>
        async function loadStats() {
            try {
                const response = await fetch('/stats');
                const data = await response.json();
                
                document.getElementById('stats').innerHTML = `
                    <p>üë• –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: ${data.total_users || 0}</p>
                    <p>üìÑ –ö–æ–Ω—Å–ø–µ–∫—Ç–æ–≤: ${data.conspects_created || 0}</p>
                    <p>üîç –ü–æ–∏—Å–∫–æ–≤: ${data.google_searches || 0}</p>
                    <p>‚úÖ –ù–∞–¥–µ–∂–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: ${data.reliable_sources || 0}</p>
                    <p>üö´ –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: ${data.filtered_sources || 0}</p>
                `;
                
                document.getElementById('time').textContent = new Date().toLocaleTimeString();
            } catch (error) {
                document.getElementById('stats').innerHTML = '–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏';
            }
        }
        
        loadStats();
        setInterval(loadStats, 10000);
    </script>
</body>
</html>
"""

# ==================== –ó–ê–ü–£–°–ö ====================
def main():
    logger.info("=" * 60)
    logger.info("üöÄ –ó–ê–ü–£–°–ö –ë–û–¢–ê –° –ü–†–û–í–ï–†–ö–û–ô –ö–ê–ß–ï–°–¢–í–ê")
    logger.info("=" * 60)
    logger.info(f"üåê URL: {RENDER_EXTERNAL_URL}")
    logger.info(f"üö™ –ü–æ—Ä—Ç: {PORT}")
    logger.info("‚úÖ –†–µ–∂–∏–º: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–µ–Ω–∞–¥–µ–∂–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
    logger.info(f"‚úÖ –ù–∞–¥–µ–∂–Ω—ã–µ –¥–æ–º–µ–Ω—ã: {len(SourceChecker.RELIABLE_DOMAINS)}")
    logger.info(f"‚úÖ –ù–µ–Ω–∞–¥–µ–∂–Ω—ã–µ –¥–æ–º–µ–Ω—ã: {len(SourceChecker.UNRELIABLE_DOMAINS)}")
    logger.info("=" * 60)
    
    server = HTTPServer(('', PORT), BotHTTPServer)
    logger.info(f"‚úÖ HTTP —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω –Ω–∞ –ø–æ—Ä—Ç—É {PORT}")
    
    try:
        server.serve_forever()
    except KeyboardInterrupt:
        logger.info("‚èπÔ∏è –°–µ—Ä–≤–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    main()
