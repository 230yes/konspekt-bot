#!/usr/bin/env python3
"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π Konspekt Helper Bot - –ê–≥—Ä–µ–≥–∞—Ü–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
–ë–æ—Ç —Å–≤—è–∑—ã–≤–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, —É–±–∏—Ä–∞–µ—Ç –¥—É–±–ª–∏ –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö
"""

import os
import logging
import json
import requests
from datetime import datetime
from http.server import HTTPServer, BaseHTTPRequestHandler
import threading
import random
import re
import html
from urllib.parse import urlparse
from collections import Counter, defaultdict
import hashlib
from difflib import SequenceMatcher

# ==================== –ù–ê–°–¢–†–û–ô–ö–ê ====================
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
GOOGLE_CSE_ID = os.getenv("GOOGLE_CSE_ID", "13aac457275834df9")
RENDER_EXTERNAL_URL = os.getenv("RENDER_EXTERNAL_URL", "")
PORT = int(os.getenv("PORT", 10000))

if not TELEGRAM_TOKEN:
    logger.error("‚ùå TELEGRAM_TOKEN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")
    exit(1)

# ==================== –°–¢–ê–¢–ò–°–¢–ò–ö–ê ====================
stats = {
    "total_users": 0,
    "total_messages": 0,
    "conspects_created": 0,
    "google_searches": 0,
    "reliable_sources": 0,
    "filtered_sources": 0,
    "aggregated_facts": 0,
    "duplicates_removed": 0,
    "start_time": datetime.now().isoformat(),
    "user_states": {}
}

# ==================== –°–ò–°–¢–ï–ú–ê –ü–†–û–í–ï–†–ö–ò –ò–°–¢–û–ß–ù–ò–ö–û–í ====================
class SourceChecker:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
    
    # –ù–∞–¥–µ–∂–Ω—ã–µ –¥–æ–º–µ–Ω—ã (–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ, –Ω–∞—É–∫–∞, –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏)
    RELIABLE_DOMAINS = [
        '.edu', '.ac.', '.gov', '.org', 
        'wikipedia.org', 'arxiv.org', 'sciencedirect.com',
        'nature.com', 'sciencemag.org', 'researchgate.net',
        'springer.com', 'ieee.org', 'ncbi.nlm.nih.gov',
        'who.int', 'unesco.org', 'bbc.com', 'reuters.com',
        'theguardian.com', 'nytimes.com', 'meduza.io'
    ]
    
    # –ù–µ–Ω–∞–¥–µ–∂–Ω—ã–µ –¥–æ–º–µ–Ω—ã (–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç, —Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏—è)
    UNRELIABLE_DOMAINS = [
        'reddit.com', '4chan.org', 'tiktok.com', 
        'twitter.com', 'x.com', 'instagram.com',
        'facebook.com', 'pikabu.ru', 'vk.com',
        'livejournal.com', '9gag.com', 'buzzfeed.com'
    ]
    
    # –°–ª–æ–≤–∞-–º–∞—Ä–∫–µ—Ä—ã –Ω–µ–Ω–∞—É—á–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    PSEUDOSCIENCE_KEYWORDS = [
        '–ª–∂–µ–Ω–∞—É–∫–∞', '–ø—Å–µ–≤–¥–æ–Ω–∞—É–∫–∞', '–∫–æ–Ω—Å–ø–∏—Ä–æ–ª–æ–≥–∏—è', '—Ç–µ–æ—Ä–∏—è –∑–∞–≥–æ–≤–æ—Ä–∞',
        '—á—É–¥–µ—Å–Ω–æ–µ –∏—Å—Ü–µ–ª–µ–Ω–∏–µ', '–º–∞–≥–∏—á–µ—Å–∫–∞—è —Å–∏–ª–∞', '—ç–∫—Å—Ç—Ä–∞—Å–µ–Ω—Å', '—è—Å–Ω–æ–≤–∏–¥—è—â–∏–π',
        '–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –º–µ–¥–∏—Ü–∏–Ω–∞', '–±–∏–æ—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞', '—Ç–æ—Ä—Å–∏–æ–Ω–Ω—ã–µ –ø–æ–ª—è',
        '—Ö–æ–ª–æ–¥–Ω—ã–π —è–¥–µ—Ä–Ω—ã–π —Å–∏–Ω—Ç–µ–∑', '–≤–µ—á–Ω—ã–π –¥–≤–∏–≥–∞—Ç–µ–ª—å', '–ø–∞–º—è—Ç—å –≤–æ–¥—ã'
    ]
    
    # –ü—Ä–∏–∑–Ω–∞–∫–∏ –Ω–µ–Ω–∞–¥–µ–∂–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    UNRELIABLE_PATTERNS = [
        r'—à–æ–∫[!.]?', r'—Å–µ–Ω—Å–∞—Ü[–∏—è][!.]?', r'–≤—ã –Ω–µ –ø–æ–≤–µ—Ä–∏—Ç–µ', r'–≤—Å–µ–º –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ',
        r'—É—á[—ë–µ]–Ω—ã–µ —Å–∫—Ä—ã–≤–∞—é—Ç', r'–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –º–æ–ª—á–∏—Ç', r'100% –¥–æ–∫–∞–∑–∞–Ω–æ',
        r'–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ –æ–ø—Ä–æ–≤–µ—Ä–≥–Ω—É—Ç–æ', r'—ç—Ç–æ —Å–∫—Ä—ã–≤–∞—é—Ç', r'—Ç–∞–π–Ω–æ–µ –∑–Ω–∞–Ω–∏–µ',
        r'—Å–µ–∫—Ä–µ—Ç–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã', r'–∑–∞–ø—Ä–µ—â[–µ—ë]–Ω–Ω–∞—è –ø—Ä–∞–≤–¥–∞'
    ]
    
    # –ü—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞—É—á–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    SCIENTIFIC_PATTERNS = [
        r'–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏[–µ—è] –ø–æ–∫–∞–∑–∞–ª[–∏–æ]', r'—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç[—ã]? –ø–æ–¥—Ç–≤–µ—Ä–¥–∏–ª[–∏]',
        r'–ø–æ –¥–∞–Ω–Ω—ã–º', r'—Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º', r'–º–µ—Ç–∞-–∞–Ω–∞–ª–∏–∑',
        r'—Ä–µ—Ü–µ–Ω–∑–∏—Ä—É–µ–º–æ–µ –∏–∑–¥–∞–Ω–∏–µ', r'–∫–ª–∏–Ω–∏—á–µ—Å–∫–æ–µ –∏—Å–ø—ã—Ç–∞–Ω–∏–µ',
        r'–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ', r'–¥–≤–æ–π–Ω–æ–π —Å–ª–µ–ø–æ–π –º–µ—Ç–æ–¥'
    ]
    
    def check_source_quality(self, url, title, snippet):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º"""
        score = 0
        reasons = []
        
        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ–º–µ–Ω–∞
        domain_quality = self._check_domain(url)
        if domain_quality == "reliable":
            score += 3
            reasons.append("‚úÖ –ù–∞–¥–µ–∂–Ω—ã–π –¥–æ–º–µ–Ω")
        elif domain_quality == "unreliable":
            score -= 2
            reasons.append("‚ö†Ô∏è –ù–µ–Ω–∞–¥–µ–∂–Ω—ã–π –¥–æ–º–µ–Ω")
        
        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –Ω–∞ —Å–µ–Ω—Å–∞—Ü–∏–æ–Ω–Ω–æ—Å—Ç—å
        title_score = self._check_sensationalism(title)
        score += title_score
        if title_score < 0:
            reasons.append("‚ö†Ô∏è –°–µ–Ω—Å–∞—Ü–∏–æ–Ω–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫")
        
        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –Ω–∞ –Ω–∞—É—á–Ω–æ—Å—Ç—å
        content_score = self._check_content_quality(snippet)
        score += content_score
        if content_score > 0:
            reasons.append("‚úÖ –ù–∞—É—á–Ω—ã–π —Å—Ç–∏–ª—å")
        
        # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Å–µ–≤–¥–æ–Ω–∞—É–∫—É
        if self._check_pseudoscience(title + " " + snippet):
            score -= 3
            reasons.append("‚ùå –ü—Ä–∏–∑–Ω–∞–∫–∏ –ø—Å–µ–≤–¥–æ–Ω–∞—É–∫–∏")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏
        if score >= 3:
            quality = "high"
        elif score >= 0:
            quality = "medium"
        else:
            quality = "low"
        
        return {
            "quality": quality,
            "score": score,
            "reasons": reasons,
            "domain": urlparse(url).netloc if url else "unknown"
        }
    
    def _check_domain(self, url):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ–º–µ–Ω –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        if not url:
            return "neutral"
        
        url_lower = url.lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–¥–µ–∂–Ω—ã–µ –¥–æ–º–µ–Ω—ã
        for domain in self.RELIABLE_DOMAINS:
            if domain in url_lower:
                return "reliable"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ–Ω–∞–¥–µ–∂–Ω—ã–µ –¥–æ–º–µ–Ω—ã
        for domain in self.UNRELIABLE_DOMAINS:
            if domain in url_lower:
                return "unreliable"
        
        return "neutral"
    
    def _check_sensationalism(self, text):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Å–µ–Ω—Å–∞—Ü–∏–æ–Ω–Ω–æ—Å—Ç—å"""
        if not text:
            return 0
        
        text_lower = text.lower()
        
        # –°—á–µ—Ç—á–∏–∫ —Å–µ–Ω—Å–∞—Ü–∏–æ–Ω–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤
        sensational_count = 0
        for pattern in self.UNRELIABLE_PATTERNS:
            if re.search(pattern, text_lower):
                sensational_count += 1
        
        if sensational_count >= 2:
            return -2  # –û—á–µ–Ω—å —Å–µ–Ω—Å–∞—Ü–∏–æ–Ω–Ω—ã–π
        elif sensational_count == 1:
            return -1  # –ù–µ–º–Ω–æ–≥–æ —Å–µ–Ω—Å–∞—Ü–∏–æ–Ω–Ω—ã–π
        
        return 0  # –ù–æ—Ä–º–∞–ª—å–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
    
    def _check_content_quality(self, text):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è"""
        if not text:
            return 0
        
        text_lower = text.lower()
        
        # –°—á–µ—Ç—á–∏–∫ –Ω–∞—É—á–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤
        scientific_count = 0
        for pattern in self.SCIENTIFIC_PATTERNS:
            if re.search(pattern, text_lower):
                scientific_count += 1
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ü–∏—Ñ—Ä –∏ –¥–∞–Ω–Ω—ã—Ö
        has_numbers = bool(re.search(r'\d+[%‚Ä∞¬∞]|\d+\.\d+', text))
        has_references = bool(re.search(r'–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏[–µ—è]|—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç|–¥–∞–Ω–Ω—ã–µ', text_lower))
        
        score = 0
        if scientific_count >= 2:
            score += 2
        elif scientific_count == 1:
            score += 1
        
        if has_numbers:
            score += 1
        if has_references:
            score += 1
        
        return score
    
    def _check_pseudoscience(self, text):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø—Å–µ–≤–¥–æ–Ω–∞—É–∫–∏"""
        text_lower = text.lower()
        
        for keyword in self.PSEUDOSCIENCE_KEYWORDS:
            if keyword in text_lower:
                return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∑–∞–≥–æ–≤–æ—Ä–æ–≤
        conspiracy_patterns = [
            r'—Ç–∞–π–Ω–æ–µ –ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ', r'–º–∏—Ä–æ–≤–∞—è –∑–∞–∫—É–ª–∏—Å–∞',
            r'—Å–∫—Ä—ã–≤–∞[—é]?—Ç –ø—Ä–∞–≤–¥—É', r'–Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ',
            r'–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –Ω–∞—É–∫–∞ –æ—à–∏–±–∞–µ—Ç—Å—è'
        ]
        
        for pattern in conspiracy_patterns:
            if re.search(pattern, text_lower):
                return True
        
        return False

# ==================== –°–ò–°–¢–ï–ú–ê –ê–ì–†–ï–ì–ê–¶–ò–ò –ò –§–ò–õ–¨–¢–†–ê–¶–ò–ò ====================
class InformationAggregator:
    """–ê–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –∏ —Å–≤—è–∑—ã–≤–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
    
    def __init__(self):
        self.source_checker = SourceChecker()
        
    def aggregate_information(self, items, query):
        """–ê–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
        all_facts = []
        all_definitions = []
        all_statistics = []
        sources_by_domain = defaultdict(list)
        content_hashes = set()  # –î–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        
        for item in items[:15]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–æ–ª—å—à–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            title = item.get("title", "")
            snippet = item.get("snippet", "")
            link = item.get("link", "")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            source_check = self.source_checker.check_source_quality(link, title, snippet)
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            if source_check["quality"] == "low":
                continue
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            processed_data = self._process_source_item(title, snippet, link, query, source_check)
            
            if processed_data:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —Ö–µ—à—É —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
                content_hash = self._generate_content_hash(processed_data["fact"])
                if content_hash in content_hashes:
                    stats["duplicates_removed"] += 1
                    continue
                
                content_hashes.add(content_hash)
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∞–∫—Ç—ã
                if processed_data["fact"]:
                    all_facts.append({
                        "text": processed_data["fact"],
                        "source": link,
                        "domain": urlparse(link).netloc,
                        "quality": source_check["quality"],
                        "type": self._classify_fact_type(processed_data["fact"])
                    })
                
                # –î–æ–±–∞–≤–ª—è–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
                if processed_data["definition"]:
                    all_definitions.append(processed_data["definition"])
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                all_statistics.extend(processed_data["statistics"])
                
                # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–æ –¥–æ–º–µ–Ω–∞–º
                domain = urlparse(link).netloc
                sources_by_domain[domain].append({
                    "url": link,
                    "quality": source_check["quality"]
                })
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏ —Å–≤—è–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        analyzed_info = self._analyze_and_link_facts(all_facts, query)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        result = {
            "linked_facts": analyzed_info["linked_facts"],
            "fact_clusters": analyzed_info["fact_clusters"],
            "definitions": self._merge_definitions(all_definitions)[:6],
            "statistics": self._merge_statistics(all_statistics)[:10],
            "timeline_data": analyzed_info["timeline_data"][:5],
            "comparison_data": analyzed_info["comparison_data"][:5],
            "key_entities": analyzed_info["key_entities"][:12],
            "controversial_points": analyzed_info["controversial_points"][:3],
            "source_coverage": self._calculate_source_coverage(sources_by_domain),
            "total_unique_facts": len(analyzed_info["linked_facts"]),
            "domains_used": list(sources_by_domain.keys())[:8]
        }
        
        stats["aggregated_facts"] += len(result["linked_facts"])
        return result
    
    def _process_source_item(self, title, snippet, link, query, source_check):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        full_text = f"{title}. {snippet}"
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∞–∫—Ç
        fact = self._extract_comprehensive_fact(full_text, query)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        definition = self._extract_enhanced_definition(full_text)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        statistics = self._extract_detailed_statistics(full_text)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—ã –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
        dates = self._extract_dates(full_text)
        if dates and fact:
            fact = f"{fact} ({dates[0]})"
        
        return {
            "fact": fact,
            "definition": definition,
            "statistics": statistics,
            "dates": dates,
            "quality": source_check["quality"]
        }
    
    def _extract_comprehensive_fact(self, text, query):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π —Ñ–∞–∫—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
        sentences = re.split(r'[.!?]+', text)
        
        relevant_sentences = []
        for sentence in sentences:
            sentence = sentence.strip()
            if 40 < len(sentence) < 250:
                if self._is_comprehensive_sentence(sentence, query):
                    relevant_sentences.append(sentence)
        
        if not relevant_sentences:
            return None
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        if len(relevant_sentences) > 1:
            # –ù–∞—Ö–æ–¥–∏–º –Ω–∞–∏–±–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
            best_sentence = max(relevant_sentences, key=lambda s: len(s.split()))
            return best_sentence[:220]
        
        return relevant_sentences[0][:200]
    
    def _is_comprehensive_sentence(self, sentence, query):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º"""
        sentence_lower = sentence.lower()
        query_words = [w.lower() for w in query.split() if len(w) > 3]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
        relevance_score = sum(1 for word in query_words if word in sentence_lower)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
        has_specifics = bool(re.search(r'\d{4}|\d+%|\d+\.\d+', sentence))
        has_entities = bool(re.search(r'[–ê-–Ø–Å][–∞-—è—ë]+\s+[–ê-–Ø–Å][–∞-—è—ë]+', sentence))
        has_verbs = len([w for w in sentence.split() if w.endswith(('—Å—è', '—Ç—å', '–ª', '–ª–∞'))]) > 1
        
        return (relevance_score > 0 or has_specifics) and (has_verbs or has_entities)
    
    def _extract_enhanced_definition(self, text):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
        patterns = [
            r'—ç—Ç–æ\s+[^.!?]{10,150}(?:[.!?]|\s+‚Äî\s+[^.!?]{5,50})',
            r'–æ–ø—Ä–µ–¥–µ–ª[—è—é]–µ—Ç—Å—è\s+–∫–∞–∫\s+[^.!?]{10,150}[.!?]',
            r'—è–≤–ª—è–µ—Ç—Å—è\s+[^.!?]{10,150}(?:[.!?]|\s+‚Äî\s+[^.!?]{5,50})',
            r'–ø–æ–¥\s+[^.!?]{3,20}\s+–ø–æ–Ω–∏–º–∞[—é—è]—Ç\s+[^.!?]{10,150}[.!?]'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                definition = match.group(0).strip()
                if 30 < len(definition) < 180:
                    # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –µ—Å–ª–∏ –µ—Å—Ç—å
                    context_match = re.search(r'[^.!?]{10,80}\s+‚Äî\s+', definition)
                    if context_match:
                        return definition[:160] + "..."
                    else:
                        return definition[:140] + "..."
        
        return None
    
    def _extract_detailed_statistics(self, text):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        patterns = [
            # –ü—Ä–æ—Ü–µ–Ω—Ç—ã —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
            r'\d+\.?\d*%\s+(?:[^.!?]{5,40})',
            # –ë–æ–ª—å—à–∏–µ —á–∏—Å–ª–∞ —Å –ø–æ—è—Å–Ω–µ–Ω–∏–µ–º
            r'\d+[,.]?\d*\s*(?:–º–ª–Ω|–º–ª—Ä–¥|—Ç—ã—Å|–º–∏–ª–ª–∏–æ–Ω|–º–∏–ª–ª–∏–∞—Ä–¥|—Ç—ã—Å—è—á)[^.!?]{5,40}',
            # –î–µ–Ω—å–≥–∏ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
            r'\$\d+[,.]?\d*\s+(?:[^.!?]{5,30})',
            # –î–∞—Ç—ã –∏ –ø–µ—Ä–∏–æ–¥—ã
            r'\d{4}\s*(?:–≥–æ–¥[—É–∞]?|–≥\.?)\s+(?:[^.!?]{5,30})',
            # –î–∏–∞–ø–∞–∑–æ–Ω—ã
            r'–æ—Ç\s+\d+\s+–¥–æ\s+\d+\s+(?:[^.!?]{5,20})',
            # –°—Ä–∞–≤–Ω–µ–Ω–∏—è
            r'–≤\s+\d+[,.]?\d*\s+—Ä–∞–∑–∞\s+(?:[^.!?]{5,30})'
        ]
        
        statistics = []
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                if 10 < len(match) < 100:
                    statistics.append(match.strip())
        
        return list(set(statistics))[:8]
    
    def _extract_dates(self, text):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞—Ç—ã"""
        patterns = [
            r'\d{1,2}\s+[–∞-—è—ë]+\s+\d{4}',
            r'\d{4}\s+–≥–æ–¥[–∞—É]?',
            r'–≤\s+\d{4}\s+–≥–æ–¥—É',
            r'\d{1,2}\.\d{1,2}\.\d{4}'
        ]
        
        dates = []
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            dates.extend(matches)
        
        return dates[:3]
    
    def _generate_content_hash(self, text):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ö–µ—à –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
        if not text:
            return ""
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç (—É–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã, –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É)
        normalized = re.sub(r'\s+', ' ', text.lower()).strip()
        return hashlib.md5(normalized.encode()).hexdigest()[:16]
    
    def _classify_fact_type(self, fact):
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ç–∏–ø —Ñ–∞–∫—Ç–∞"""
        fact_lower = fact.lower()
        
        if re.search(r'\d{4}|\d+\.\d+|\d+%', fact):
            return "statistical"
        elif any(word in fact_lower for word in ['–æ–±–Ω–∞—Ä—É–∂–µ–Ω', '–æ—Ç–∫—Ä—ã—Ç', '–∏–∑–æ–±—Ä–µ—Ç—ë–Ω']):
            return "discovery"
        elif any(word in fact_lower for word in ['–≤—ã–∑–≤–∞–ª', '–ø—Ä–∏–≤–µ–ª', '–ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏–µ–º']):
            return "consequence"
        elif any(word in fact_lower for word in ['—Å–æ–≥–ª–∞—Å–Ω–æ', '–ø–æ –¥–∞–Ω–Ω—ã–º', '–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ']):
            return "research"
        elif any(word in fact_lower for word in ['–≤–∞–∂–Ω', '–∑–Ω–∞—á–µ–Ω–∏', '–≤–ª–∏—è–Ω–∏']):
            return "significance"
        
        return "general"
    
    def _analyze_and_link_facts(self, facts, query):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏ —Å–≤—è–∑—ã–≤–∞–µ—Ç —Ñ–∞–∫—Ç—ã –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        if not facts:
            return {
                "linked_facts": [],
                "fact_clusters": [],
                "timeline_data": [],
                "comparison_data": [],
                "key_entities": [],
                "controversial_points": []
            }
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ñ–∞–∫—Ç—ã –ø–æ —Ç–∏–ø–∞–º –∏ —Ç–µ–º–∞–º
        fact_clusters = defaultdict(list)
        
        for fact in facts:
            fact_type = fact["type"]
            fact_clusters[fact_type].append(fact)
        
        # –°–æ–∑–¥–∞–µ–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ñ–∞–∫—Ç—ã
        linked_facts = []
        
        # 1. –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∞–∫—Ç—ã —Å —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        if "statistical" in fact_clusters:
            stats_facts = fact_clusters["statistical"]
            if len(stats_facts) >= 2:
                linked = self._link_statistical_facts(stats_facts)
                linked_facts.extend(linked)
        
        # 2. –§–∞–∫—Ç—ã –æ–± –æ—Ç–∫—Ä—ã—Ç–∏—è—Ö –∏ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏—è—Ö
        if "discovery" in fact_clusters:
            disc_facts = fact_clusters["discovery"]
            linked_facts.extend([f["text"] for f in disc_facts[:3]])
        
        # 3. –§–∞–∫—Ç—ã –æ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è—Ö
        if "consequence" in fact_clusters:
            cons_facts = fact_clusters["consequence"]
            if cons_facts:
                linked_facts.append(f"üîó –ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è: {cons_facts[0]['text']}")
        
        # 4. –û–±—â–∏–µ —Ñ–∞–∫—Ç—ã
        if "general" in fact_clusters:
            gen_facts = fact_clusters["general"]
            linked_facts.extend([f["text"] for f in gen_facts[:5]])
        
        # 5. –ï—Å–ª–∏ –º–∞–ª–æ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤, –¥–æ–±–∞–≤–ª—è–µ–º –ª—É—á—à–∏–µ –∏–∑ –≤—Å–µ—Ö
        if len(linked_facts) < 8:
            all_facts_sorted = sorted(facts, key=lambda x: len(x["text"]), reverse=True)
            additional_facts = [f["text"] for f in all_facts_sorted[:10] 
                              if f["text"] not in linked_facts]
            linked_facts.extend(additional_facts[:8-len(linked_facts)])
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
        key_entities = self._extract_key_entities_from_facts(facts)
        
        # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —Å–ø–æ—Ä–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã
        controversial = self._find_controversial_points(facts)
        
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        timeline_data = self._create_timeline_data(facts)
        
        # –î–∞–Ω–Ω—ã–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        comparison_data = self._create_comparison_data(facts)
        
        return {
            "linked_facts": linked_facts[:15],  # –ë–æ–ª—å—à–µ —Ñ–∞–∫—Ç–æ–≤
            "fact_clusters": [{k: len(v)} for k, v in fact_clusters.items()],
            "timeline_data": timeline_data,
            "comparison_data": comparison_data,
            "key_entities": key_entities,
            "controversial_points": controversial
        }
    
    def _link_statistical_facts(self, stats_facts):
        """–°–≤—è–∑—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∞–∫—Ç—ã"""
        if len(stats_facts) < 2:
            return [f["text"] for f in stats_facts]
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Å—Ö–æ–∂–∏–º —á–∏—Å–ª–∞–º
        number_groups = defaultdict(list)
        
        for fact in stats_facts:
            numbers = re.findall(r'\d+\.?\d*', fact["text"])
            for num in numbers[:2]:
                if float(num) > 1:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –º–∞–ª–µ–Ω—å–∫–∏–µ —á–∏—Å–ª–∞
                    key = f"{float(num):.1f}"
                    number_groups[key].append(fact)
        
        linked = []
        for num, facts in number_groups.items():
            if len(facts) >= 2:
                # –ù–∞—Ö–æ–¥–∏–º –æ–±—â—É—é —Ç–µ–º—É
                domains = set(f["domain"] for f in facts)
                sources_info = f"({len(facts)} –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {', '.join(list(domains)[:2])})"
                best_fact = max(facts, key=lambda f: len(f["text"]))
                linked.append(f"üìä {best_fact['text']} {sources_info}")
            else:
                linked.append(f"üìä {facts[0]['text']}")
        
        return linked[:5]
    
    def _extract_key_entities_from_facts(self, facts):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ —Ñ–∞–∫—Ç–æ–≤"""
        all_text = " ".join([f["text"] for f in facts])
        
        # –ò–º–µ–Ω–∞ –∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
        entities = re.findall(r'[–ê-–Ø–Å][–∞-—è—ë]+\s+[–ê-–Ø–Å][–∞-—è—ë]+|[–ê-–Ø–Å][–ê-–Ø–Å–∞-—è—ë]+\s+(?:—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç|–∏–Ω—Å—Ç–∏—Ç—É—Ç)', all_text)
        
        # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ —á–∞—Å—Ç–æ—Ç–µ
        entity_counter = Counter(entities)
        return [entity for entity, count in entity_counter.most_common(15)]
    
    def _find_controversial_points(self, facts):
        """–ù–∞—Ö–æ–¥–∏—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —Å–ø–æ—Ä–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã"""
        if len(facts) < 3:
            return []
        
        # –ò—â–µ–º —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è —Å –º–æ–¥–∞–ª—å–Ω—ã–º–∏ –≥–ª–∞–≥–æ–ª–∞–º–∏ –∏ –æ—Ü–µ–Ω–æ—á–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
        controversial_patterns = [
            r'–≤–µ—Ä–æ—è—Ç–Ω–æ', r'–≤–æ–∑–º–æ–∂–Ω–æ', r'–ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ',
            r'—Å–ø–æ—Ä–Ω–æ', r'–¥–∏—Å–∫—É—Å—Å–∏–æ–Ω–Ω–æ', r'–ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤–æ',
            r'–Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Å—á–∏—Ç–∞—é—Ç', r'–ø–æ –º–Ω–µ–Ω–∏—é'
        ]
        
        controversial = []
        for fact in facts:
            fact_lower = fact["text"].lower()
            for pattern in controversial_patterns:
                if re.search(pattern, fact_lower):
                    controversial.append(f"üí¨ {fact['text'][:120]}...")
                    break
        
        return controversial[:4]
    
    def _create_timeline_data(self, facts):
        """–°–æ–∑–¥–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–∏–Ω–∏–∏"""
        timeline = []
        
        for fact in facts:
            # –ò—â–µ–º –≥–æ–¥—ã –≤ —Ñ–∞–∫—Ç–∞—Ö
            years = re.findall(r'\b\d{4}\b', fact["text"])
            for year in years[:2]:
                if 1000 < int(year) < 2100:
                    # –£–ø—Ä–æ—â–∞–µ–º —Ñ–∞–∫—Ç –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ª–∏–Ω–∏–∏
                    clean_fact = re.sub(r'\([^)]*\)', '', fact["text"])
                    clean_fact = clean_fact[:80] + ("..." if len(clean_fact) > 80 else "")
                    timeline.append(f"{year}: {clean_fact}")
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≥–æ–¥—É
        timeline.sort(key=lambda x: int(re.search(r'\d{4}', x).group()))
        return timeline[:8]
    
    def _create_comparison_data(self, facts):
        """–°–æ–∑–¥–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        if len(facts) < 2:
            return []
        
        # –ò—â–µ–º —Ñ–∞–∫—Ç—ã —Å —á–∏—Å–ª–∞–º–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        comparison = []
        for i in range(min(3, len(facts))):
            for j in range(i+1, min(4, len(facts))):
                fact1, fact2 = facts[i], facts[j]
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–∞ –∏–∑ —Ñ–∞–∫—Ç–æ–≤
                nums1 = re.findall(r'\d+\.?\d*', fact1["text"])
                nums2 = re.findall(r'\d+\.?\d*', fact2["text"])
                
                if nums1 and nums2:
                    try:
                        num1 = float(nums1[0])
                        num2 = float(nums2[0])
                        if num1 > 0 and num2 > 0 and abs(num1 - num2) > 0.1:
                            ratio = max(num1, num2) / min(num1, num2)
                            if 1.5 < ratio < 10:  # –†–∞–∑—É–º–Ω–æ–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ
                                comparison.append(f"üìà –°—Ä–∞–≤–Ω–µ–Ω–∏–µ: {num1:.1f} vs {num2:.1f} (–≤ {ratio:.1f} —Ä–∞–∑)")
                    except ValueError:
                        continue
        
        return comparison[:4]
    
    def _merge_definitions(self, definitions):
        """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è"""
        if not definitions:
            return []
        
        # –£–±–∏—Ä–∞–µ–º –ø–æ—Ö–æ–∂–∏–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        unique_defs = []
        for def1 in definitions:
            is_duplicate = False
            for def2 in unique_defs:
                similarity = SequenceMatcher(None, def1.lower(), def2.lower()).ratio()
                if similarity > 0.7:  # –ë–æ–ª–µ–µ 70% —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                unique_defs.append(def1)
        
        return unique_defs
    
    def _merge_statistics(self, statistics):
        """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        if not statistics:
            return []
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø–∞–º
        percent_stats = [s for s in statistics if '%' in s]
        money_stats = [s for s in statistics if any(w in s.lower() for w in ['$', '–¥–æ–ª–ª–∞—Ä', '—Ä—É–±–ª', '–µ–≤—Ä–æ'])]
        other_stats = [s for s in statistics if s not in percent_stats and s not in money_stats]
        
        merged = []
        if percent_stats:
            merged.append(f"üìä –ü—Ä–æ—Ü–µ–Ω—Ç—ã: {', '.join(percent_stats[:3])}")
        if money_stats:
            merged.append(f"üí∞ –§–∏–Ω–∞–Ω—Å—ã: {', '.join(money_stats[:3])}")
        if other_stats:
            merged.extend(other_stats[:5])
        
        return merged
    
    def _calculate_source_coverage(self, sources_by_domain):
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ—Ö–≤–∞—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        total_sources = sum(len(sources) for sources in sources_by_domain.values())
        unique_domains = len(sources_by_domain)
        
        coverage = {
            "total_sources": total_sources,
            "unique_domains": unique_domains,
            "domain_distribution": {domain: len(sources) 
                                  for domain, sources in list(sources_by_domain.items())[:5]}
        }
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ
        if total_sources > 0:
            diversity_score = unique_domains / total_sources
            coverage["diversity_score"] = f"{diversity_score:.2f}"
            if diversity_score > 0.4:
                coverage["assessment"] = "‚úÖ –í—ã—Å–æ–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"
            elif diversity_score > 0.2:
                coverage["assessment"] = "‚ö†Ô∏è –°—Ä–µ–¥–Ω–µ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"
            else:
                coverage["assessment"] = "‚ùå –ù–∏–∑–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"
        
        return coverage

# ==================== –û–°–ù–û–í–ù–û–ô –ê–ù–ê–õ–ò–ó–ê–¢–û–† ====================
class InformationAnalyzer:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å –∞–≥—Ä–µ–≥–∞—Ü–∏–µ–π"""
    
    def __init__(self):
        self.aggregator = InformationAggregator()
        self.source_checker = SourceChecker()
    
    def analyze_topic(self, query, search_results):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–º—É —Å –∞–≥—Ä–µ–≥–∞—Ü–∏–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        items = search_results.get("items", [])
        
        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        aggregated_data = self.aggregator.aggregate_information(items, query)
        
        return {
            "topic": query,
            "type": self._determine_topic_type(query),
            "aggregated_data": aggregated_data,
            "timestamp": datetime.now().isoformat(),
            "quality_report": self._generate_quality_report(aggregated_data)
        }
    
    def _determine_topic_type(self, query):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø —Ç–µ–º—ã"""
        query_lower = query.lower()
        
        science_terms = ["—Ñ–∏–∑–∏–∫–∞", "—Ö–∏–º–∏—è", "–±–∏–æ–ª–æ–≥–∏—è", "–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞", "–Ω–∞—É–∫–∞", "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ"]
        tech_terms = ["—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è", "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "–∫–æ–º–ø—å—é—Ç–µ—Ä"]
        history_terms = ["–∏—Å—Ç–æ—Ä–∏—è", "–≤–æ–π–Ω–∞", "—Ä–µ–≤–æ–ª—é—Ü–∏—è", "–¥—Ä–µ–≤–Ω–∏–π", "—Å—Ä–µ–¥–Ω–µ–≤–µ–∫–æ–≤—å–µ"]
        
        if any(term in query_lower for term in science_terms):
            return "–Ω–∞—É—á–Ω–∞—è"
        elif any(term in query_lower for term in tech_terms):
            return "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è"
        elif any(term in query_lower for term in history_terms):
            return "–∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∞—è"
        
        return "–æ–±—â–∞—è"
    
    def _generate_quality_report(self, aggregated_data):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –æ –∫–∞—á–µ—Å—Ç–≤–µ"""
        coverage = aggregated_data.get("source_coverage", {})
        total_facts = aggregated_data.get("total_unique_facts", 0)
        
        report = []
        
        if total_facts > 0:
            report.append(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–∫—Ç–æ–≤: {total_facts}")
        
        if "total_sources" in coverage:
            report.append(f"üìö –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {coverage['total_sources']}")
        
        if "unique_domains" in coverage:
            report.append(f"üåê –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤: {coverage['unique_domains']}")
        
        if "assessment" in coverage:
            report.append(coverage["assessment"])
        
        if aggregated_data.get("controversial_points"):
            report.append(f"üí¨ –°–ø–æ—Ä–Ω—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤: {len(aggregated_data['controversial_points'])}")
        
        return "\n".join(report)

# ==================== –£–ú–ù–´–ô –ü–û–ò–°–ö ====================
class SmartGoogleSearch:
    def __init__(self):
        self.api_key = GOOGLE_API_KEY
        self.cse_id = GOOGLE_CSE_ID
        self.base_url = "https://www.googleapis.com/customsearch/v1"
        self.analyzer = InformationAnalyzer()
        
    def search_and_analyze(self, query):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞"""
        if not query or len(query.strip()) < 2:
            return {"error": "–ö–æ—Ä–æ—Ç–∫–∏–π –∑–∞–ø—Ä–æ—Å"}
        
        stats["google_searches"] += 1
        
        params = {
            "key": self.api_key,
            "cx": self.cse_id,
            "q": query,
            "num": 12,  # –£–≤–µ–ª–∏—á–∏–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            "hl": "ru",
            "lr": "lang_ru",
            "gl": "ru"
        }
        
        try:
            response = requests.get(self.base_url, params=params, timeout=15)
            
            if response.status_code != 200:
                return self._create_fallback(query)
            
            data = response.json()
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞
            structured_info = self.analyzer.analyze_topic(query, data)
            
            return {
                "success": True,
                "query": query,
                "structured_info": structured_info,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return self._create_fallback(query)
    
    def _create_fallback(self, query):
        """–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π fallback"""
        return {
            "success": False,
            "query": query,
            "structured_info": {
                "topic": query,
                "type": "–æ–±—â–∞—è",
                "aggregated_data": {
                    "linked_facts": ["–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Ç—Ä–µ–±—É–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ –Ω–∞–¥–µ–∂–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º"],
                    "definitions": [],
                    "statistics": [],
                    "key_entities": [query.capitalize()],
                    "total_unique_facts": 0,
                    "source_coverage": {"total_sources": 0, "unique_domains": 0}
                },
                "quality_report": "‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ—Ç –ø–æ–∏—Å–∫–æ–≤–∏–∫–∞"
            },
            "fallback": True
        }

# ==================== –ì–ï–ù–ï–†–ê–¢–û–† –ö–û–ù–°–ü–ï–ö–¢–û–í ====================
class SmartConspectGenerator:
    def __init__(self):
        self.searcher = SmartGoogleSearch()
        logger.info("‚úÖ –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≥–æ—Ç–æ–≤ —Å –∞–≥—Ä–µ–≥–∞—Ü–∏–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏")
    
    def generate(self, topic, volume="extended"):  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω—Å–ø–µ–∫—Ç —Å –∞–≥—Ä–µ–≥–∞—Ü–∏–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        if self._is_easter_egg(topic):
            return self._create_easter_egg_response()
        
        search_results = self.searcher.search_and_analyze(topic)
        structured_info = search_results.get("structured_info", {})
        aggregated_data = structured_info.get("aggregated_data", {})
        quality_report = structured_info.get("quality_report", "")
        
        if volume == "detailed":
            return self._generate_detailed(topic, aggregated_data, quality_report)
        elif volume == "short":
            return self._generate_short(topic, aggregated_data, quality_report)
        else:
            return self._generate_extended(topic, aggregated_data, quality_report)
    
    def _is_easter_egg(self, text):
        text_lower = text.lower()
        return "–ø–∞—Å—Ö–∞–ª–∫–∞" in text_lower
    
    def _create_easter_egg_response(self):
        return "ü•ö *–ü–∞—Å—Ö–∞–ª–∫–∞ –Ω–∞–π–¥–µ–Ω–∞!* –ë–æ—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—É—é –∞–≥—Ä–µ–≥–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö."
    
    def _generate_short(self, topic, data, quality_report):
        """–ö—Ä–∞—Ç–∫–æ - –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã"""
        facts = data.get("linked_facts", [])
        
        if not facts:
            return f"üìå *{topic}*\n\n{quality_report}\n\n–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
        
        conspect = f"üìå *{topic}*\n\n{quality_report}\n\n"
        
        # –õ—É—á—à–∏–µ —Ñ–∞–∫—Ç—ã
        for i, fact in enumerate(facts[:6], 1):
            conspect += f"{i}. {fact}\n"
        
        # –ö–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        entities = data.get("key_entities", [])
        if entities:
            conspect += f"\nüîë *–ö–ª—é—á–µ–≤—ã–µ –ø–æ–Ω—è—Ç–∏—è:* {', '.join(entities[:4])}\n"
        
        return conspect
    
    def _generate_detailed(self, topic, data, quality_report):
        """–ü–æ–¥—Ä–æ–±–Ω–æ - —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"""
        conspect = f"üìö *{topic}*\n\n{quality_report}\n\n"
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∞–∫—Ç—ã
        facts = data.get("linked_facts", [])
        if facts:
            conspect += "üéØ *–û—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∞–∫—Ç—ã:*\n\n"
            for i, fact in enumerate(facts[:10], 1):
                conspect += f"{i}. {fact}\n"
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        definitions = data.get("definitions", [])
        if definitions:
            conspect += f"\nüìñ *–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è:*\n\n"
            for definition in definitions[:4]:
                conspect += f"‚Ä¢ {definition}\n"
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        statistics = data.get("statistics", [])
        if statistics:
            conspect += f"\nüìä *–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:*\n\n"
            for stat in statistics[:6]:
                conspect += f"‚Ä¢ {stat}\n"
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        timeline = data.get("timeline_data", [])
        if timeline:
            conspect += f"\nüïí *–•—Ä–æ–Ω–æ–ª–æ–≥–∏—è:*\n\n"
            for event in timeline[:4]:
                conspect += f"‚Ä¢ {event}\n"
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö
        coverage = data.get("source_coverage", {})
        if "total_sources" in coverage:
            conspect += f"\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
            conspect += f"üìö –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {coverage['total_sources']} | "
            conspect += f"üåê –î–æ–º–µ–Ω–æ–≤: {coverage.get('unique_domains', 0)}"
        
        return conspect
    
    def _generate_extended(self, topic, data, quality_report):
        """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ - –≤—Å—è –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"""
        conspect = f"üî¨ *–ü–û–õ–ù–´–ô –ê–ù–ê–õ–ò–ó: {topic}*\n\n{quality_report}\n\n"
        
        # –í–í–ï–î–ï–ù–ò–ï
        conspect += "="*50 + "\n"
        conspect += "–í–í–ï–î–ï–ù–ò–ï –ò –ú–ï–¢–û–î–û–õ–û–ì–ò–Ø\n"
        conspect += "="*50 + "\n\n"
        
        coverage = data.get("source_coverage", {})
        conspect += f"*–ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è:* –ê–≥—Ä–µ–≥–∞—Ü–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ {coverage.get('total_sources', 0)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤\n"
        conspect += f"*–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–∫—Ç–æ–≤:* {data.get('total_unique_facts', 0)}\n"
        conspect += f"*–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤:* {stats.get('duplicates_removed', 0)}\n"
        conspect += f"*–í—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞:* {datetime.now().strftime('%H:%M')}\n\n"
        
        # –û–°–ù–û–í–ù–´–ï –§–ê–ö–¢–´
        conspect += "="*50 + "\n"
        conspect += "–û–°–ù–û–í–ù–´–ï –§–ê–ö–¢–´ –ò –î–ê–ù–ù–´–ï\n"
        conspect += "="*50 + "\n\n"
        
        facts = data.get("linked_facts", [])
        if facts:
            for i, fact in enumerate(facts[:15], 1):
                conspect += f"{i}. {fact}\n\n"
        else:
            conspect += "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ —Ç–µ–º–µ —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è\n\n"
        
        # –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ò –¶–ò–§–†–´
        statistics = data.get("statistics", [])
        if statistics:
            conspect += "="*50 + "\n"
            conspect += "–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ò –ß–ò–°–õ–û–í–´–ï –î–ê–ù–ù–´–ï\n"
            conspect += "="*50 + "\n\n"
            
            for stat in statistics:
                conspect += f"‚Ä¢ {stat}\n"
            conspect += "\n"
        
        # –û–ü–†–ï–î–ï–õ–ï–ù–ò–Ø –ò –ü–û–ù–Ø–¢–ò–Ø
        definitions = data.get("definitions", [])
        if definitions:
            conspect += "="*50 + "\n"
            conspect += "–û–ü–†–ï–î–ï–õ–ï–ù–ò–Ø –ò –ö–õ–Æ–ß–ï–í–´–ï –ü–û–ù–Ø–¢–ò–Ø\n"
            conspect += "="*50 + "\n\n"
            
            for i, definition in enumerate(definitions, 1):
                conspect += f"{i}. {definition}\n\n"
        
        # –•–†–û–ù–û–õ–û–ì–ò–Ø
        timeline = data.get("timeline_data", [])
        if timeline:
            conspect += "="*50 + "\n"
            conspect += "–•–†–û–ù–û–õ–û–ì–ò–ß–ï–°–ö–ò–ï –î–ê–ù–ù–´–ï\n"
            conspect += "="*50 + "\n\n"
            
            for event in timeline:
                conspect += f"‚Ä¢ {event}\n"
            conspect += "\n"
        
        # –ö–õ–Æ–ß–ï–í–´–ï –°–£–©–ù–û–°–¢–ò
        entities = data.get("key_entities", [])
        if entities:
            conspect += "="*50 + "\n"
            conspect += "–ö–õ–Æ–ß–ï–í–´–ï –°–£–©–ù–û–°–¢–ò –ò –û–†–ì–ê–ù–ò–ó–ê–¶–ò–ò\n"
            conspect += "="*50 + "\n\n"
            
            for i, entity in enumerate(entities[:12], 1):
                conspect += f"{i}. {entity}\n"
            conspect += "\n"
        
        # –°–†–ê–í–ù–ï–ù–ò–Ø –ò –ê–ù–ê–õ–ò–ó
        comparison = data.get("comparison_data", [])
        if comparison:
            conspect += "="*50 + "\n"
            conspect += "–°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó\n"
            conspect += "="*50 + "\n\n"
            
            for comp in comparison:
                conspect += f"‚Ä¢ {comp}\n"
            conspect += "\n"
        
        # –°–ü–û–†–ù–´–ï –ú–û–ú–ï–ù–¢–´
        controversial = data.get("controversial_points", [])
        if controversial:
            conspect += "="*50 + "\n"
            conspect += "–°–ü–û–†–ù–´–ï –ò –î–ò–°–ö–£–°–°–ò–û–ù–ù–´–ï –ú–û–ú–ï–ù–¢–´\n"
            conspect += "="*50 + "\n\n"
            
            for point in controversial:
                conspect += f"‚Ä¢ {point}\n"
            conspect += "\n"
        
        # –ê–ù–ê–õ–ò–ó –ò–°–¢–û–ß–ù–ò–ö–û–í
        conspect += "="*50 + "\n"
        conspect += "–ê–ù–ê–õ–ò–ó –ò–°–¢–û–ß–ù–ò–ö–û–í –ò –î–û–°–¢–û–í–ï–†–ù–û–°–¢–ò\n"
        conspect += "="*50 + "\n\n"
        
        conspect += f"*–í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:* {coverage.get('total_sources', 0)}\n"
        conspect += f"*–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤:* {coverage.get('unique_domains', 0)}\n"
        conspect += f"*–°–≤—è–∑–µ–π –º–µ–∂–¥—É —Ñ–∞–∫—Ç–∞–º–∏:* {len(facts)}\n"
        conspect += f"*–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤:* {stats.get('duplicates_removed', 0)}\n\n"
        
        if "domain_distribution" in coverage:
            conspect += "*–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –¥–æ–º–µ–Ω–∞–º:*\n"
            for domain, count in coverage["domain_distribution"].items():
                conspect += f"‚Ä¢ {domain}: {count} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤\n"
        
        # –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï
        conspect += "\n" + "="*50 + "\n"
        conspect += "–ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï –ò –í–´–í–û–î–´\n"
        conspect += "="*50 + "\n\n"
        
        total_facts = data.get("total_unique_facts", 0)
        if total_facts >= 10:
            conspect += "‚úÖ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Ö–æ—Ä–æ—à–æ –æ—Å–≤–µ—â–µ–Ω–∞ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö\n"
            conspect += "‚úÖ –ù–∞–π–¥–µ–Ω—ã —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ñ–∞–∫—Ç—ã\n"
            conspect += "‚úÖ –í—ã—è–≤–ª–µ–Ω—ã –∫–ª—é—á–µ–≤—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ –∏ —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—è\n"
        elif total_facts >= 5:
            conspect += "‚ö†Ô∏è –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ\n"
            conspect += "‚ö†Ô∏è –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º\n"
        else:
            conspect += "‚ùå –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞\n"
            conspect += "‚ùå –¢—Ä–µ–±—É—é—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è\n"
        
        conspect += f"\nü§ñ *@Konspekt_help_bot* | üß† *–ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö* | üïí {datetime.now().strftime('%d.%m.%Y')}"
        
        return conspect

# ==================== TELEGRAM BOT ====================
class TelegramBot:
    def __init__(self):
        if not TELEGRAM_TOKEN:
            raise ValueError("TELEGRAM_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        self.token = TELEGRAM_TOKEN
        self.bot_url = f"https://api.telegram.org/bot{self.token}"
        self.generator = SmartConspectGenerator()
        
        if RENDER_EXTERNAL_URL:
            self._setup_webhook()
        
        logger.info("‚úÖ Telegram –±–æ—Ç –≥–æ—Ç–æ–≤ —Å –∞–≥—Ä–µ–≥–∞—Ü–∏–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏")
    
    def _setup_webhook(self):
        webhook_url = f"{RENDER_EXTERNAL_URL}/webhook"
        try:
            response = requests.post(
                f"{self.bot_url}/setWebhook",
                json={"url": webhook_url},
                timeout=10
            )
            if response.json().get("ok"):
                logger.info(f"‚úÖ –í–µ–±—Ö—É–∫: {webhook_url}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–µ–±—Ö—É–∫–∞: {e}")
    
    def process_message(self, chat_id, text):
        text = text.strip()
        self._update_stats(chat_id)
        
        if text.startswith("/"):
            if text == "/start":
                return self._send_welcome(chat_id)
            elif text == "/help":
                return self._send_help(chat_id)
            elif text == "/stats":
                return self._send_stats(chat_id)
            elif text == "/quality":
                return self._send_quality_info(chat_id)
            else:
                return self._send_message(chat_id, "‚ùì –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞")
        
        if text in ["1", "2", "3"]:
            return self._handle_volume(chat_id, text)
        
        return self._handle_topic(chat_id, text)
    
    def _send_welcome(self, chat_id):
        welcome = (
            "ü§ñ *–ë–æ—Ç —Å –∞–≥—Ä–µ–≥–∞—Ü–∏–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏*\n\n"
            "üîç *–ß—Ç–æ –Ω–æ–≤–æ–≥–æ:*\n"
            "‚Ä¢ ‚úÖ –°–æ–±–∏—Ä–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —Å 12+ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤\n"
            "‚Ä¢ ‚úÖ –£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏\n"
            "‚Ä¢ ‚úÖ –°–≤—è–∑—ã–≤–∞–µ—Ç —Ä–æ–¥—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ñ–∞–∫—Ç—ã\n"
            "‚Ä¢ ‚úÖ –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –≤ 3 —Ä–∞–∑–∞ –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö\n\n"
            "üìä *–£—Ä–æ–≤–Ω–∏ –∞–Ω–∞–ª–∏–∑–∞:*\n"
            "‚Ä¢ 1 ‚Äî –ö–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã (6+ –ø—É–Ω–∫—Ç–æ–≤)\n"
            "‚Ä¢ 2 ‚Äî –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è\n"
            "‚Ä¢ 3 ‚Äî –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π\n\n"
            "üìå –ü—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—à–∏—Ç–µ —Ç–µ–º—É"
        )
        return self._send_message(chat_id, welcome)
    
    def _send_help(self, chat_id):
        help_text = (
            "üîç *–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –∞–≥—Ä–µ–≥–∞—Ü–∏—è:*\n\n"
            "1. *–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö:* –ë–æ—Ç –ø—Ä–æ–≤–µ—Ä—è–µ—Ç 12+ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤\n"
            "2. *–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è:* –£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –Ω–µ–Ω–∞–¥–µ–∂–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç\n"
            "3. *–ê–≥—Ä–µ–≥–∞—Ü–∏—è:* –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Ä–∞–∑–Ω—ã—Ö –º–µ—Å—Ç\n"
            "4. *–°–≤—è–∑—ã–≤–∞–Ω–∏–µ:* –ù–∞—Ö–æ–¥–∏—Ç —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Ñ–∞–∫—Ç–∞–º–∏\n"
            "5. *–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ:* –°–æ–∑–¥–∞–µ—Ç —á–µ—Ç–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É\n\n"
            "üìä *–ü—Ä–∏–º–µ—Ä –¥–ª—è —Ç–µ–º—ã '–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç':*\n"
            "‚Ä¢ –†–∞–Ω—å—à–µ: 4-5 —Ä–∞–∑—Ä–æ–∑–Ω–µ–Ω–Ω—ã—Ö —Ñ–∞–∫—Ç–∞\n"
            "‚Ä¢ –°–µ–π—á–∞—Å: 10-15 —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤ + —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ + —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—è\n\n"
            "üìå *–ü–æ–ø—Ä–æ–±—É–π—Ç–µ:* '–ò—Å—Ç–æ—Ä–∏—è –†–∏–º–∞', '–ö–≤–∞–Ω—Ç–æ–≤–∞—è —Ñ–∏–∑–∏–∫–∞', '–≠–∫–æ–Ω–æ–º–∏–∫–∞ –ö–∏—Ç–∞—è'"
        )
        return self._send_message(chat_id, help_text)
    
    def _send_stats(self, chat_id):
        stat_text = (
            f"üìä *–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏:*\n\n"
            f"üë• –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {stats['total_users']}\n"
            f"üí¨ –°–æ–æ–±—â–µ–Ω–∏–π: {stats['total_messages']}\n"
            f"üìÑ –ö–æ–Ω—Å–ø–µ–∫—Ç–æ–≤: {stats['conspects_created']}\n"
            f"üîç –ü–æ–∏—Å–∫–æ–≤: {stats['google_searches']}\n"
            f"‚úÖ –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–æ —Ñ–∞–∫—Ç–æ–≤: {stats['aggregated_facts']}\n"
            f"üö´ –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {stats['duplicates_removed']}\n"
            f"üìà –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å: {stats['aggregated_facts']/(stats['google_searches']*10+1):.1f} —Ñ–∞–∫—Ç–æ–≤/–ø–æ–∏—Å–∫"
        )
        return self._send_message(chat_id, stat_text)
    
    def _send_quality_info(self, chat_id):
        info = (
            "üî¨ *–°–∏—Å—Ç–µ–º–∞ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏:*\n\n"
            "*–≠—Ç–∞–ø—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏:*\n"
            "1. –°–±–æ—Ä —Å 12+ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤\n"
            "2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞\n"
            "3. –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (—Ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ)\n"
            "4. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ñ–∞–∫—Ç–æ–≤ –ø–æ —Ç–∏–ø–∞–º\n"
            "5. –°–≤—è–∑—ã–≤–∞–Ω–∏–µ —Ä–æ–¥—Å—Ç–≤–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏\n"
            "6. –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —á–∏—Ç–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç\n\n"
            "*–ß—Ç–æ –∏—Å–∫–ª—é—á–∞–µ—Ç—Å—è:*\n"
            "‚Ä¢ –î—É–±–ª–∏—Ä—É—é—â–∞—è—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è\n"
            "‚Ä¢ –ö–æ–Ω—Ç–µ–Ω—Ç —Å –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–∞–π—Ç–æ–≤\n"
            "‚Ä¢ –ù–µ–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω—ã–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è\n\n"
            "üìå –†–µ–∑—É–ª—å—Ç–∞—Ç: –≤ 3 —Ä–∞–∑–∞ –±–æ–ª—å—à–µ –ø–æ–ª–µ–∑–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"
        )
        return self._send_message(chat_id, info)
    
    def _handle_topic(self, chat_id, topic):
        user_id = str(chat_id)
        if user_id not in stats["user_states"]:
            stats["user_states"][user_id] = {}
        
        stats["user_states"][user_id]["pending_topic"] = topic
        
        response = (
            f"üéØ *–¢–µ–º–∞: {topic}*\n\n"
            f"üîç *–ë—É–¥–µ—Ç —Å–æ–±—Ä–∞–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å 12+ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤*\n\n"
            f"üìä *–£—Ä–æ–≤–µ–Ω—å –∞–Ω–∞–ª–∏–∑–∞:*\n\n"
            f"1Ô∏è‚É£ –ö–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã (6+ –ø—É–Ω–∫—Ç–æ–≤)\n"
            f"2Ô∏è‚É£ –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è\n"
            f"3Ô∏è‚É£ –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π\n\n"
            f"–û—Ç–ø—Ä–∞–≤—å—Ç–µ 1, 2 –∏–ª–∏ 3"
        )
        return self._send_message(chat_id, response)
    
    def _handle_volume(self, chat_id, volume_choice):
        user_id = str(chat_id)
        user_state = stats["user_states"].get(user_id, {})
        topic = user_state.get("pending_topic", "")
        
        if not topic:
            return self._send_message(chat_id, "‚ùå –°–Ω–∞—á–∞–ª–∞ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ —Ç–µ–º—É")
        
        volume_map = {"1": "short", "2": "detailed", "3": "extended"}
        volume = volume_map.get(volume_choice, "short")
        
        # –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ
        self._send_message(chat_id, f"üîç *–°–æ–±–∏—Ä–∞—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ —Ç–µ–º–µ:* {topic}\nüìä –£—Ä–æ–≤–µ–Ω—å: {volume_choice}/3\n‚è≥ –≠—Ç–æ –∑–∞–π–º–µ—Ç 10-15 —Å–µ–∫—É–Ω–¥...")
        
        try:
            conspect = self.generator.generate(topic, volume)
            stats["conspects_created"] += 1
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º
            self._send_conspect_safely(chat_id, conspect)
            
            # –ö–æ—Ä–æ—Ç–∫–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ
            return self._send_message(chat_id, "‚úÖ *–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!*\n\n–ù–æ–≤–∞—è —Ç–µ–º–∞? –ü—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—à–∏—Ç–µ –µ—ë")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            return self._send_message(
                chat_id,
                f"‚ùå *–û—à–∏–±–∫–∞ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏*\n\n–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥—É—é —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –∏–ª–∏ —Ç–µ–º—É"
            )
    
    def _send_conspect_safely(self, chat_id, conspect):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Å–ø–µ–∫—Ç"""
        max_length = 4000
        
        if len(conspect) <= max_length:
            self._send_message(chat_id, conspect)
            return
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º
        sections = re.split(r'(={10,})', conspect)
        
        current = ""
        for section in sections:
            if re.match(r'={10,}', section):
                if current and len(current) > 1000:
                    self._send_message(chat_id, current.strip())
                    current = section + "\n\n"
                else:
                    current += section + "\n\n"
            else:
                if len(current + section) > max_length and current:
                    self._send_message(chat_id, current.strip())
                    current = section
                else:
                    current += section
        
        if current.strip():
            self._send_message(chat_id, current.strip())
    
    def _update_stats(self, chat_id):
        user_id = str(chat_id)
        
        if user_id not in stats["user_states"]:
            stats["total_users"] += 1
            stats["user_states"][user_id] = {
                "first_seen": datetime.now().isoformat(),
                "message_count": 0
            }
        
        stats["user_states"][user_id]["last_seen"] = datetime.now().isoformat()
        stats["user_states"][user_id]["message_count"] += 1
        stats["total_messages"] += 1
    
    def _send_message(self, chat_id, text):
        try:
            response = requests.post(
                f"{self.bot_url}/sendMessage",
                json={
                    "chat_id": chat_id,
                    "text": text,
                    "parse_mode": "Markdown",
                    "disable_web_page_preview": True
                },
                timeout=15
            )
            return response.json().get("ok", False)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {e}")
            return False

# ==================== HTTP –°–ï–†–í–ï–† ====================
class BotHTTPServer(BaseHTTPRequestHandler):
    def do_GET(self):
        path = self.path.split('?')[0]
        
        if path == "/":
            self._send_html(INDEX_HTML)
        elif path == "/health":
            self._send_json({"status": "ok", "time": datetime.now().isoformat()})
        elif path == "/stats":
            self._send_json(stats)
        elif path == "/quality_info":
            info = {
                "aggregated_facts": stats.get("aggregated_facts", 0),
                "duplicates_removed": stats.get("duplicates_removed", 0),
                "total_searches": stats.get("google_searches", 0)
            }
            self._send_json(info)
        else:
            self.send_response(404)
            self.end_headers()
    
    def do_POST(self):
        if self.path == "/webhook":
            content_length = int(self.headers.get('Content-Length', 0))
            if content_length:
                try:
                    data = self.rfile.read(content_length)
                    update = json.loads(data.decode('utf-8'))
                    
                    threading.Thread(
                        target=self._handle_update,
                        args=(update,),
                        daemon=True
                    ).start()
                    
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–µ–±—Ö—É–∫–∞: {e}")
            
            self.send_response(200)
            self.end_headers()
            self.wfile.write(b'OK')
        else:
            self.send_response(404)
            self.end_headers()
    
    def _handle_update(self, update):
        try:
            if "message" in update and "text" in update["message"]:
                chat_id = update["message"]["chat"]["id"]
                text = update["message"]["text"]
                
                bot = TelegramBot()
                bot.process_message(chat_id, text)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
    
    def _send_html(self, content):
        self.send_response(200)
        self.send_header('Content-Type', 'text/html; charset=utf-8')
        self.end_headers()
        self.wfile.write(content.encode('utf-8'))
    
    def _send_json(self, data):
        self.send_response(200)
        self.send_header('Content-Type', 'application/json; charset=utf-8')
        self.end_headers()
        self.wfile.write(json.dumps(data, ensure_ascii=False, indent=2).encode('utf-8'))
    
    def log_message(self, format, *args):
        pass

INDEX_HTML = """<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>ü§ñ –ë–æ—Ç —Å –∞–≥—Ä–µ–≥–∞—Ü–∏–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; background: #f0f2f5; }
        .container { max-width: 800px; margin: 0 auto; background: white; padding: 25px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
        .status { color: green; font-weight: bold; padding: 10px; background: #e8f5e8; border-radius: 5px; }
        .features { display: grid; grid-template-columns: repeat(2, 1fr); gap: 15px; margin: 20px 0; }
        .feature { background: #f8f9fa; padding: 15px; border-radius: 8px; border-left: 4px solid #3b82f6; }
        .feature h4 { margin-top: 0; color: #1e40af; }
    </style>
</head>
<body>
    <div class="container">
        <h2>ü§ñ –ë–æ—Ç —Å –∞–≥—Ä–µ–≥–∞—Ü–∏–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏</h2>
        <p class="status">‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç —Å —Å–∏—Å—Ç–µ–º–æ–π –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –∏–∑ 12+ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤</p>
        
        <div class="features">
            <div class="feature">
                <h4>üìö –ê–≥—Ä–µ–≥–∞—Ü–∏—è</h4>
                <p>–°–æ–±–∏—Ä–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —Å 12+ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤</p>
            </div>
            <div class="feature">
                <h4>üîç –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è</h4>
                <p>–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –Ω–µ–Ω–∞–¥–µ–∂–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç</p>
            </div>
            <div class="feature">
                <h4>üîó –°–≤—è–∑—ã–≤–∞–Ω–∏–µ</h4>
                <p>–ù–∞—Ö–æ–¥–∏—Ç —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Ä–æ–¥—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Ñ–∞–∫—Ç–∞–º–∏</p>
            </div>
            <div class="feature">
                <h4>üìä –û–±—ä–µ–º</h4>
                <p>–í 3 —Ä–∞–∑–∞ –±–æ–ª—å—à–µ –ø–æ–ª–µ–∑–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏</p>
            </div>
        </div>
        
        <h3>üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã:</h3>
        <div id="stats">–ó–∞–≥—Ä—É–∑–∫–∞...</div>
        
        <h3>üîó –ë—ã—Å—Ç—Ä—ã–µ —Å—Å—ã–ª–∫–∏:</h3>
        <p><a href="https://t.me/Konspekt_help_bot" target="_blank">ü§ñ –û—Ç–∫—Ä—ã—Ç—å –±–æ—Ç–∞</a></p>
        <p><a href="/stats" target="_blank">üìà –ü–æ–ª–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ (JSON)</a></p>
        <p><a href="/quality_info" target="_blank">üî¨ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∞–≥—Ä–µ–≥–∞—Ü–∏–∏</a></p>
        
        <p style="color: #666; margin-top: 20px;">
            –û–±–Ω–æ–≤–ª–µ–Ω–æ: <span id="time"></span>
        </p>
    </div>
    
    <script>
        async function loadStats() {
            try {
                const response = await fetch('/stats');
                const data = await response.json();
                
                document.getElementById('stats').innerHTML = `
                    <p>üë• –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: ${data.total_users || 0}</p>
                    <p>üìÑ –ö–æ–Ω—Å–ø–µ–∫—Ç–æ–≤: ${data.conspects_created || 0}</p>
                    <p>üîç –ü–æ–∏—Å–∫–æ–≤: ${data.google_searches || 0}</p>
                    <p>‚úÖ –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–æ —Ñ–∞–∫—Ç–æ–≤: ${data.aggregated_facts || 0}</p>
                    <p>üö´ –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: ${data.duplicates_removed || 0}</p>
                `;
                
                document.getElementById('time').textContent = new Date().toLocaleTimeString();
            } catch (error) {
                document.getElementById('stats').innerHTML = '–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏';
            }
        }
        
        loadStats();
        setInterval(loadStats, 10000);
    </script>
</body>
</html>
"""

# ==================== –ó–ê–ü–£–°–ö ====================
def main():
    logger.info("=" * 60)
    logger.info("üöÄ –ó–ê–ü–£–°–ö –ë–û–¢–ê –° –ê–ì–†–ï–ì–ê–¶–ò–ï–ô –ò–ù–§–û–†–ú–ê–¶–ò–ò")
    logger.info("=" * 60)
    logger.info(f"üåê URL: {RENDER_EXTERNAL_URL}")
    logger.info(f"üö™ –ü–æ—Ä—Ç: {PORT}")
    logger.info("‚úÖ –†–µ–∂–∏–º: –ê–≥—Ä–µ–≥–∞—Ü–∏—è –∏–∑ 12+ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
    logger.info("‚úÖ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
    logger.info("‚úÖ –°–≤—è–∑—ã–≤–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏")
    logger.info("=" * 60)
    
    server = HTTPServer(('', PORT), BotHTTPServer)
    logger.info(f"‚úÖ HTTP —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω –Ω–∞ –ø–æ—Ä—Ç—É {PORT}")
    
    try:
        server.serve_forever()
    except KeyboardInterrupt:
        logger.info("‚èπÔ∏è –°–µ—Ä–≤–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    main()
