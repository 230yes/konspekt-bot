#!/usr/bin/env python3
"""
–£–º–Ω—ã–π Konspekt Helper Bot - –¢–û–õ–¨–ö–û –§–ê–ö–¢–´
–ë–æ—Ç –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ –ø—Ä–∏—Å—ã–ª–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Å—É—Ç—å –ø–æ –∑–∞–ø—Ä–æ—Å—É
"""

import os
import logging
import json
import requests
from datetime import datetime
from http.server import HTTPServer, BaseHTTPRequestHandler
import threading
import random
import re
import html

# ==================== –ù–ê–°–¢–†–û–ô–ö–ê ====================
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
GOOGLE_CSE_ID = os.getenv("GOOGLE_CSE_ID", "13aac457275834df9")
RENDER_EXTERNAL_URL = os.getenv("RENDER_EXTERNAL_URL", "")
PORT = int(os.getenv("PORT", 10000))

if not TELEGRAM_TOKEN:
    logger.error("‚ùå TELEGRAM_TOKEN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")
    exit(1)

# ==================== –°–¢–ê–¢–ò–°–¢–ò–ö–ê ====================
stats = {
    "total_users": 0,
    "total_messages": 0,
    "conspects_created": 0,
    "google_searches": 0,
    "start_time": datetime.now().isoformat(),
    "user_states": {}
}

# ==================== –£–ú–ù–´–ô –ê–ù–ê–õ–ò–ó–ê–¢–û–† –ò–ù–§–û–†–ú–ê–¶–ò–ò ====================
class InformationAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é - –¢–û–õ–¨–ö–û –§–ê–ö–¢–´"""
    
    def analyze_topic(self, query, search_results):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–º—É - –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ"""
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ç–µ–º—ã
        topic_type = self._determine_topic_type(query)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        analysis = self._analyze_search_results(search_results, query)
        
        return {
            "topic": query,
            "type": topic_type,
            "analysis": analysis,
            "timestamp": datetime.now().isoformat()
        }
    
    def _determine_topic_type(self, query):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø —Ç–µ–º—ã"""
        query_lower = query.lower()
        
        if any(word in query_lower for word in ["–∏—Å—Ç–æ—Ä–∏—è", "–≤–æ–π–Ω–∞", "—Ä–µ–≤–æ–ª—é—Ü–∏—è", "–¥—Ä–µ–≤–Ω–∏–π"]):
            return "–∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∞—è"
        elif any(word in query_lower for word in ["—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è", "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ", "–∫–æ–º–ø—å—é—Ç–µ—Ä", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç"]):
            return "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è"
        elif any(word in query_lower for word in ["–º–µ–¥–∏—Ü–∏–Ω–∞", "–∑–¥–æ—Ä–æ–≤—å–µ", "–±–æ–ª–µ–∑–Ω—å", "–ª–µ—á–µ–Ω–∏–µ"]):
            return "–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è"
        elif any(word in query_lower for word in ["—ç–∫–æ–Ω–æ–º–∏–∫–∞", "—Ñ–∏–Ω–∞–Ω—Å—ã", "—Ä—ã–Ω–æ–∫", "–±–∏–∑–Ω–µ—Å"]):
            return "—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∞—è"
        elif any(word in query_lower for word in ["–Ω–∞—É–∫–∞", "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ", "—Ç–µ–æ—Ä–∏—è", "—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç"]):
            return "–Ω–∞—É—á–Ω–∞—è"
        return "–æ–±—â–∞—è"
    
    def _analyze_search_results(self, results, query):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ - —Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã"""
        items = results.get("items", [])
        
        key_points = []
        statistics = []
        definitions = []
        sources = []
        
        for item in items[:8]:  # –ë–µ—Ä–µ–º –±–æ–ª—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            title = item.get("title", "")
            snippet = item.get("snippet", "")
            link = item.get("link", "")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ —Ñ–∞–∫—Ç—ã
            fact = self._extract_fact(title, snippet, query)
            if fact:
                key_points.append(fact)
                sources.append(link)
            
            # –¶–∏—Ñ—Ä—ã –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            numbers = self._extract_numbers(title + " " + snippet)
            statistics.extend(numbers)
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
            definition = self._extract_definition(title + " " + snippet)
            if definition:
                definitions.append(definition)
        
        # –¢–µ—Ä–º–∏–Ω—ã –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ
        key_terms = self._extract_terms_from_points(key_points)
        
        return {
            "key_points": key_points[:10],
            "statistics": statistics[:6],
            "definitions": definitions[:4],
            "key_terms": key_terms[:8],
            "total_sources": len(items),
            "sources": sources[:3]
        }
    
    def _extract_fact(self, title, snippet, query):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ñ–∞–∫—Ç –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        text = f"{title}. {snippet}"
        
        # –£–±–∏—Ä–∞–µ–º —Ä–µ–∫–ª–∞–º—É –∏ –º—É—Å–æ—Ä
        if self._is_junk(text):
            return None
        
        # –ù–∞—Ö–æ–¥–∏–º –Ω–∞–∏–±–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
        sentences = re.split(r'[.!?]+', text)
        
        for sentence in sentences:
            sentence = sentence.strip()
            if 30 < len(sentence) < 200:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
                if self._is_relevant_fact(sentence, query):
                    # –û—á–∏—â–∞–µ–º –æ—Ç –ª–∏—à–Ω–µ–≥–æ
                    clean_sentence = re.sub(r'\s+', ' ', sentence)
                    clean_sentence = clean_sentence[:180]
                    return clean_sentence
        
        return None
    
    def _is_junk(self, text):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –º—É—Å–æ—Ä–æ–º"""
        junk_phrases = [
            "–∫–ª–∏–∫–Ω–∏—Ç–µ", "–Ω–∞–∂–º–∏—Ç–µ", "–ø–æ–¥—Ä–æ–±–Ω–µ–µ", "—á–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ",
            "—É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ", "—Ä–µ–∫–ª–∞–º–∞", "sponsored", "advertisement",
            "–∫—É–ø–∏—Ç—å", "–∑–∞–∫–∞–∑–∞—Ç—å", "—Ü–µ–Ω–∞", "–∞–∫—Ü–∏—è", "—Å–∫–∏–¥–∫–∞"
        ]
        
        text_lower = text.lower()
        return any(phrase in text_lower for phrase in junk_phrases)
    
    def _is_relevant_fact(self, sentence, query):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Ñ–∞–∫—Ç–∞"""
        query_words = [word.lower() for word in query.split() if len(word) > 3]
        sentence_lower = sentence.lower()
        
        # –°—á–∏—Ç–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        matches = sum(1 for word in query_words if word in sentence_lower)
        
        # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª–∏–Ω–Ω—ã–º –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º
        return matches > 0 and len(sentence.split()) > 5
    
    def _extract_numbers(self, text):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —á–∏—Å–ª–∞ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        patterns = [
            r'\d+\.?\d*%',  # –ü—Ä–æ—Ü–µ–Ω—Ç—ã
            r'\d+\.?\d*\s*(?:–º–ª–Ω|–º–ª—Ä–¥|—Ç—ã—Å)',  # –° —á–∏—Å–ª–∞–º–∏
            r'\$\d+\.?\d*',  # –î–æ–ª–ª–∞—Ä—ã
            r'\d+\.?\d*\s*(?:–¥–æ–ª–ª–∞—Ä–æ–≤|—Ä—É–±–ª–µ–π|–µ–≤—Ä–æ)'
        ]
        
        numbers = []
        for pattern in patterns:
            matches = re.findall(pattern, text)
            numbers.extend(matches)
        
        return list(set(numbers))[:5]
    
    def _extract_definition(self, text):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è"""
        patterns = [
            r'—ç—Ç–æ\s+[^.!?]{10,100}[.!?]',
            r'—è–≤–ª—è–µ—Ç—Å—è\s+[^.!?]{10,100}[.!?]',
            r'–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è\s+–∫–∞–∫\s+[^.!?]{10,100}[.!?]',
            r'–ø–æ–¥\s+[^.!?]{5,20}\s+–ø–æ–Ω–∏–º–∞—é—Ç\s+[^.!?]{10,100}[.!?]'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                definition = match.group(0).strip()
                if 30 < len(definition) < 150:
                    return definition[:120] + "..."
        
        return None
    
    def _extract_terms_from_points(self, key_points):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫"""
        all_text = " ".join(key_points)
        words = re.findall(r'\b[–∞-—è—ë]{4,}\b', all_text.lower())
        
        # –£–±–∏—Ä–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = {"—ç—Ç–æ—Ç", "—Ç–∞–∫–æ–π", "–∫–∞–∫–æ–π", "–∫–æ—Ç–æ—Ä—ã–π", "–æ—á–µ–Ω—å", "–º–æ–∂–µ—Ç", "–±—É–¥–µ—Ç"}
        freq = {}
        
        for word in words:
            if word not in stop_words:
                freq[word] = freq.get(word, 0) + 1
        
        # –ë–µ—Ä–µ–º –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–µ
        sorted_terms = sorted(freq.items(), key=lambda x: x[1], reverse=True)
        return [term.capitalize() for term, count in sorted_terms[:12]]

# ==================== –£–ú–ù–´–ô –ü–û–ò–°–ö ====================
class SmartGoogleSearch:
    def __init__(self):
        self.api_key = GOOGLE_API_KEY
        self.cse_id = GOOGLE_CSE_ID
        self.base_url = "https://www.googleapis.com/customsearch/v1"
        self.analyzer = InformationAnalyzer()
        
    def search_and_analyze(self, query):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã"""
        if not query or len(query.strip()) < 2:
            return {"error": "–ö–æ—Ä–æ—Ç–∫–∏–π –∑–∞–ø—Ä–æ—Å"}
        
        stats["google_searches"] += 1
        
        params = {
            "key": self.api_key,
            "cx": self.cse_id,
            "q": query,
            "num": 8,
            "hl": "ru",
            "lr": "lang_ru",
            "gl": "ru"
        }
        
        try:
            response = requests.get(self.base_url, params=params, timeout=15)
            
            if response.status_code != 200:
                return self._create_fallback(query)
            
            data = response.json()
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã
            structured_info = self.analyzer.analyze_topic(query, data)
            
            return {
                "success": True,
                "query": query,
                "structured_info": structured_info,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return self._create_fallback(query)
    
    def _create_fallback(self, query):
        """–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π fallback"""
        return {
            "success": False,
            "query": query,
            "structured_info": {
                "topic": query,
                "type": "–æ–±—â–∞—è",
                "analysis": {
                    "key_points": [f"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ —Ç–µ–º–µ '{query}' —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è"],
                    "statistics": [],
                    "definitions": [],
                    "key_terms": [query.capitalize()],
                    "total_sources": 0,
                    "sources": []
                }
            },
            "fallback": True
        }

# ==================== –ì–ï–ù–ï–†–ê–¢–û–† –ö–û–ù–°–ü–ï–ö–¢–û–í - –¢–û–õ–¨–ö–û –§–ê–ö–¢–´ ====================
class SmartConspectGenerator:
    def __init__(self):
        self.searcher = SmartGoogleSearch()
        logger.info("‚úÖ –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≥–æ—Ç–æ–≤ - —Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã")
    
    def generate(self, topic, volume="short"):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω—Å–ø–µ–∫—Ç - —Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã"""
        # –ü–∞—Å—Ö–∞–ª–∫–∞
        if self._is_easter_egg(topic):
            return self._create_easter_egg_response()
        
        # –ü–æ–∏—Å–∫ –∏ –∞–Ω–∞–ª–∏–∑
        search_results = self.searcher.search_and_analyze(topic)
        structured_info = search_results.get("structured_info", {})
        analysis = structured_info.get("analysis", {})
        
        # –í –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –æ–±—ä–µ–º–∞
        if volume == "detailed":
            return self._generate_detailed(topic, analysis)
        elif volume == "extended":
            return self._generate_extended(topic, analysis)
        else:
            return self._generate_short(topic, analysis)
    
    def _is_easter_egg(self, text):
        text_lower = text.lower()
        return any(phrase in text_lower for phrase in [
            "–ø–ª–∞–Ω –∑–∞—Ö–≤–∞—Ç–∞ –ø–æ–ª—å—à–∏", "–∑–∞—Ö–≤–∞—Ç –ø–æ–ª—å—à–∏", "—á–∞–π–Ω–∞—è –ø–∞—Å—Ö–∞–ª–∫–∞"
        ])
    
    def _create_easter_egg_response(self):
        return "üçµ *–ü–∞—Å—Ö–∞–ª–∫–∞!* –ß–∞–π–Ω—ã–µ —Ü–µ—Ä–µ–º–æ–Ω–∏–∏ –∏–∑—É—á–∞—é—Ç—Å—è –≤ –∫—É–ª—å—Ç—É—Ä–æ–ª–æ–≥–∏–∏."
    
    def _generate_short(self, topic, analysis):
        """–ö—Ä–∞—Ç–∫–æ - —Ç–æ–ª—å–∫–æ —Å–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ"""
        key_points = analysis.get("key_points", [])
        
        if not key_points:
            return f"üìå *{topic}*\n\n–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
        
        conspect = f"üìå *{topic}*\n\n"
        
        # –¢–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã–µ —Ç–æ—á–∫–∏
        for i, point in enumerate(key_points[:4], 1):
            conspect += f"‚Ä¢ {point}\n"
        
        # –û–¥–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –µ—Å–ª–∏ –µ—Å—Ç—å
        statistics = analysis.get("statistics", [])
        if statistics:
            conspect += f"\nüìä {statistics[0]}\n"
        
        # –ö–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –µ—Å–ª–∏ –µ—Å—Ç—å
        terms = analysis.get("key_terms", [])
        if terms:
            conspect += f"üîë {', '.join(terms[:3])}\n"
        
        return conspect
    
    def _generate_detailed(self, topic, analysis):
        """–ü–æ–¥—Ä–æ–±–Ω–æ - —Ñ–∞–∫—Ç—ã + –¥–∞–Ω–Ω—ã–µ"""
        key_points = analysis.get("key_points", [])
        
        if not key_points:
            return f"üìö *{topic}*\n\n–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
        
        conspect = f"üìö *{topic}*\n\n"
        
        # –í—Å–µ –∫–ª—é—á–µ–≤—ã–µ —Ç–æ—á–∫–∏
        for i, point in enumerate(key_points[:8], 1):
            conspect += f"{i}. {point}\n"
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        statistics = analysis.get("statistics", [])
        if statistics:
            conspect += f"\nüìä *–î–∞–Ω–Ω—ã–µ:*\n"
            for stat in statistics[:4]:
                conspect += f"‚Ä¢ {stat}\n"
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        definitions = analysis.get("definitions", [])
        if definitions:
            conspect += f"\nüìñ *–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è:*\n"
            for definition in definitions[:3]:
                conspect += f"‚Ä¢ {definition}\n"
        
        # –¢–µ—Ä–º–∏–Ω—ã
        terms = analysis.get("key_terms", [])
        if terms:
            conspect += f"\nüîë *–¢–µ—Ä–º–∏–Ω—ã:* {', '.join(terms[:6])}\n"
        
        # –ò—Å—Ç–æ—á–Ω–∏–∫–∏
        sources = analysis.get("sources", [])
        if sources:
            conspect += f"\nüîç –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {analysis.get('total_sources', 0)}"
        
        return conspect
    
    def _generate_extended(self, topic, analysis):
        """–ü–æ–ª–Ω–æ - –≤—Å–µ —Ñ–∞–∫—Ç—ã"""
        key_points = analysis.get("key_points", [])
        
        if not key_points:
            return f"üî¨ *{topic}*\n\n–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
        
        conspect = f"üî¨ *{topic}*\n\n"
        
        # –í—Å–µ –∫–ª—é—á–µ–≤—ã–µ —Ç–æ—á–∫–∏
        for i, point in enumerate(key_points, 1):
            conspect += f"{i}. {point}\n"
        
        # –í—Å–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        statistics = analysis.get("statistics", [])
        if statistics:
            conspect += f"\nüìä *–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ —Ü–∏—Ñ—Ä—ã:*\n\n"
            for stat in statistics:
                conspect += f"‚Ä¢ {stat}\n"
        
        # –í—Å–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        definitions = analysis.get("definitions", [])
        if definitions:
            conspect += f"\nüìñ *–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏ –ø–æ–Ω—è—Ç–∏—è:*\n\n"
            for definition in definitions:
                conspect += f"‚Ä¢ {definition}\n"
        
        # –í—Å–µ —Ç–µ—Ä–º–∏–Ω—ã
        terms = analysis.get("key_terms", [])
        if terms:
            conspect += f"\nüî§ *–ö–ª—é—á–µ–≤–∞—è —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è:*\n\n"
            for i, term in enumerate(terms[:10], 1):
                conspect += f"{i}. {term}\n"
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–∏—Å–∫–µ
        conspect += f"\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
        conspect += f"üìà –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–∫—Ç–æ–≤: {len(key_points)}\n"
        conspect += f"üîç –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {analysis.get('total_sources', 0)}\n"
        conspect += f"üïí {datetime.now().strftime('%H:%M')}"
        
        return conspect

# ==================== TELEGRAM BOT ====================
class TelegramBot:
    def __init__(self):
        if not TELEGRAM_TOKEN:
            raise ValueError("TELEGRAM_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        self.token = TELEGRAM_TOKEN
        self.bot_url = f"https://api.telegram.org/bot{self.token}"
        self.generator = SmartConspectGenerator()
        
        if RENDER_EXTERNAL_URL:
            self._setup_webhook()
        
        logger.info("‚úÖ Telegram –±–æ—Ç –≥–æ—Ç–æ–≤ - —Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã")
    
    def _setup_webhook(self):
        webhook_url = f"{RENDER_EXTERNAL_URL}/webhook"
        try:
            response = requests.post(
                f"{self.bot_url}/setWebhook",
                json={"url": webhook_url},
                timeout=10
            )
            if response.json().get("ok"):
                logger.info(f"‚úÖ –í–µ–±—Ö—É–∫: {webhook_url}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–µ–±—Ö—É–∫–∞: {e}")
    
    def process_message(self, chat_id, text):
        text = text.strip()
        self._update_stats(chat_id)
        
        if text.startswith("/"):
            if text == "/start":
                return self._send_welcome(chat_id)
            elif text == "/help":
                return self._send_help(chat_id)
            elif text == "/stats":
                return self._send_stats(chat_id)
            else:
                return self._send_message(chat_id, "‚ùì –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞")
        
        if text in ["1", "2", "3"]:
            return self._handle_volume(chat_id, text)
        
        return self._handle_topic(chat_id, text)
    
    def _send_welcome(self, chat_id):
        welcome = (
            "ü§ñ *–ë–æ—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏*\n\n"
            "üìå –û—Ç–ø—Ä–∞–≤—å—Ç–µ —Ç–µ–º—É ‚Üí –ü–æ–ª—É—á–∏—Ç–µ —Ñ–∞–∫—Ç—ã\n\n"
            "üìä *–£—Ä–æ–≤–Ω–∏:*\n"
            "‚Ä¢ 1 ‚Äî –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–∑–∏—Å—ã\n"
            "‚Ä¢ 2 ‚Äî –§–∞–∫—Ç—ã + –¥–∞–Ω–Ω—ã–µ\n"
            "‚Ä¢ 3 ‚Äî –í—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è\n\n"
            "üöÄ –ü—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—à–∏—Ç–µ —Ç–µ–º—É"
        )
        return self._send_message(chat_id, welcome)
    
    def _send_help(self, chat_id):
        help_text = (
            "üìå *–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç:*\n"
            "1. –ü–∏—à–µ—Ç–µ —Ç–µ–º—É\n"
            "2. –í—ã–±–∏—Ä–∞–µ—Ç–µ 1, 2 –∏–ª–∏ 3\n"
            "3. –ü–æ–ª—É—á–∞–µ—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é\n\n"
            "üéØ *–ü—Ä–∏–º–µ—Ä—ã:*\n"
            "‚Ä¢ –ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç\n"
            "‚Ä¢ –ò—Å—Ç–æ—Ä–∏—è –†–∏–º–∞\n"
            "‚Ä¢ –ö–≤–∞–Ω—Ç–æ–≤–∞—è —Ñ–∏–∑–∏–∫–∞\n"
            "‚Ä¢ –≠–∫–æ–Ω–æ–º–∏–∫–∞ –ö–∏—Ç–∞—è\n\n"
            "ü§ñ –ë–æ—Ç –∏—â–µ—Ç —Ñ–∞–∫—Ç—ã –∏ –ø—Ä–∏—Å—ã–ª–∞–µ—Ç –∏—Ö"
        )
        return self._send_message(chat_id, help_text)
    
    def _send_stats(self, chat_id):
        stat_text = (
            f"üìä *–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:*\n\n"
            f"üë• –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {stats['total_users']}\n"
            f"üí¨ –°–æ–æ–±—â–µ–Ω–∏–π: {stats['total_messages']}\n"
            f"üìÑ –ö–æ–Ω—Å–ø–µ–∫—Ç–æ–≤: {stats['conspects_created']}\n"
            f"üîç –ü–æ–∏—Å–∫–æ–≤: {stats['google_searches']}"
        )
        return self._send_message(chat_id, stat_text)
    
    def _handle_topic(self, chat_id, topic):
        user_id = str(chat_id)
        if user_id not in stats["user_states"]:
            stats["user_states"][user_id] = {}
        
        stats["user_states"][user_id]["pending_topic"] = topic
        
        response = (
            f"üéØ *–¢–µ–º–∞: {topic}*\n\n"
            f"üìä –£—Ä–æ–≤–µ–Ω—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏:\n\n"
            f"1Ô∏è‚É£ –ö—Ä–∞—Ç–∫–∏–µ —Ç–µ–∑–∏—Å—ã\n"
            f"2Ô∏è‚É£ –§–∞–∫—Ç—ã + –¥–∞–Ω–Ω—ã–µ\n"
            f"3Ô∏è‚É£ –ü–æ–ª–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è\n\n"
            f"–û—Ç–ø—Ä–∞–≤—å—Ç–µ 1, 2 –∏–ª–∏ 3"
        )
        return self._send_message(chat_id, response)
    
    def _handle_volume(self, chat_id, volume_choice):
        user_id = str(chat_id)
        user_state = stats["user_states"].get(user_id, {})
        topic = user_state.get("pending_topic", "")
        
        if not topic:
            return self._send_message(chat_id, "‚ùå –°–Ω–∞—á–∞–ª–∞ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ —Ç–µ–º—É")
        
        volume_map = {"1": "short", "2": "detailed", "3": "extended"}
        volume = volume_map.get(volume_choice, "short")
        
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ
        self._send_message(chat_id, f"üîç –ò—â—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é...")
        
        try:
            conspect = self.generator.generate(topic, volume)
            stats["conspects_created"] += 1
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º
            self._send_conspect_safely(chat_id, conspect)
            
            # –ö–æ—Ä–æ—Ç–∫–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ
            return self._send_message(chat_id, "‚úÖ –ì–æ—Ç–æ–≤–æ! –ù–æ–≤–∞—è —Ç–µ–º–∞?")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            return self._send_message(
                chat_id,
                f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞\n–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥—É—é —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É"
            )
    
    def _send_conspect_safely(self, chat_id, conspect):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Å–ø–µ–∫—Ç"""
        max_length = 4000
        
        if len(conspect) <= max_length:
            self._send_message(chat_id, conspect)
            return
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∞–±–∑–∞—Ü–∞–º
        paragraphs = conspect.split('\n\n')
        
        current = ""
        for para in paragraphs:
            if len(current + para) > max_length and current:
                self._send_message(chat_id, current.strip())
                current = para
            else:
                if current:
                    current += "\n\n" + para
                else:
                    current = para
        
        if current.strip():
            self._send_message(chat_id, current.strip())
    
    def _update_stats(self, chat_id):
        user_id = str(chat_id)
        
        if user_id not in stats["user_states"]:
            stats["total_users"] += 1
            stats["user_states"][user_id] = {
                "first_seen": datetime.now().isoformat(),
                "message_count": 0
            }
        
        stats["user_states"][user_id]["last_seen"] = datetime.now().isoformat()
        stats["user_states"][user_id]["message_count"] += 1
        stats["total_messages"] += 1
    
    def _send_message(self, chat_id, text):
        try:
            response = requests.post(
                f"{self.bot_url}/sendMessage",
                json={
                    "chat_id": chat_id,
                    "text": text,
                    "parse_mode": "Markdown",
                    "disable_web_page_preview": True
                },
                timeout=15
            )
            return response.json().get("ok", False)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {e}")
            return False

# ==================== HTTP –°–ï–†–í–ï–† ====================
class BotHTTPServer(BaseHTTPRequestHandler):
    def do_GET(self):
        path = self.path.split('?')[0]
        
        if path == "/":
            self._send_html(INDEX_HTML)
        elif path == "/health":
            self._send_json({"status": "ok", "time": datetime.now().isoformat()})
        elif path == "/stats":
            self._send_json(stats)
        else:
            self.send_response(404)
            self.end_headers()
    
    def do_POST(self):
        if self.path == "/webhook":
            content_length = int(self.headers.get('Content-Length', 0))
            if content_length:
                try:
                    data = self.rfile.read(content_length)
                    update = json.loads(data.decode('utf-8'))
                    
                    threading.Thread(
                        target=self._handle_update,
                        args=(update,),
                        daemon=True
                    ).start()
                    
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–µ–±—Ö—É–∫–∞: {e}")
            
            self.send_response(200)
            self.end_headers()
            self.wfile.write(b'OK')
        else:
            self.send_response(404)
            self.end_headers()
    
    def _handle_update(self, update):
        try:
            if "message" in update and "text" in update["message"]:
                chat_id = update["message"]["chat"]["id"]
                text = update["message"]["text"]
                
                bot = TelegramBot()
                bot.process_message(chat_id, text)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
    
    def _send_html(self, content):
        self.send_response(200)
        self.send_header('Content-Type', 'text/html; charset=utf-8')
        self.end_headers()
        self.wfile.write(content.encode('utf-8'))
    
    def _send_json(self, data):
        self.send_response(200)
        self.send_header('Content-Type', 'application/json; charset=utf-8')
        self.end_headers()
        self.wfile.write(json.dumps(data, ensure_ascii=False, indent=2).encode('utf-8'))
    
    def log_message(self, format, *args):
        pass

INDEX_HTML = """<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>ü§ñ –ë–æ—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; background: #f0f2f5; }
        .container { max-width: 600px; margin: 0 auto; background: white; padding: 20px; border-radius: 8px; }
        .status { color: green; font-weight: bold; }
    </style>
</head>
<body>
    <div class="container">
        <h2>ü§ñ –ë–æ—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏</h2>
        <p class="status">‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç</p>
        <p>–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã –ø–æ –∑–∞–ø—Ä–æ—Å—É</p>
        
        <h3>üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:</h3>
        <div id="stats">–ó–∞–≥—Ä—É–∑–∫–∞...</div>
        
        <h3>üîó –°—Å—ã–ª–∫–∏:</h3>
        <p><a href="https://t.me/Konspekt_help_bot" target="_blank">ü§ñ –û—Ç–∫—Ä—ã—Ç—å –±–æ—Ç–∞</a></p>
        <p><a href="/stats" target="_blank">üìà JSON —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞</a></p>
        
        <p style="color: #666; margin-top: 20px;">
            –û–±–Ω–æ–≤–ª–µ–Ω–æ: <span id="time"></span>
        </p>
    </div>
    
    <script>
        async function loadStats() {
            try {
                const response = await fetch('/stats');
                const data = await response.json();
                
                document.getElementById('stats').innerHTML = `
                    <p>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: ${data.total_users || 0}</p>
                    <p>–°–æ–æ–±—â–µ–Ω–∏–π: ${data.total_messages || 0}</p>
                    <p>–ö–æ–Ω—Å–ø–µ–∫—Ç–æ–≤: ${data.conspects_created || 0}</p>
                    <p>–ü–æ–∏—Å–∫–æ–≤: ${data.google_searches || 0}</p>
                `;
                
                document.getElementById('time').textContent = new Date().toLocaleTimeString();
            } catch (error) {
                document.getElementById('stats').innerHTML = '–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏';
            }
        }
        
        loadStats();
        setInterval(loadStats, 10000);
    </script>
</body>
</html>
"""

# ==================== –ó–ê–ü–£–°–ö ====================
def main():
    logger.info("=" * 60)
    logger.info("üöÄ –ó–ê–ü–£–°–ö –ë–û–¢–ê - –¢–û–õ–¨–ö–û –§–ê–ö–¢–´")
    logger.info("=" * 60)
    logger.info(f"üåê URL: {RENDER_EXTERNAL_URL}")
    logger.info(f"üö™ –ü–æ—Ä—Ç: {PORT}")
    logger.info("‚úÖ –†–µ–∂–∏–º: –§–∞–∫—Ç—ã –±–µ–∑ —à–∞–±–ª–æ–Ω–æ–≤")
    logger.info("=" * 60)
    
    server = HTTPServer(('', PORT), BotHTTPServer)
    logger.info(f"‚úÖ HTTP —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω –Ω–∞ –ø–æ—Ä—Ç—É {PORT}")
    
    try:
        server.serve_forever()
    except KeyboardInterrupt:
        logger.info("‚èπÔ∏è –°–µ—Ä–≤–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    main()
