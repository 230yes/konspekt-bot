#!/usr/bin/env python3
"""
Konspekt Helper Bot - –° –∞–Ω–∞–ª–∏–∑–æ–º –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –∏ —Å–≤—è–∑–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
"""

import os
import logging
import json
import requests
from datetime import datetime
from http.server import HTTPServer, BaseHTTPRequestHandler
import threading
import random
import re
import hashlib
import time

# ==================== –ù–ê–°–¢–†–û–ô–ö–ê ====================
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
GOOGLE_CSE_ID = os.getenv("GOOGLE_CSE_ID", "13aac457275834df9")
RENDER_EXTERNAL_URL = os.getenv("RENDER_EXTERNAL_URL", "")
PORT = int(os.getenv("PORT", 10000))

if not TELEGRAM_TOKEN:
    logger.error("‚ùå TELEGRAM_TOKEN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")
    exit(1)

# ==================== –°–¢–ê–¢–ò–°–¢–ò–ö–ê ====================
stats = {
    "total_users": 0,
    "total_messages": 0,
    "conspects_created": 0,
    "google_searches": 0,
    "start_time": datetime.now().isoformat(),
    "user_states": {}
}

# ==================== –°–ò–°–¢–ï–ú–ê –û–¶–ï–ù–ö–ò –î–û–°–¢–û–í–ï–†–ù–û–°–¢–ò ====================
class ReliabilityAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    
    # –ù–∞–¥–µ–∂–Ω—ã–µ –¥–æ–º–µ–Ω—ã (–Ω–∞—É—á–Ω—ã–µ, –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ, –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ)
    RELIABLE_DOMAINS = {
        "wikipedia.org": 0.9,
        "arxiv.org": 0.95,
        "nature.com": 0.95,
        "science.org": 0.95,
        "springer.com": 0.9,
        "elsevier.com": 0.9,
        "ieee.org": 0.9,
        "acm.org": 0.9,
        "nasa.gov": 0.95,
        "nih.gov": 0.95,
        "who.int": 0.95,
        "un.org": 0.9,
        "gov": 0.85,  # –õ—é–±—ã–µ .gov —Å–∞–π—Ç—ã
        "edu": 0.85,  # –û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —É—á—Ä–µ–∂–¥–µ–Ω–∏—è
        "researchgate.net": 0.8,
        "ncbi.nlm.nih.gov": 0.95,
        "jstor.org": 0.9,
        "sciencedirect.com": 0.9,
        "google.com": 0.7,
        "youtube.com": 0.4,
        "reddit.com": 0.3,
        "twitter.com": 0.3,
        "facebook.com": 0.2,
        "tiktok.com": 0.2,
        "blogspot.com": 0.5,
        "wordpress.com": 0.5,
        "medium.com": 0.6,
        "quora.com": 0.4,
        "forum.": 0.3,
        "chat.": 0.2,
        "discord.": 0.2,
        "4chan.org": 0.1,
    }
    
    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –Ω–∞—É—á–Ω–æ—Å—Ç–∏
    SCIENTIFIC_INDICATORS = {
        "positive": [
            "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ", "—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç", "–∞–Ω–∞–ª–∏–∑", "–º–µ—Ç–æ–¥", "–º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è",
            "—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã", "–≤—ã–≤–æ–¥—ã", "–¥–∞–Ω–Ω—ã–µ", "—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞", "–≤—ã–±–æ—Ä–∫–∞",
            "–∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–∞—è –≥—Ä—É–ø–ø–∞", "–≥–∏–ø–æ—Ç–µ–∑–∞", "—Ç–µ–æ—Ä–∏—è", "–ø—É–±–ª–∏–∫–∞—Ü–∏—è",
            "—Ä–µ—Ü–µ–Ω–∑–∏—Ä–æ–≤–∞–Ω–∏–µ", "–∏–Ω–¥–µ–∫—Å —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è", "impact factor",
            "—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç", "–∏–Ω—Å—Ç–∏—Ç—É—Ç", "–ª–∞–±–æ—Ä–∞—Ç–æ—Ä–∏—è", "–ø—Ä–æ—Ñ–µ—Å—Å–æ—Ä",
            "–¥–æ–∫—Ç–æ—Ä –Ω–∞—É–∫", "–∫–∞–Ω–¥–∏–¥–∞—Ç –Ω–∞—É–∫", "–Ω–∞—É—á–Ω–∞—è —Å—Ç–∞—Ç—å—è",
            "–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è", "—Å–∏–º–ø–æ–∑–∏—É–º", "–ø–∞—Ç–µ–Ω—Ç", "–æ—Ç–∫—Ä—ã—Ç–∏–µ",
            "–¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ", "—ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–π", "—Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–π",
            "—Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π", "–∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π", "—Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π"
        ],
        "negative": [
            "—è –¥—É–º–∞—é", "–º–Ω–µ –∫–∞–∂–µ—Ç—Å—è", "–ø–æ –º–æ–µ–º—É –º–Ω–µ–Ω–∏—é", "–Ω–∞–≤–µ—Ä–Ω–æ–µ",
            "–≤–æ–∑–º–æ–∂–Ω–æ", "–≤–µ—Ä–æ—è—Ç–Ω–æ", "—Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ", "–∫–∞–∫ –±—ã",
            "—Ç–∏–ø–∞", "–≤—Ä–æ–¥–µ", "–±–ª—è", "—Ö—É–π–Ω—è", "–ø–∏–∑–¥–µ—Ü", "–æ—Ö—É–µ–Ω–Ω–æ",
            "–µ–±–∞—Ç—å", "–Ω–∞—Ö—É–π", "–ø–æ—Ö–æ–¥—É", "—á—ë", "—â–∞—Å", "–∞–≥–∞",
            "–ª–æ–ª", "–∫–µ–∫", "—Ä–æ—Ñ–ª", "–∏–º—Ö–æ", "–∏–º–µ—é –º–Ω–µ–Ω–∏–µ",
            "–≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –ø–∏—à—É—Ç", "–≥–æ–≤–æ—Ä—è—Ç", "—Å–ª—É—Ö–∏", "—Å–ø–ª–µ—Ç–Ω–∏",
            "–∫–æ–Ω—Å–ø–∏—Ä–æ–ª–æ–≥–∏—è", "–∑–∞–≥–æ–≤–æ—Ä", "–∏–Ω–æ–ø–ª–∞–Ω–µ—Ç—è–Ω–µ", "—Ä–µ–ø—Ç–∏–ª–æ–∏–¥—ã",
            "–º–∞—Å–æ–Ω—ã", "–∏–ª–ª—é–º–∏–Ω–∞—Ç—ã", "–≥–æ–ª–æ–≥—Ä–∞–º–º–∞", "—Ñ–µ–π–∫", "—Ñ–∞–ª—å—à–∏–≤–∫–∞"
        ]
    }
    
    def analyze_source(self, url, domain):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø–æ –¥–æ–º–µ–Ω—É"""
        reliability_score = 0.5  # –°—Ä–µ–¥–Ω–µ–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ —Å–ø–∏—Å–∫—É –Ω–∞–¥–µ–∂–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤
        for domain_pattern, score in self.RELIABLE_DOMAINS.items():
            if domain_pattern in domain or domain_pattern in url:
                reliability_score = max(reliability_score, score)
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫
        if reliability_score >= 0.8:
            category = "–Ω–∞—É—á–Ω—ã–π/–∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–π"
            color = "üü¢"
        elif reliability_score >= 0.6:
            category = "–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π/–Ω–æ–≤–æ—Å—Ç–Ω–æ–π"
            color = "üü°"
        elif reliability_score >= 0.4:
            category = "–±–ª–æ–≥/—Ñ–æ—Ä—É–º"
            color = "üü†"
        else:
            category = "—Å–æ–º–Ω–∏—Ç–µ–ª—å–Ω—ã–π/—Ä–∞–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π"
            color = "üî¥"
        
        return {
            "score": reliability_score,
            "category": category,
            "color": color,
            "domain": domain
        }
    
    def analyze_content_quality(self, text):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∏ –Ω–∞—É—á–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        text_lower = text.lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—É—á–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        scientific_score = 0
        for indicator in self.SCIENTIFIC_INDICATORS["positive"]:
            if indicator in text_lower:
                scientific_score += 1
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã (–±—Ä–µ–¥, —Å–ª–µ–Ω–≥, –º–Ω–µ–Ω–∏—è)
        unscientific_score = 0
        for indicator in self.SCIENTIFIC_INDICATORS["negative"]:
            if indicator in text_lower:
                unscientific_score += 2  # –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–æ –Ω–∞–∫–∞–∑—ã–≤–∞–µ–º
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–µ–∫—Å—Ç–∞
        has_numbers = bool(re.search(r'\d+', text))
        has_dates = bool(re.search(r'\d{4}', text))
        has_citations = bool(re.search(r'\[\d+\]|\([^)]+\)', text))
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å (–æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤)
        subjective_markers = ["—è", "–º–Ω–µ", "–º–æ–π", "–º–æ—ë", "–º—ã", "–Ω–∞–º", "–Ω–∞—à", "–Ω–∞—à–µ"]
        subjective_count = sum(1 for marker in subjective_markers if marker in text_lower)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∏—Ç–æ–≥–æ–≤—ã–π score
        quality_score = (
            (scientific_score * 0.3) -
            (unscientific_score * 0.4) +
            (has_numbers * 0.1) +
            (has_dates * 0.1) +
            (has_citations * 0.2) -
            (subjective_count * 0.1)
        )
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º score
        quality_score = max(0, min(1, 0.5 + quality_score / 10))
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å –∫–∞—á–µ—Å—Ç–≤–∞
        if quality_score >= 0.7:
            quality_level = "–≤—ã—Å–æ–∫–∏–π (–Ω–∞—É—á–Ω—ã–π)"
            emoji = "üéì"
        elif quality_score >= 0.5:
            quality_level = "—Å—Ä–µ–¥–Ω–∏–π (–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π)"
            emoji = "üìö"
        elif quality_score >= 0.3:
            quality_level = "–Ω–∏–∑–∫–∏–π (–ø–æ–ø—É–ª—è—Ä–Ω—ã–π)"
            emoji = "üì∞"
        else:
            quality_level = "—Å–æ–º–Ω–∏—Ç–µ–ª—å–Ω—ã–π (–Ω–µ–Ω–∞–¥–µ–∂–Ω—ã–π)"
            emoji = "‚ö†Ô∏è"
        
        return {
            "score": quality_score,
            "level": quality_level,
            "emoji": emoji,
            "scientific_count": scientific_score,
            "unscientific_count": unscientific_score,
            "has_numbers": has_numbers,
            "has_dates": has_dates,
            "has_citations": has_citations
        }
    
    def is_likely_bs(self, text):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –±—Ä–µ–¥–æ–º/–Ω–µ–Ω–∞—É—á–Ω—ã–º"""
        text_lower = text.lower()
        
        # –ö—Ä–∏—Ç–µ—Ä–∏–∏ –±—Ä–µ–¥–∞
        bs_indicators = [
            ("–∑–∞–≥–æ–≤–æ—Ä", 2),
            ("–∏–Ω–æ–ø–ª–∞–Ω–µ—Ç—è–Ω–µ", 3),
            ("—Ä–µ–ø—Ç–∏–ª–æ–∏–¥—ã", 3),
            ("–∏–ª–ª—é–º–∏–Ω–∞—Ç—ã", 3),
            ("–º–∞—Å–æ–Ω—ã", 2),
            ("–ª–∂–µ–Ω–∞—É–∫–∞", 3),
            ("—Ñ–µ–π–∫", 2),
            ("—Ñ–∞–ª—å—à–∏–≤–∫–∞", 2),
            ("–æ–±–º–∞–Ω", 2),
            ("–º–∏—Å—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è", 2),
            ("—ç–∫—Å—Ç—Ä–∞—Å–µ–Ω—Å", 2),
            ("–∞—Å—Ç—Ä–∞–ª", 2),
            ("—ç–Ω–µ—Ä–≥–∏—è –≤—Å–µ–ª–µ–Ω–Ω–æ–π", 2),
            ("—á–∏—Å—Ç–∫–∞ —á–∞–∫—Ä", 2),
            ("–≥–æ—Ä–æ—Å–∫–æ–ø", 1),
            ("–Ω—É–º–µ—Ä–æ–ª–æ–≥–∏—è", 1),
            ("—Ö–∏—Ä–æ–º–∞–Ω—Ç–∏—è", 1),
            ("–±–ª—è", 3),
            ("–ø–∏–∑–¥–µ—Ü", 3),
            ("–æ—Ö—É–µ–Ω–Ω–æ", 2),
            ("–µ–±–∞—Ç—å", 3),
            ("–Ω–∞—Ö—É–π", 3),
            ("—Ä–æ—Ñ–ª", 1),
            ("–∏–º—Ö–æ", 1),
            ("—Ç–∏–ø–∞", 1),
            ("–∫–∞–∫ –±—ã", 1),
            ("–≤—Ä–æ–¥–µ", 1),
            ("–ø–æ—Ö–æ–¥—É", 1)
        ]
        
        bs_score = 0
        for indicator, weight in bs_indicators:
            if indicator in text_lower:
                bs_score += weight
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
        subjective_words = ["—è –¥—É–º–∞—é", "–º–Ω–µ –∫–∞–∂–µ—Ç—Å—è", "–ø–æ –º–æ–µ–º—É –º–Ω–µ–Ω–∏—é", "–Ω–∞–≤–µ—Ä–Ω–æ–µ", "–≤–æ–∑–º–æ–∂–Ω–æ"]
        for word in subjective_words:
            if word in text_lower:
                bs_score += 1
        
        # –ï—Å–ª–∏ –º–Ω–æ–≥–æ –∑–∞–≥–ª–∞–≤–Ω—ã—Ö –±—É–∫–≤ (–∫—Ä–∏–∫) –∏–ª–∏ –º–Ω–æ–≥–æ –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤
        if (sum(1 for c in text if c.isupper()) / len(text) > 0.3) or text.count('!') > 3:
            bs_score += 2
        
        return bs_score >= 3  # –ü–æ—Ä–æ–≥ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–∞–∫ –±—Ä–µ–¥

# ==================== –£–ú–ù–´–ô –ü–û–ò–°–ö –° –ê–ù–ê–õ–ò–ó–û–ú ====================
class IntelligentSearchAPI:
    def __init__(self):
        self.api_key = GOOGLE_API_KEY
        self.cse_id = GOOGLE_CSE_ID
        self.base_url = "https://www.googleapis.com/customsearch/v1"
        self.analyzer = ReliabilityAnalyzer()
        self.cache = {}
        logger.info("‚úÖ –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ —Å –∞–Ω–∞–ª–∏–∑–æ–º –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –≥–æ—Ç–æ–≤")
    
    def search(self, query, num_results=8):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ —Å –∞–Ω–∞–ª–∏–∑–æ–º –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏"""
        if not query or len(query.strip()) < 2:
            return self._create_empty_result(query)
        
        cache_key = hashlib.md5(query.encode()).hexdigest()
        if cache_key in self.cache:
            logger.info(f"–ò—Å–ø–æ–ª—å–∑—É—é –∫—ç—à –¥–ª—è: {query}")
            return self.cache[cache_key]
        
        stats["google_searches"] += 1
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –Ω–∞—É—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        params = {
            "key": self.api_key,
            "cx": self.cse_id,
            "q": query,
            "num": min(num_results, 10),
            "hl": "ru",
            "lr": "lang_ru",
            "gl": "ru",
            "cr": "countryRU",
            "sort": "review"  # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        }
        
        try:
            logger.info(f"üîç –ü–æ–∏—Å–∫ —Å –∞–Ω–∞–ª–∏–∑–æ–º: {query}")
            response = requests.get(self.base_url, params=params, timeout=15)
            
            if response.status_code != 200:
                logger.warning(f"–û—à–∏–±–∫–∞ API: {response.status_code}")
                return self._create_intelligent_fallback(query)
            
            data = response.json()
            processed_results = self._process_with_analysis(data, query)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ–±—â—É—é –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            overall_analysis = self._analyze_overall_reliability(processed_results)
            
            result = {
                "success": True,
                "query": query,
                "raw_results": data.get("items", []),
                "processed_items": processed_results,
                "overall_analysis": overall_analysis,
                "search_info": data.get("searchInformation", {}),
                "timestamp": datetime.now().isoformat()
            }
            
            self.cache[cache_key] = result
            return result
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return self._create_intelligent_fallback(query)
    
    def _process_with_analysis(self, data, query):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –∞–Ω–∞–ª–∏–∑–æ–º –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏"""
        items = data.get("items", [])
        processed_items = []
        query_keywords = set(re.findall(r'\w{3,}', query.lower()))
        
        for item in items:
            title = item.get("title", "")
            snippet = item.get("snippet", "")
            link = item.get("link", "")
            display_link = item.get("displayLink", "")
            
            # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
            title = self._clean_text(title)
            snippet = self._clean_text(snippet)
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ
            if len(snippet) < 30:
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –±—Ä–µ–¥
            if self.analyzer.is_likely_bs(f"{title} {snippet}"):
                logger.info(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–∞–∫ –±—Ä–µ–¥: {title[:50]}...")
                continue
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫
            source_analysis = self.analyzer.analyze_source(link, display_link)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            content_analysis = self.analyzer.analyze_content_quality(f"{title} {snippet}")
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å—É
            relevance_score = self._calculate_relevance(title, snippet, query_keywords)
            
            # –û–±—â–∏–π score (–∏—Å—Ç–æ—á–Ω–∏–∫ + –∫–æ–Ω—Ç–µ–Ω—Ç + —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å)
            total_score = (
                source_analysis["score"] * 0.4 +
                content_analysis["score"] * 0.4 +
                relevance_score * 0.2
            )
            
            processed_items.append({
                "title": title,
                "snippet": snippet,
                "link": link,
                "source_domain": display_link,
                "source_analysis": source_analysis,
                "content_analysis": content_analysis,
                "relevance_score": relevance_score,
                "total_score": total_score,
                "processed_text": self._enhance_snippet(snippet, query),
                "key_facts": self._extract_key_facts(snippet),
                "is_scientific": content_analysis["score"] >= 0.6 and source_analysis["score"] >= 0.7
            })
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –æ–±—â–µ–º—É score (–ª—É—á—à–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏ –∫–æ–Ω—Ç–µ–Ω—Ç - –ø–µ—Ä–≤—ã–µ)
        processed_items.sort(key=lambda x: x["total_score"], reverse=True)
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–µ–º–∞–º/–∞—Å–ø–µ–∫—Ç–∞–º –¥–ª—è —Å–≤—è–∑–Ω–æ—Å—Ç–∏
        grouped_items = self._group_by_topics(processed_items, query)
        
        return grouped_items
    
    def _enhance_snippet(self, snippet, query):
        """–£–ª—É—á—à–∞–µ—Ç —Å–Ω–∏–ø–ø–µ—Ç, –¥–æ–±–∞–≤–ª—è—è –∫–æ–Ω—Ç–µ–∫—Å—Ç"""
        # –ù–∞—Ö–æ–¥–∏–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤ —Å–Ω–∏–ø–ø–µ—Ç–µ
        keywords = re.findall(r'\w{4,}', query.lower())
        enhanced = snippet
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–∞—Ä–∫–µ—Ä—ã –≤–∞–∂–Ω–æ—Å—Ç–∏
        for keyword in keywords[:3]:
            if keyword in snippet.lower():
                # –ü–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å
                enhanced = enhanced.replace(
                    keyword,
                    f"**{keyword}**",
                    1
                )
        
        return enhanced
    
    def _extract_key_facts(self, text):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        facts = []
        
        # –ò—â–µ–º —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è —Å —Ü–∏—Ñ—Ä–∞–º–∏
        number_patterns = [
            r'\d+%',  # –ü—Ä–æ—Ü–µ–Ω—Ç—ã
            r'\d+\s*[\-‚Äì]\s*\d+',  # –î–∏–∞–ø–∞–∑–æ–Ω—ã
            r'–±–æ–ª–µ–µ\s+\d+', r'–º–µ–Ω–µ–µ\s+\d+',  # –°—Ä–∞–≤–Ω–µ–Ω–∏—è
            r'\d+\s+(–≥–æ–¥|–ª–µ—Ç|–º–µ—Å—è—Ü|–¥–µ–Ω—å)',  # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–µ—Ä–∏–æ–¥—ã
        ]
        
        for pattern in number_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            facts.extend(matches)
        
        # –ò—â–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        definition_patterns = [
            r'‚Äî —ç—Ç–æ [^.]{10,50}\.',
            r'—è–≤–ª—è–µ—Ç—Å—è [^.]{10,50}\.',
            r'–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –∫–∞–∫ [^.]{10,50}\.'
        ]
        
        for pattern in definition_patterns:
            matches = re.findall(pattern, text)
            facts.extend(matches)
        
        return list(set(facts))[:5]  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏, –±–µ—Ä–µ–º –¥–æ 5 —Ñ–∞–∫—Ç–æ–≤
    
    def _calculate_relevance(self, title, snippet, query_keywords):
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å—É"""
        text = f"{title} {snippet}".lower()
        score = 0
        
        for keyword in query_keywords:
            if len(keyword) > 3:
                if keyword in text:
                    score += 1
                if keyword in title.lower():
                    score += 2
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º score
        max_possible = len(query_keywords) * 3
        if max_possible > 0:
            return min(1.0, score / max_possible)
        return 0.5
    
    def _group_by_topics(self, items, query):
        """–ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Ç–µ–º–∞–º/–∞—Å–ø–µ–∫—Ç–∞–º –¥–ª—è —Å–≤—è–∑–Ω–æ—Å—Ç–∏"""
        if not items:
            return items
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã —Ç–µ–º—ã
        aspects = self._identify_aspects(items, query)
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º items –ø–æ –∞—Å–ø–µ–∫—Ç–∞–º
        grouped = []
        for aspect in aspects:
            aspect_items = []
            for item in items[:5]:  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ª—É—á—à–∏–µ
                item_text = f"{item['title']} {item['snippet']}".lower()
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∞—Å–ø–µ–∫—Ç—É
                aspect_keywords = set(re.findall(r'\w{4,}', aspect.lower()))
                matches = sum(1 for kw in aspect_keywords if kw in item_text)
                
                if matches >= 1:  # –•–æ—Ç—è –±—ã –æ–¥–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                    aspect_items.append(item)
            
            if aspect_items:
                grouped.append({
                    "aspect": aspect,
                    "items": aspect_items[:3],  # –ù–µ –±–æ–ª–µ–µ 3 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–∞ –∞—Å–ø–µ–∫—Ç
                    "summary": self._generate_aspect_summary(aspect_items, aspect)
                })
        
        # –ï—Å–ª–∏ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
        if not grouped:
            return [{
                "aspect": "–û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è",
                "items": items[:4],
                "summary": self._generate_general_summary(items[:4])
            }]
        
        return grouped
    
    def _identify_aspects(self, items, query):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–ª—é—á–µ–≤—ã–µ –∞—Å–ø–µ–∫—Ç—ã —Ç–µ–º—ã"""
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç—ã
        all_text = " ".join([f"{i['title']} {i['snippet']}" for i in items[:5]])
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∞—Å—Ç—ã–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ (–ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã)
        words = re.findall(r'\b[–∞-—è—ë]{5,}\b', all_text.lower())
        
        # –ò—Å–∫–ª—é—á–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = {
            "–∫–æ—Ç–æ—Ä—ã–π", "–∫–æ—Ç–æ—Ä—ã–µ", "–∫–æ—Ç–æ—Ä—ã–µ", "—Ç–∞–∫–∂–µ", "–æ—á–µ–Ω—å",
            "–±—É–¥–µ—Ç", "–º–æ–∂–Ω–æ", "–Ω—É–∂–Ω–æ", "–¥–æ–ª–∂–µ–Ω", "–º–æ–≥—É—Ç"
        }
        
        word_freq = {}
        for word in words:
            if word not in stop_words:
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # –ë–µ—Ä–µ–º —Ç–æ–ø-5 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤
        top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:5]
        
        # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –≤ –∞—Å–ø–µ–∫—Ç—ã
        aspects = []
        for word, freq in top_words:
            if freq >= 2:  # –°–ª–æ–≤–æ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è —Ö–æ—Ç—è –±—ã 2 —Ä–∞–∑–∞
                aspects.append(word.capitalize())
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∞—Å–ø–µ–∫—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ
        if not aspects:
            aspects = [
                "–û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–Ω—è—Ç–∏—è",
                "–ö–ª—é—á–µ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏", 
                "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∏ –∑–Ω–∞—á–µ–Ω–∏–µ",
                "–¢–µ–Ω–¥–µ–Ω—Ü–∏–∏ —Ä–∞–∑–≤–∏—Ç–∏—è",
                "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏ –æ—Ç–∫—Ä—ã—Ç–∏—è"
            ]
        
        return aspects[:4]  # –ù–µ –±–æ–ª–µ–µ 4 –∞—Å–ø–µ–∫—Ç–æ–≤
    
    def _generate_aspect_summary(self, items, aspect):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–≤–æ–¥–∫—É –ø–æ –∞—Å–ø–µ–∫—Ç—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        if not items:
            return "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –¥–∞–Ω–Ω–æ–º—É –∞—Å–ø–µ–∫—Ç—É —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è."
        
        # –°–æ–±–∏—Ä–∞–µ–º –∫–ª—é—á–µ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        key_points = []
        for item in items[:3]:
            snippet = item.get("snippet", "")
            if len(snippet) > 50:
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
                sentences = re.split(r'[.!?]+', snippet)
                for sentence in sentences:
                    sentence = sentence.strip()
                    if len(sentence) > 30 and aspect.lower()[:10] in sentence.lower():
                        key_points.append(sentence[:150])
                        break
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏
        unique_points = []
        seen = set()
        for point in key_points:
            point_hash = hashlib.md5(point.lower().encode()).hexdigest()[:10]
            if point_hash not in seen:
                seen.add(point_hash)
                unique_points.append(point)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–≤–æ–¥–∫—É
        if unique_points:
            summary = f"–ü–æ –∞—Å–ø–µ–∫—Ç—É ¬´{aspect}¬ª —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —Å–æ–æ–±—â–∞—é—Ç: "
            summary += "; ".join([f"{i+1}) {p}" for i, p in enumerate(unique_points[:3])])
            summary += ". –≠—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Ç—Ä–µ–±—É–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º."
        else:
            summary = f"–ê—Å–ø–µ–∫—Ç ¬´{aspect}¬ª —É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è –≤ –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö, –Ω–æ —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è."
        
        return summary
    
    def _generate_general_summary(self, items):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–±—â—É—é —Å–≤–æ–¥–∫—É"""
        if not items:
            return "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ —Ç–µ–º–µ —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è."
        
        # –ë–µ—Ä–µ–º –∫–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã –∏–∑ –ª—É—á—à–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        summaries = []
        for item in items[:3]:
            if item.get("is_scientific", False):
                reliability = "üî¨ –ù–∞—É—á–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫: "
            else:
                reliability = "üì∞ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫: "
            
            snippet = item.get("snippet", "")[:100]
            if snippet:
                summaries.append(f"{reliability}{snippet}")
        
        if summaries:
            return "–û—Å–Ω–æ–≤–Ω—ã–µ –≤—ã–≤–æ–¥—ã –∏–∑ –∞–Ω–∞–ª–∏–∑–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:\n‚Ä¢ " + "\n‚Ä¢ ".join(summaries)
        
        return "–ü—Ä–æ–≤–µ–¥–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤—ã—è–≤–∏–ª —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –Ω–∞ —Ç–µ–º—É."
    
    def _analyze_overall_reliability(self, processed_items):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—â—É—é –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –≤—Å–µ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        if not processed_items:
            return {
                "reliability": "–Ω–∏–∑–∫–∞—è",
                "scientific_count": 0,
                "total_sources": 0,
                "avg_source_score": 0,
                "avg_content_score": 0
            }
        
        total_items = len(processed_items)
        scientific_count = sum(1 for item in processed_items if item.get("is_scientific", False))
        
        avg_source_score = sum(item["source_analysis"]["score"] for item in processed_items) / total_items
        avg_content_score = sum(item["content_analysis"]["score"] for item in processed_items) / total_items
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±—â—É—é –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å
        if scientific_count >= 3 and avg_source_score >= 0.7:
            reliability = "–≤—ã—Å–æ–∫–∞—è"
        elif scientific_count >= 1 and avg_source_score >= 0.5:
            reliability = "—Å—Ä–µ–¥–Ω—è—è"
        else:
            reliability = "–Ω–∏–∑–∫–∞—è"
        
        return {
            "reliability": reliability,
            "scientific_count": scientific_count,
            "total_sources": total_items,
            "avg_source_score": avg_source_score,
            "avg_content_score": avg_content_score,
            "warning": "‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ" if reliability == "–Ω–∏–∑–∫–∞—è" else None
        }
    
    def _clean_text(self, text):
        if not text:
            return ""
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def _create_intelligent_fallback(self, query):
        """–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π fallback —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ–º"""
        logger.info(f"üîÑ –ò—Å–ø–æ–ª—å–∑—É—é –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π fallback –¥–ª—è: {query}")
        
        return {
            "success": False,
            "query": query,
            "processed_items": [{
                "aspect": "–í–Ω–∏–º–∞–Ω–∏–µ: –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ",
                "items": [{
                    "title": f"–ê–Ω–∞–ª–∏–∑ —Ç–µ–º—ã: {query}",
                    "snippet": "‚ö†Ô∏è *–í–Ω–∏–º–∞–Ω–∏–µ:* –ü–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –≤–µ—Ä–Ω—É–ª–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–ø–æ–ª–Ω–æ–π –∏–ª–∏ —Ç—Ä–µ–±–æ–≤–∞—Ç—å –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ —Å–≤–µ–¥–µ–Ω–∏—è –∏ –æ–±—Ä–∞—â–∞—Ç—å—Å—è –∫ –Ω–∞—É—á–Ω—ã–º –ø—É–±–ª–∏–∫–∞—Ü–∏—è–º –¥–ª—è —É–≥–ª—É–±–ª–µ–Ω–Ω–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è.",
                    "source_analysis": {"score": 0.5, "category": "–ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ", "color": "üü°"},
                    "content_analysis": {"score": 0.5, "level": "–ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ", "emoji": "‚ö†Ô∏è"},
                    "is_scientific": False,
                    "key_facts": ["–¢—Ä–µ–±—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", "–ù–µ–æ–±—Ö–æ–¥–∏–º—ã –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏"]
                }],
                "summary": "–ò–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è —Å –æ–≥–æ–≤–æ—Ä–∫–∞–º–∏. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞."
            }],
            "overall_analysis": {
                "reliability": "–Ω–∏–∑–∫–∞—è",
                "scientific_count": 0,
                "warning": "‚ö†Ô∏è –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏!"
            },
            "timestamp": datetime.now().isoformat()
        }
    
    def _create_empty_result(self, query):
        return {
            "success": False,
            "query": query,
            "processed_items": [],
            "overall_analysis": {"reliability": "–Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞", "warning": "–ü—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å"},
            "timestamp": datetime.now().isoformat()
        }

# ==================== –ì–ï–ù–ï–†–ê–¢–û–† –ö–û–ù–°–ü–ï–ö–¢–û–í –° –°–í–Ø–ó–´–í–ê–ù–ò–ï–ú ====================
class IntelligentConspectGenerator:
    def __init__(self):
        self.searcher = IntelligentSearchAPI()
        logger.info("‚úÖ –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å –∞–Ω–∞–ª–∏–∑–æ–º –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –≥–æ—Ç–æ–≤")
    
    def generate(self, topic, volume="short"):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–≤—è–∑–Ω—ã–π –∫–æ–Ω—Å–ø–µ–∫—Ç —Å –∞–Ω–∞–ª–∏–∑–æ–º –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏"""
        if self._is_easter_egg(topic):
            return self._create_easter_egg_response()
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ —Å –∞–Ω–∞–ª–∏–∑–æ–º
        search_results = self.searcher.search(topic)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–æ–Ω—Å–ø–µ–∫—Ç —Å —É—á–µ—Ç–æ–º –æ–±—ä–µ–º–∞
        if volume == "detailed":
            return self._generate_detailed(topic, search_results)
        elif volume == "extended":
            return self._generate_extended(topic, search_results)
        else:
            return self._generate_short(topic, search_results)
    
    def _is_easter_egg(self, text):
        text_lower = text.lower()
        return any(phrase in text_lower for phrase in [
            "–ø–ª–∞–Ω –∑–∞—Ö–≤–∞—Ç–∞ –ø–æ–ª—å—à–∏", "–∑–∞—Ö–≤–∞—Ç –ø–æ–ª—å—à–∏", "—á–∞–π–Ω–∞—è –ø–∞—Å—Ö–∞–ª–∫–∞"
        ])
    
    def _create_easter_egg_response(self):
        responses = [
            "üçµ *–°–µ–∫—Ä–µ—Ç–Ω–∞—è –ø–∞—Å—Ö–∞–ª–∫–∞ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–∞!*\n\n–°—Ç–∞—Ç—É—Å: –ß–∞–π–Ω—ã–π –ê–Ω–∞–ª–∏—Ç–∏–∫\n–§–æ–∫—Å—è –≤ –ø—É—Ç–∏ —Å –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏...",
            "üçµ *Easter Egg Verified!*\n\nTea Status: ANALYTICAL\nFoksya delivering fact-checked info..."
        ]
        return random.choice(responses)
    
    def _generate_short(self, topic, results):
        """–ö—Ä–∞—Ç–∫–∏–π –∫–æ–Ω—Å–ø–µ–∫—Ç —Å –æ—Ü–µ–Ω–∫–æ–π –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏"""
        processed_items = results.get("processed_items", [])
        overall_analysis = results.get("overall_analysis", {})
        
        conspect = f"üìÑ *–ê–ù–ê–õ–ò–ó: {topic.upper()}*\n\n"
        
        # –û—Ü–µ–Ω–∫–∞ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏
        reliability = overall_analysis.get("reliability", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")
        reliability_emoji = {"–≤—ã—Å–æ–∫–∞—è": "üü¢", "—Å—Ä–µ–¥–Ω—è—è": "üü°", "–Ω–∏–∑–∫–∞—è": "üî¥"}.get(reliability, "‚ö™")
        
        conspect += f"{reliability_emoji} *–î–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å:* {reliability.upper()}\n"
        conspect += f"üî¨ *–ù–∞—É—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:* {overall_analysis.get('scientific_count', 0)}\n"
        conspect += f"üìä *–í—Å–µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:* {overall_analysis.get('total_sources', 0)}\n\n"
        
        # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –µ—Å–ª–∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –Ω–∏–∑–∫–∞—è
        if reliability == "–Ω–∏–∑–∫–∞—è":
            conspect += "‚ö†Ô∏è *–í–ù–ò–ú–ê–ù–ò–ï:* –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Ç—Ä–µ–±—É–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏!\n\n"
        
        # –ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã –∏–∑ —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        conspect += "üìù *–û–°–ù–û–í–ù–´–ï –í–´–í–û–î–´:*\n\n"
        
        if processed_items:
            for group in processed_items[:2]:  # –ë–µ—Ä–µ–º 2 –æ—Å–Ω–æ–≤–Ω—ã–µ –≥—Ä—É–ø–ø—ã
                aspect = group.get("aspect", "–û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è")
                summary = group.get("summary", "")
                
                if summary:
                    conspect += f"‚Ä¢ **{aspect}:** {summary}\n\n"
        else:
            conspect += "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ —Ç–µ–º–µ —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è –∏ –ø—Ä–æ–≤–µ—Ä–∫–∏.\n\n"
        
        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞
        conspect += "üß† *–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–¶–ï–ù–ö–ê:*\n"
        conspect += "1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏\n"
        conspect += "2. –ò—â–∏—Ç–µ –Ω–∞—É—á–Ω—ã–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏\n"
        conspect += "3. –°—Ä–∞–≤–Ω–∏—Ç–µ —Ä–∞–∑–Ω—ã–µ —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è\n"
        conspect += "4. –û—Å—Ç–µ—Ä–µ–≥–∞–π—Ç–µ—Å—å –Ω–µ–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n\n"
        
        conspect += "ü§ñ *@Konspekt_help_bot* | üîç *–ê–Ω–∞–ª–∏–∑ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏*"
        
        return conspect
    
    def _generate_detailed(self, topic, results):
        """–ü–æ–¥—Ä–æ–±–Ω—ã–π –∫–æ–Ω—Å–ø–µ–∫—Ç —Å –∞–Ω–∞–ª–∏–∑–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        processed_items = results.get("processed_items", [])
        overall_analysis = results.get("overall_analysis", {})
        
        conspect = f"üìö *–î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó: {topic.upper()}*\n\n"
        
        # –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –∏ –æ—Ü–µ–Ω–∫–∞
        conspect += "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
        conspect += "üî¨ *–ú–ï–¢–û–î–û–õ–û–ì–ò–Ø –ò –û–¶–ï–ù–ö–ê –î–û–°–¢–û–í–ï–†–ù–û–°–¢–ò*\n"
        conspect += "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\n"
        
        reliability = overall_analysis.get("reliability", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")
        reliability_color = {"–≤—ã—Å–æ–∫–∞—è": "üü¢", "—Å—Ä–µ–¥–Ω—è—è": "üü°", "–Ω–∏–∑–∫–∞—è": "üî¥"}.get(reliability, "‚ö™")
        
        conspect += f"*–û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏:* {reliability_color} **{reliability.upper()}**\n"
        conspect += f"*–ù–∞—É—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:* {overall_analysis.get('scientific_count', 0)} –∏–∑ {overall_analysis.get('total_sources', 0)}\n"
        conspect += f"*–°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:* {overall_analysis.get('avg_source_score', 0):.2f}/1.0\n"
        conspect += f"*–°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞:* {overall_analysis.get('avg_content_score', 0):.2f}/1.0\n\n"
        
        if overall_analysis.get("warning"):
            conspect += f"‚ö†Ô∏è **–í–ù–ò–ú–ê–ù–ò–ï:** {overall_analysis['warning']}\n\n"
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ –∞—Å–ø–µ–∫—Ç–∞–º (—Å–≤—è–∑–∞–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è)
        conspect += "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
        conspect += "üìä *–ê–ù–ê–õ–ò–ó –ü–û –ö–õ–Æ–ß–ï–í–´–ú –ê–°–ü–ï–ö–¢–ê–ú*\n"
        conspect += "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\n"
        
        if processed_items:
            for group in processed_items:
                aspect = group.get("aspect", "–ê—Å–ø–µ–∫—Ç")
                items = group.get("items", [])
                summary = group.get("summary", "")
                
                conspect += f"**{aspect}:**\n"
                conspect += f"{summary}\n\n"
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è —ç—Ç–æ–≥–æ –∞—Å–ø–µ–∫—Ç–∞
                if items:
                    conspect += "*–ò—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–æ —ç—Ç–æ–º—É –∞—Å–ø–µ–∫—Ç—É:*\n"
                    for i, item in enumerate(items[:2], 1):
                        source_info = item.get("source_analysis", {})
                        content_info = item.get("content_analysis", {})
                        
                        conspect += f"{i}. {source_info.get('color', '‚ö™')} "
                        conspect += f"{content_info.get('emoji', 'üìÑ')} "
                        conspect += f"{item.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')[:50]}...\n"
                        conspect += f"   *–î–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å:* {source_info.get('category', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')} "
                        conspect += f"({source_info.get('score', 0):.1f}/1.0)\n"
                        conspect += f"   *–ö–∞—á–µ—Å—Ç–≤–æ:* {content_info.get('level', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}\n\n"
                
                conspect += "‚Äï\n\n"
        else:
            conspect += "–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–¥–µ–ª–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ –∞—Å–ø–µ–∫—Ç—ã. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∞.\n\n"
        
        # –°–≤—è–∑—É—é—â–∏–π –∞–Ω–∞–ª–∏–∑
        conspect += "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n"
        conspect += "üîó *–°–í–Ø–ó–£–Æ–©–ò–ô –ê–ù–ê–õ–ò–ó –ò –í–´–í–û–î–´*\n"
        conspect += "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\n"
        
        conspect += "**–°–í–Ø–ó–¨ –ú–ï–ñ–î–£ –ò–°–¢–û–ß–ù–ò–ö–ê–ú–ò:**\n"
        
        if processed_items and len(processed_items) > 1:
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –º–µ–∂–¥—É –∞—Å–ø–µ–∫—Ç–∞–º–∏
            aspects = [g.get("aspect", "") for g in processed_items]
            conspect += f"–í—ã–¥–µ–ª–µ–Ω–æ {len(aspects)} –∫–ª—é—á–µ–≤—ã—Ö –∞—Å–ø–µ–∫—Ç–∞: {', '.join(aspects[:3])}.\n"
            conspect += "–ê–Ω–∞–ª–∏–∑ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ "
            
            # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å
            scientific_count = overall_analysis.get("scientific_count", 0)
            if scientific_count >= 2:
                conspect += "**—á–∞—Å—Ç–∏—á–Ω–æ —Å–æ–≥–ª–∞—Å—É–µ—Ç—Å—è** –º–µ–∂–¥—É –Ω–∞—É—á–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏.\n"
            elif scientific_count >= 1:
                conspect += "–∏–º–µ–µ—Ç **–ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è**, —Ç—Ä–µ–±—É—é—â–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏.\n"
            else:
                conspect += "—Ç—Ä–µ–±—É–µ—Ç **–∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏** –∏–∑-–∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è –Ω–∞—É—á–Ω—ã—Ö –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π.\n"
        else:
            conspect += "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∞, —Å–≤—è–∑—å –º–µ–∂–¥—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞.\n"
        
        conspect += "\n**–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –ü–†–û–í–ï–†–ö–ï:**\n"
        conspect += "1. –°—Ä–∞–≤–Ω–∏—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å –Ω–∞—É—á–Ω—ã–º–∏ –±–∞–∑–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö\n"
        conspect += "2. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç–Ω–æ—Å—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤\n"
        conspect += "3. –ò—â–∏—Ç–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –≤ —Ä–µ—Ü–µ–Ω–∑–∏—Ä—É–µ–º—ã—Ö –∂—É—Ä–Ω–∞–ª–∞—Ö\n"
        conspect += "4. –û—Å—Ç–µ—Ä–µ–≥–∞–π—Ç–µ—Å—å –Ω–µ–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω—ã—Ö —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π\n\n"
        
        conspect += "üîç *–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –∞–Ω–∞–ª–∏–∑ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤*\n"
        conspect += "ü§ñ *@Konspekt_help_bot* | üß† *–ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ*"
        
        return conspect
    
    def _generate_extended(self, topic, results):
        """–†–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –∫–æ–Ω—Å–ø–µ–∫—Ç —Å –ø–æ–ª–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º –∏ —Å–≤—è–∑—ã–≤–∞–Ω–∏–µ–º"""
        processed_items = results.get("processed_items", [])
        overall_analysis = results.get("overall_analysis", {})
        
        # –°–æ–∑–¥–∞–µ–º —á–∞—Å—Ç–∏ –∫–æ–Ω—Å–ø–µ–∫—Ç–∞
        parts = []
        
        # –ß–∞—Å—Ç—å 1: –í–≤–µ–¥–µ–Ω–∏–µ –∏ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è
        part1 = f"üìñ *–ö–û–ú–ü–õ–ï–ö–°–ù–û–ï –ò–°–°–õ–ï–î–û–í–ê–ù–ò–ï –° –ê–ù–ê–õ–ò–ó–û–ú –î–û–°–¢–û–í–ï–†–ù–û–°–¢–ò: {topic.upper()}*\n\n"
        part1 += "=" * 60 + "\n"
        part1 += "–ß–ê–°–¢–¨ 1: –ú–ï–¢–û–î–û–õ–û–ì–ò–Ø –ò –û–¶–ï–ù–ö–ê –î–û–°–¢–û–í–ï–†–ù–û–°–¢–ò\n"
        part1 += "=" * 60 + "\n\n"
        
        part1 += "**–¶–ï–õ–¨ –ò–°–°–õ–ï–î–û–í–ê–ù–ò–Ø:** –ü—Ä–æ–≤–µ—Å—Ç–∏ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–º—ã —Å –æ—Ü–µ–Ω–∫–æ–π –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.\n\n"
        
        reliability = overall_analysis.get("reliability", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")
        reliability_badge = {
            "–≤—ã—Å–æ–∫–∞—è": "üü¢ –í–´–°–û–ö–ê–Ø –î–û–°–¢–û–í–ï–†–ù–û–°–¢–¨",
            "—Å—Ä–µ–¥–Ω—è—è": "üü° –°–†–ï–î–ù–Ø–Ø –î–û–°–¢–û–í–ï–†–ù–û–°–¢–¨", 
            "–Ω–∏–∑–∫–∞—è": "üî¥ –ù–ò–ó–ö–ê–Ø –î–û–°–¢–û–í–ï–†–ù–û–°–¢–¨"
        }.get(reliability, "‚ö™ –ù–ï–û–ü–†–ï–î–ï–õ–ï–ù–ê")
        
        part1 += f"**–û–ë–©–ê–Ø –û–¶–ï–ù–ö–ê –î–û–°–¢–û–í–ï–†–ù–û–°–¢–ò:** {reliability_badge}\n\n"
        
        part1 += "**–ú–ï–¢–û–î–û–õ–û–ì–ò–Ø –ê–ù–ê–õ–ò–ó–ê:**\n"
        part1 += "1. –ü–æ–∏—Å–∫ –∏ —Å–±–æ—Ä –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤\n"
        part1 += "2. –û—Ü–µ–Ω–∫–∞ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–Ω–∞—É—á–Ω—ã–µ, –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ, –±–ª–æ–≥–∏, —Ñ–æ—Ä—É–º—ã)\n"
        part1 += "3. –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–Ω–∞—É—á–Ω–æ—Å—Ç—å, –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, —Å—Ç—Ä—É–∫—Ç—É—Ä–∞)\n"
        part1 += "4. –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –∞—Å–ø–µ–∫—Ç–∞–º\n"
        part1 += "5. –°–≤—è–∑—ã–≤–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤\n"
        part1 += "6. –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –∏ –≤—ã–≤–æ–¥—ã\n\n"
        
        part1 += f"**–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ò–°–¢–û–ß–ù–ò–ö–û–í:**\n"
        part1 += f"‚Ä¢ –í—Å–µ–≥–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {overall_analysis.get('total_sources', 0)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤\n"
        part1 += f"‚Ä¢ –ù–∞—É—á–Ω—ã—Ö/–∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏—Ö: {overall_analysis.get('scientific_count', 0)}\n"
        part1 += f"‚Ä¢ –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {overall_analysis.get('avg_source_score', 0):.2f}/1.0\n"
        part1 += f"‚Ä¢ –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {overall_analysis.get('avg_content_score', 0):.2f}/1.0\n\n"
        
        if overall_analysis.get("warning"):
            part1 += f"‚ö†Ô∏è **–í–ê–ñ–ù–û–ï –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï:** {overall_analysis['warning']}\n\n"
        
        parts.append(part1)
        
        # –ß–∞—Å—Ç—å 2: –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ –∞—Å–ø–µ–∫—Ç–∞–º
        part2 = "=" * 60 + "\n"
        part2 += "–ß–ê–°–¢–¨ 2: –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ü–û –¢–ï–ú–ê–¢–ò–ß–ï–°–ö–ò–ú –ê–°–ü–ï–ö–¢–ê–ú\n"
        part2 += "=" * 60 + "\n\n"
        
        if processed_items:
            for group_idx, group in enumerate(processed_items, 1):
                aspect = group.get("aspect", f"–ê—Å–ø–µ–∫—Ç {group_idx}")
                items = group.get("items", [])
                summary = group.get("summary", "")
                
                part2 += f"**{group_idx}. {aspect.upper()}**\n\n"
                part2 += f"*–°–≤–æ–¥–∫–∞ –∞–Ω–∞–ª–∏–∑–∞:* {summary}\n\n"
                
                # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è —ç—Ç–æ–≥–æ –∞—Å–ø–µ–∫—Ç–∞
                if items:
                    part2 += "*–ê–ù–ê–õ–ò–ó –ò–°–¢–û–ß–ù–ò–ö–û–í –ü–û –î–ê–ù–ù–û–ú–£ –ê–°–ü–ï–ö–¢–£:*\n\n"
                    
                    for i, item in enumerate(items, 1):
                        title = item.get("title", "")
                        snippet = item.get("snippet", "")
                        source_info = item.get("source_analysis", {})
                        content_info = item.get("content_analysis", {})
                        is_scientific = item.get("is_scientific", False)
                        
                        part2 += f"**–ò—Å—Ç–æ—á–Ω–∏–∫ {i}:** {title}\n"
                        part2 += f"*–¢–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞:* {source_info.get('color', '')} {source_info.get('category', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}\n"
                        part2 += f"*–û—Ü–µ–Ω–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞:* {source_info.get('score', 0):.2f}/1.0\n"
                        part2 += f"*–ö–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞:* {content_info.get('emoji', '')} {content_info.get('level', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}\n"
                        part2 += f"*–ù–∞—É—á–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫:* {'‚úÖ –î–∞' if is_scientific else '‚ùå –ù–µ—Ç'}\n"
                        
                        # –ö–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã –∏–∑ —ç—Ç–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                        key_facts = item.get("key_facts", [])
                        if key_facts:
                            part2 += f"*–ö–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã:* {', '.join(key_facts[:2])}\n"
                        
                        # –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
                        processed_text = item.get("processed_text", snippet[:200])
                        part2 += f"*–û—Å–Ω–æ–≤–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:* {processed_text}\n\n"
                
                part2 += "‚Äï" * 40 + "\n\n"
        else:
            part2 += "–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–¥–µ–ª–∏—Ç—å —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π.\n\n"
        
        parts.append(part2)
        
        # –ß–∞—Å—Ç—å 3: –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∏ —Å–≤—è–∑—É—é—â–∏–π –∞–Ω–∞–ª–∏–∑
        part3 = "=" * 60 + "\n"
        part3 += "–ß–ê–°–¢–¨ 3: –°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–´–ô –ò –°–í–Ø–ó–£–Æ–©–ò–ô –ê–ù–ê–õ–ò–ó\n"
        part3 += "=" * 60 + "\n\n"
        
        if processed_items and len(processed_items) > 1:
            part3 += "**–°–†–ê–í–ù–ï–ù–ò–ï –ò –°–í–Ø–ó–´–í–ê–ù–ò–ï –ò–ù–§–û–†–ú–ê–¶–ò–ò:**\n\n"
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –º–µ–∂–¥—É –∞—Å–ø–µ–∫—Ç–∞–º–∏
            aspects_info = []
            for group in processed_items:
                aspect = group.get("aspect", "")
                items = group.get("items", [])
                scientific_items = [i for i in items if i.get("is_scientific", False)]
                
                aspects_info.append({
                    "aspect": aspect,
                    "total_sources": len(items),
                    "scientific_sources": len(scientific_items),
                    "avg_score": sum(i.get("total_score", 0) for i in items) / len(items) if items else 0
                })
            
            part3 += "*–°–≤–æ–¥–∫–∞ –ø–æ –∞—Å–ø–µ–∫—Ç–∞–º:*\n"
            for info in aspects_info:
                part3 += f"‚Ä¢ **{info['aspect']}:** {info['total_sources']} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ "
                part3 += f"({info['scientific_sources']} –Ω–∞—É—á–Ω—ã—Ö), "
                part3 += f"—Å—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞: {info['avg_score']:.2f}/1.0\n"
            
            part3 += "\n*–ê–ù–ê–õ–ò–ó –°–û–ì–õ–ê–°–û–í–ê–ù–ù–û–°–¢–ò:*\n"
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
            scientific_counts = [info["scientific_sources"] for info in aspects_info]
            total_scientific = sum(scientific_counts)
            
            if total_scientific >= 3:
                part3 += "‚úÖ **–í—ã—Å–æ–∫–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å:** –ù–µ—Å–∫–æ–ª—å–∫–æ –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ —Ä–∞–∑–Ω—ã–º –∞—Å–ø–µ–∫—Ç–∞–º.\n"
            elif total_scientific >= 1:
                part3 += "‚ö†Ô∏è **–ß–∞—Å—Ç–∏—á–Ω–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å:** –ï—Å—Ç—å –Ω–∞—É—á–Ω—ã–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è, –Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–ø–æ–ª–Ω–æ–π.\n"
            else:
                part3 += "‚ùå **–ù–∏–∑–∫–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å:** –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–∞—É—á–Ω—ã–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è, –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Ç—Ä–µ–±—É–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏.\n"
            
            part3 += "\n*–°–í–Ø–ó–¨ –ú–ï–ñ–î–£ –ê–°–ü–ï–ö–¢–ê–ú–ò:*\n"
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–≤—è–∑–∏ –º–µ–∂–¥—É –∞—Å–ø–µ–∫—Ç–∞–º–∏
            if len(processed_items) >= 2:
                # –ò—â–µ–º –æ–±—â–∏–µ —Ç–µ–º—ã/–ø–æ–Ω—è—Ç–∏—è –º–µ–∂–¥—É –∞—Å–ø–µ–∫—Ç–∞–º–∏
                all_texts = []
                for group in processed_items[:3]:
                    for item in group.get("items", []):
                        all_texts.append(f"{item.get('title', '')} {item.get('snippet', '')}")
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–±—â–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
                common_terms = self._find_common_terms(all_texts)
                if common_terms:
                    part3 += f"–û–±—â–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –º–µ–∂–¥—É –∞—Å–ø–µ–∫—Ç–∞–º–∏: {', '.join(common_terms[:5])}\n"
                    part3 += "–≠—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫—É—é —Å–≤—è–∑—å –º–µ–∂–¥—É —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —á–∞—Å—Ç—è–º–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.\n"
                else:
                    part3 += "–Ø–≤–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –∞—Å–ø–µ–∫—Ç–∞–º–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ.\n"
            else:
                part3 += "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∞—Å–ø–µ–∫—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å–≤—è–∑–µ–π.\n"
            
        else:
            part3 += "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.\n"
        
        part3 += "\n**–ü–†–û–ë–õ–ï–ú–´ –ò –ü–†–û–¢–ò–í–û–†–ï–ß–ò–Ø:**\n"
        
        # –ò—â–µ–º –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è
        contradictions = self._find_contradictions(processed_items)
        if contradictions:
            for contradiction in contradictions[:3]:
                part3 += f"‚Ä¢ {contradiction}\n"
        else:
            part3 += "–Ø–≤–Ω—ã—Ö –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π –º–µ–∂–¥—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ.\n"
        
        parts.append(part3)
        
        # –ß–∞—Å—Ç—å 4: –í—ã–≤–æ–¥—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        part4 = "=" * 60 + "\n"
        part4 += "–ß–ê–°–¢–¨ 4: –í–´–í–û–î–´, –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ò –ü–ï–†–°–ü–ï–ö–¢–ò–í–´\n"
        part4 += "=" * 60 + "\n\n"
        
        part4 += "**–û–°–ù–û–í–ù–´–ï –í–´–í–û–î–´ –ò–°–°–õ–ï–î–û–í–ê–ù–ò–Ø:**\n\n"
        
        conclusions = [
            f"1. –î–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —Ç–µ–º–µ ¬´{topic}¬ª –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –∫–∞–∫ **{reliability}**",
            "2. –ö–∞—á–µ—Å—Ç–≤–æ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –≤–∞—Ä—å–∏—Ä—É–µ—Ç—Å—è –ø–æ —Ä–∞–∑–Ω—ã–º –∞—Å–ø–µ–∫—Ç–∞–º —Ç–µ–º—ã",
            "3. –ù–∞–ª–∏—á–∏–µ –Ω–∞—É—á–Ω—ã—Ö –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –≤–ª–∏—è–µ—Ç –Ω–∞ –æ—Ü–µ–Ω–∫—É –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏",
            "4. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Ç—Ä–µ–±—É–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è",
            "5. –î–ª—è —É–≥–ª—É–±–ª–µ–Ω–Ω–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏"
        ]
        
        for conclusion in conclusions:
            part4 += f"{conclusion}\n"
        
        part4 += "\n**–ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:**\n\n"
        
        recommendations = [
            "1. –í—Å–µ–≥–¥–∞ –ø—Ä–æ–≤–µ—Ä—è–π—Ç–µ –∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç–Ω–æ—Å—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏",
            "2. –û—Ç–¥–∞–≤–∞–π—Ç–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–µ –Ω–∞—É—á–Ω—ã–º –∏ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–º –ø—É–±–ª–∏–∫–∞—Ü–∏—è–º",
            "3. –°—Ä–∞–≤–Ω–∏–≤–∞–π—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤",
            "4. –û–±—Ä–∞—â–∞–π—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (–∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏)",
            "5. –ò—â–∏—Ç–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –≤ —Ä–µ—Ü–µ–Ω–∑–∏—Ä—É–µ–º—ã—Ö –Ω–∞—É—á–Ω—ã—Ö –∂—É—Ä–Ω–∞–ª–∞—Ö",
            "6. –û—Å—Ç–µ—Ä–µ–≥–∞–π—Ç–µ—Å—å —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –æ–∫—Ä–∞—à–µ–Ω–Ω—ã—Ö –∏ —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω—ã—Ö —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π",
            "7. –ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ —Ñ–∞–∫—Ç—ã —á–µ—Ä–µ–∑ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"
        ]
        
        for recommendation in recommendations:
            part4 += f"{recommendation}\n"
        
        part4 += "\n**–ü–ï–†–°–ü–ï–ö–¢–ò–í–´ –î–ê–õ–¨–ù–ï–ô–®–ï–ì–û –ò–°–°–õ–ï–î–û–í–ê–ù–ò–Ø:**\n\n"
        part4 += "‚Ä¢ –ü—Ä–æ–≤–µ—Å—Ç–∏ —É–≥–ª—É–±–ª–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–∞—É—á–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã –ø–æ —Ç–µ–º–µ\n"
        part4 += "‚Ä¢ –ò–∑—É—á–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ —ç–≤–æ–ª—é—Ü–∏—é —Ç–µ–º—ã\n"
        part4 += "‚Ä¢ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–π –æ–ø—ã—Ç –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è\n"
        part4 += "‚Ä¢ –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è\n"
        part4 += "‚Ä¢ –ò–∑—É—á–∏—Ç—å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏ –∏ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã —Ä–∞–∑–≤–∏—Ç–∏—è\n"
        
        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        part4 += f"\n" + "=" * 60 + "\n"
        part4 += "–¢–ï–•–ù–ò–ß–ï–°–ö–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø\n"
        part4 += "=" * 60 + "\n\n"
        
        part4 += f"*–î–∞—Ç–∞ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞:* {datetime.now().strftime('%d.%m.%Y %H:%M')}\n"
        part4 += f"*–û–±—ä–µ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è:* {sum(len(p) for p in parts)} —Å–∏–º–≤–æ–ª–æ–≤\n"
        part4 += f"*–ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è:* –ê–Ω–∞–ª–∏–∑ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ + —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–≤—è–∑—ã–≤–∞–Ω–∏–µ\n"
        part4 += f"*–°–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏:* –ù–∞—É—á–Ω–æ—Å—Ç—å √ó –ö–∞—á–µ—Å—Ç–≤–æ √ó –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å\n"
        part4 += f"*–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞:* @Konspekt_help_bot —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º\n\n"
        
        part4 += "‚ö†Ô∏è **–í–ê–ñ–ù–û:** –î–∞–Ω–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –Ω–æ—Å–∏—Ç –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä. "
        part4 += "–î–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è –≤–∞–∂–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –æ–±—Ä–∞—â–∞–π—Ç–µ—Å—å –∫ –ø–µ—Ä–≤–æ–∏—Å—Ç–æ—á–Ω–∏–∫–∞–º –∏ —ç–∫—Å–ø–µ—Ä—Ç–∞–º."
        
        parts.append(part4)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —á–∞—Å—Ç–∏
        full_conspect = "\n".join(parts)
        return full_conspect
    
    def _find_common_terms(self, texts, min_length=5):
        """–ù–∞—Ö–æ–¥–∏—Ç –æ–±—â–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –≤ —Ç–µ–∫—Å—Ç–∞—Ö"""
        if not texts:
            return []
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–∞ –∏–∑ –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–æ–≤
        all_words = []
        for text in texts:
            words = re.findall(r'\b[–∞-—è—ë]{4,}\b', text.lower())
            all_words.extend(words)
        
        # –°—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—É
        word_freq = {}
        for word in all_words:
            word_freq[word] = word_freq.get(word, 0) + 1
        
        # –ò—â–µ–º —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤–æ –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–∞—Ö
        common_words = []
        for word, freq in word_freq.items():
            if freq >= len(texts):  # –í—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤–æ –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–∞—Ö
                common_words.append(word)
        
        return sorted(common_words, key=lambda x: word_freq[x], reverse=True)[:10]
    
    def _find_contradictions(self, processed_items):
        """–ò—â–µ—Ç –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –º–µ–∂–¥—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏"""
        contradictions = []
        
        if not processed_items or len(processed_items) < 2:
            return contradictions
        
        # –°–æ–±–∏—Ä–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        all_claims = []
        for group in processed_items:
            for item in group.get("items", []):
                snippet = item.get("snippet", "")
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —É—Ç–≤–µ—Ä–¥–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                sentences = re.split(r'[.!?]+', snippet)
                for sentence in sentences:
                    sentence = sentence.strip()
                    if len(sentence) > 20 and not any(q in sentence.lower() for q in ["?", "–∫–∞–∫", "–ø–æ—á–µ–º—É", "–∑–∞—á–µ–º"]):
                        all_claims.append({
                            "text": sentence[:100],
                            "source_score": item.get("source_analysis", {}).get("score", 0),
                            "aspect": group.get("aspect", "")
                        })
        
        # –ò—â–µ–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è
        for i in range(len(all_claims)):
            for j in range(i + 1, len(all_claims)):
                claim1 = all_claims[i]["text"].lower()
                claim2 = all_claims[j]["text"].lower()
                
                # –ò—â–µ–º –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω—ã–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è
                opposites = [
                    ("—É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è", "—É–º–µ–Ω—å—à–∞–µ—Ç—Å—è"),
                    ("—Ä–∞—Å—Ç–µ—Ç", "–ø–∞–¥–∞–µ—Ç"),
                    ("—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ", "–Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ"),
                    ("–¥–æ–∫–∞–∑–∞–Ω–æ", "–æ–ø—Ä–æ–≤–µ—Ä–≥–Ω—É—Ç–æ"),
                    ("—Å—É—â–µ—Å—Ç–≤—É–µ—Ç", "–Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç"),
                    ("–≤–µ—Ä–Ω–æ", "–Ω–µ–≤–µ—Ä–Ω–æ"),
                    ("–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ", "–æ–ø—Ä–æ–≤–µ—Ä–≥–Ω—É—Ç–æ")
                ]
                
                for pos, neg in opposites:
                    if pos in claim1 and neg in claim2:
                        contradictions.append(
                            f"–ü—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω—ã–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è: ¬´{all_claims[i]['text']}¬ª vs ¬´{all_claims[j]['text']}¬ª"
                        )
                        break
        
        return contradictions

# ==================== TELEGRAM BOT (—Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ—Ç–ø—Ä–∞–≤–∫–æ–π) ====================
class TelegramBot:
    def __init__(self):
        if not TELEGRAM_TOKEN:
            raise ValueError("TELEGRAM_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        self.token = TELEGRAM_TOKEN
        self.bot_url = f"https://api.telegram.org/bot{self.token}"
        self.generator = IntelligentConspectGenerator()
        
        if RENDER_EXTERNAL_URL:
            self._setup_webhook()
        
        logger.info("‚úÖ Telegram –±–æ—Ç —Å –∞–Ω–∞–ª–∏–∑–æ–º –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –≥–æ—Ç–æ–≤")
    
    def _setup_webhook(self):
        webhook_url = f"{RENDER_EXTERNAL_URL}/webhook"
        try:
            response = requests.post(
                f"{self.bot_url}/setWebhook",
                json={"url": webhook_url},
                timeout=10
            )
            if response.json().get("ok"):
                logger.info(f"‚úÖ –í–µ–±—Ö—É–∫ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {webhook_url}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–µ–±—Ö—É–∫–∞: {e}")
    
    def process_message(self, chat_id, text):
        text = text.strip()
        self._update_stats(chat_id)
        
        if text.startswith("/"):
            if text == "/start":
                return self._send_welcome(chat_id)
            elif text == "/help":
                return self._send_help(chat_id)
            elif text == "/stats":
                return self._send_stats(chat_id)
            elif text == "/reliability":
                return self._send_reliability_info(chat_id)
            else:
                return self._send_message(chat_id, "‚ùì –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /help")
        
        if text in ["1", "2", "3"]:
            return self._handle_volume(chat_id, text)
        
        return self._handle_topic(chat_id, text)
    
    def _send_welcome(self, chat_id):
        welcome = (
            "üëã *–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ –£–º–Ω—ã–π Konspekt Helper Bot!*\n\n"
            "üß† *–¢–µ–ø–µ—Ä—å —Å –∞–Ω–∞–ª–∏–∑–æ–º –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –∏ —Å–≤—è–∑—ã–≤–∞–Ω–∏–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏!*\n\n"
            "üîÑ *–ù–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:*\n"
            "‚Ä¢ üî¨ –ê–Ω–∞–ª–∏–∑ –Ω–∞—É—á–Ω–æ—Å—Ç–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤\n"
            "‚Ä¢ üéØ –û—Ü–µ–Ω–∫–∞ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏\n"
            "‚Ä¢ üîó –°–≤—è–∑—ã–≤–∞–Ω–∏–µ —Ä–∞–∑—Ä–æ–∑–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n"
            "‚Ä¢ ‚ö†Ô∏è –í—ã—è–≤–ª–µ–Ω–∏–µ –±—Ä–µ–¥–∞ –∏ –Ω–µ–ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n"
            "‚Ä¢ üìä –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤\n\n"
            "üìö *–û–±—ä–µ–º—ã –∫–æ–Ω—Å–ø–µ–∫—Ç–æ–≤:*\n"
            "‚Ä¢ *1* ‚Äî –ö—Ä–∞—Ç–∫–∏–π (—Å –æ—Ü–µ–Ω–∫–æ–π –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏)\n"
            "‚Ä¢ *2* ‚Äî –ü–æ–¥—Ä–æ–±–Ω—ã–π (—Å –∞–Ω–∞–ª–∏–∑–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤)\n"
            "‚Ä¢ *3* ‚Äî –ü–æ–ª–Ω—ã–π (—Å–≤—è–∑—É—é—â–µ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ)\n\n"
            "üéØ *–û—Ç–ø—Ä–∞–≤—å—Ç–µ —Ç–µ–º—É –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞!*"
        )
        return self._send_message(chat_id, welcome)
    
    def _send_help(self, chat_id):
        help_text = (
            "üìö *–°–ü–†–ê–í–ö–ê –ü–û –£–ú–ù–û–ú–£ –ë–û–¢–£*\n\n"
            "*–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –∞–Ω–∞–ª–∏–∑ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏:*\n"
            "1. üîç –ü–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤\n"
            "2. üéØ –û—Ü–µ–Ω–∫–∞ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞\n"
            "3. üî¨ –ê–Ω–∞–ª–∏–∑ –Ω–∞—É—á–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞\n"
            "4. üîó –°–≤—è–∑—ã–≤–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤\n"
            "5. ‚ö†Ô∏è –í—ã—è–≤–ª–µ–Ω–∏–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π –∏ –±—Ä–µ–¥–∞\n\n"
            "*–û—Ü–µ–Ω–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:*\n"
            "üü¢ –ù–∞—É—á–Ω—ã–µ/–∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ (–≤—ã—Å–æ–∫–∞—è –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å)\n"
            "üü° –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ/–Ω–æ–≤–æ—Å—Ç–Ω—ã–µ (—Å—Ä–µ–¥–Ω—è—è –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å)\n"
            "üü† –ë–ª–æ–≥–∏/—Ñ–æ—Ä—É–º—ã (–Ω–∏–∑–∫–∞—è –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å)\n"
            "üî¥ –°–æ–º–Ω–∏—Ç–µ–ª—å–Ω—ã–µ (—Ç—Ä–µ–±—É–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏)\n\n"
            "*–ö–æ–º–∞–Ω–¥—ã:*\n"
            "/start - –ù–∞—á–∞–ª–æ —Ä–∞–±–æ—Ç—ã\n"
            "/help - –≠—Ç–∞ —Å–ø—Ä–∞–≤–∫–∞\n"
            "/stats - –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n"
            "/reliability - –ö–∞–∫ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å\n\n"
            "*–ü—Ä–∏–º–µ—Ä —Ö–æ—Ä–æ—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞:*\n"
            "¬´–†–∞–∑–≤–∏—Ç–∏–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –≤ –º–µ–¥–∏—Ü–∏–Ω–µ¬ª"
        )
        return self._send_message(chat_id, help_text)
    
    def _send_reliability_info(self, chat_id):
        info = (
            "üî¨ *–ö–ê–ö –û–¶–ï–ù–ò–í–ê–ï–¢–°–Ø –î–û–°–¢–û–í–ï–†–ù–û–°–¢–¨*\n\n"
            "*1. –û–¶–ï–ù–ö–ê –ò–°–¢–û–ß–ù–ò–ö–û–í:*\n"
            "üü¢ **–ù–∞—É—á–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏** (0.8-1.0):\n"
            "‚Ä¢ arxiv.org, nature.com, science.org\n"
            "‚Ä¢ –†–µ—Ü–µ–Ω–∑–∏—Ä—É–µ–º—ã–µ –∂—É—Ä–Ω–∞–ª—ã\n"
            "‚Ä¢ –ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏\n\n"
            "üü° **–û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏** (0.6-0.8):\n"
            "‚Ä¢ –ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–∞–π—Ç—ã (.gov)\n"
            "‚Ä¢ –û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —É—á—Ä–µ–∂–¥–µ–Ω–∏—è (.edu)\n"
            "‚Ä¢ –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏\n\n"
            "üü† **–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏** (0.4-0.6):\n"
            "‚Ä¢ –ù–æ–≤–æ—Å—Ç–Ω—ã–µ –ø–æ—Ä—Ç–∞–ª—ã\n"
            "‚Ä¢ –ë–ª–æ–≥–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤\n"
            "‚Ä¢ –ü–æ–ø—É–ª—è—Ä–Ω–∞—è –Ω–∞—É–∫–∞\n\n"
            "üî¥ **–ù–µ–Ω–∞–¥–µ–∂–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏** (0.0-0.4):\n"
            "‚Ä¢ –°–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏\n"
            "‚Ä¢ –§–æ—Ä—É–º—ã –∏ —á–∞—Ç—ã\n"
            "‚Ä¢ –ù–µ–ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –±–ª–æ–≥–∏\n\n"
            "*2. –û–¶–ï–ù–ö–ê –ö–û–ù–¢–ï–ù–¢–ê:*\n"
            "‚Ä¢ –ù–∞–ª–∏—á–∏–µ –Ω–∞—É—á–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏\n"
            "‚Ä¢ –û–±—ä–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å (–æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤)\n"
            "‚Ä¢ –ù–∞–ª–∏—á–∏–µ —Ü–∏—Ñ—Ä, –¥–∞–Ω–Ω—ã—Ö, —Ü–∏—Ç–∞—Ç\n"
            "‚Ä¢ –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏\n"
            "‚Ä¢ –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫\n\n"
            "*3. –í–´–Ø–í–õ–ï–ù–ò–ï –ë–†–ï–î–ê:*\n"
            "‚Ä¢ –ö–æ–Ω—Å–ø–∏—Ä–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ç–µ–æ—Ä–∏–∏\n"
            "‚Ä¢ –ù–µ–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω—ã–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è\n"
            "‚Ä¢ –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –æ–∫—Ä–∞—à–µ–Ω–Ω—ã–π —è–∑—ã–∫\n"
            "‚Ä¢ –°—É–±—ä–µ–∫—Ç–∏–≤–Ω—ã–µ –º–Ω–µ–Ω–∏—è –±–µ–∑ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤\n"
            "‚Ä¢ –°–ª–µ–Ω–≥ –∏ –Ω–µ—Ñ–æ—Ä–º–∞–ª—å–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è\n\n"
            "‚ö†Ô∏è *–ë–æ—Ç –ø—Ä–µ–¥—É–ø—Ä–µ–¥–∏—Ç, –µ—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Ç—Ä–µ–±—É–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏!*"
        )
        return self._send_message(chat_id, info)
    
    def _send_stats(self, chat_id):
        stat_text = (
            f"üìä *–°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–ò–°–¢–ï–ú–´ –ê–ù–ê–õ–ò–ó–ê*\n\n"
            f"üë• –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {stats['total_users']}\n"
            f"üí¨ –°–æ–æ–±—â–µ–Ω–∏–π: {stats['total_messages']}\n"
            f"üìÑ –ö–æ–Ω—Å–ø–µ–∫—Ç–æ–≤: {stats['conspects_created']}\n"
            f"üîç –ü–æ–∏—Å–∫–æ–≤ Google: {stats['google_searches']}\n"
            f"‚è± –ó–∞–ø—É—â–µ–Ω–∞: {stats['start_time'][:10]}\n\n"
            f"üéØ –†–µ–∂–∏–º: –ê–Ω–∞–ª–∏–∑ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"
        )
        return self._send_message(chat_id, stat_text)
    
    def _handle_topic(self, chat_id, topic):
        user_id = str(chat_id)
        if user_id not in stats["user_states"]:
            stats["user_states"][user_id] = {}
        
        stats["user_states"][user_id]["pending_topic"] = topic
        
        response = (
            f"üéØ *–¢–ï–ú–ê: {topic}*\n\n"
            f"üß† *–ù–ê–ß–ò–ù–ê–Æ –ò–ù–¢–ï–õ–õ–ï–ö–¢–£–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó...*\n\n"
            f"üîÑ *–ë–£–î–ï–¢ –ü–†–û–í–ï–î–ï–ù–û:*\n"
            f"1. üîç –ü–æ–∏—Å–∫ –≤ –Ω–∞—É—á–Ω—ã—Ö –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö\n"
            f"2. üéØ –û—Ü–µ–Ω–∫–∞ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞\n"
            f"3. üî¨ –ê–Ω–∞–ª–∏–∑ –Ω–∞—É—á–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞\n"
            f"4. üîó –°–≤—è–∑—ã–≤–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤\n"
            f"5. ‚ö†Ô∏è –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±—Ä–µ–¥ –∏ –Ω–µ–ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n\n"
            f"üìä *–í–´–ë–ï–†–ò–¢–ï –£–†–û–í–ï–ù–¨ –ê–ù–ê–õ–ò–ó–ê:*\n\n"
            f"1Ô∏è‚É£ *–ö–†–ê–¢–ö–ò–ô* ‚Äî –æ—Å–Ω–æ–≤–Ω—ã–µ –≤—ã–≤–æ–¥—ã —Å –æ—Ü–µ–Ω–∫–æ–π –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏\n"
            f"2Ô∏è‚É£ *–ü–û–î–†–û–ë–ù–´–ô* ‚Äî –∞–Ω–∞–ª–∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏ –∏—Ö –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏\n"
            f"3Ô∏è‚É£ *–ü–û–õ–ù–´–ô* ‚Äî —Å–≤—è–∑—É—é—â–µ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–æ–π\n\n"
            f"üî¢ *–û—Ç–ø—Ä–∞–≤—å—Ç–µ 1, 2 –∏–ª–∏ 3*"
        )
        return self._send_message(chat_id, response)
    
    def _handle_volume(self, chat_id, volume_choice):
        user_id = str(chat_id)
        user_state = stats["user_states"].get(user_id, {})
        topic = user_state.get("pending_topic", "")
        
        if not topic:
            return self._send_message(chat_id, "‚ùå –°–Ω–∞—á–∞–ª–∞ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ —Ç–µ–º—É –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        
        volume_map = {"1": "short", "2": "detailed", "3": "extended"}
        volume = volume_map.get(volume_choice, "short")
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ
        self._send_message(
            chat_id,
            f"üß† *–í–´–ü–û–õ–ù–Ø–Æ –ò–ù–¢–ï–õ–õ–ï–ö–¢–£–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó...*\n\n"
            f"üìå –¢–µ–º–∞: {topic}\n"
            f"üìä –£—Ä–æ–≤–µ–Ω—å –∞–Ω–∞–ª–∏–∑–∞: {volume_choice}/3\n"
            f"üîç –†–µ–∂–∏–º: –ê–Ω–∞–ª–∏–∑ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –∏ —Å–≤—è–∑—ã–≤–∞–Ω–∏–µ\n\n"
            f"‚è≥ –≠—Ç–æ –∑–∞–π–º–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–µ–∫—É–Ω–¥...\n"
            f"–°–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏ —Å–≤—è–∑—ã–≤–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é..."
        )
        
        try:
            conspect = self.generator.generate(topic, volume)
            stats["conspects_created"] += 1
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∫–æ–Ω—Å–ø–µ–∫—Ç —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–º —Ä–∞–∑–±–∏–µ–Ω–∏–µ–º
            self._send_intelligent_conspect(chat_id, conspect, volume_choice)
            
            # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
            final_msg = (
                f"‚úÖ *–ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù!*\n\n"
                f"üìå –¢–µ–º–∞: {topic}\n"
                f"üìä –£—Ä–æ–≤–µ–Ω—å –∞–Ω–∞–ª–∏–∑–∞: {volume_choice}/3\n"
                f"üîç –ü–æ–∏—Å–∫–æ–≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ: {stats['google_searches']}\n"
                f"üìÑ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –∫–æ–Ω—Å–ø–µ–∫—Ç–æ–≤: {stats['conspects_created']}\n\n"
                f"‚ö†Ô∏è *–ü–û–ú–ù–ò–¢–ï:* –í—Å–µ–≥–¥–∞ –ø—Ä–æ–≤–µ—Ä—è–π—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º!\n\n"
                f"üîÑ –î—Ä—É–≥–æ–π —É—Ä–æ–≤–µ–Ω—å –∞–Ω–∞–ª–∏–∑–∞? –û—Ç–ø—Ä–∞–≤—å—Ç–µ 1, 2 –∏–ª–∏ 3\n"
                f"üéØ –ù–æ–≤–∞—è —Ç–µ–º–∞? –ü—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—à–∏—Ç–µ –µ—ë!"
            )
            return self._send_message(chat_id, final_msg)
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–Ω—Å–ø–µ–∫—Ç–∞: {e}")
            return self._send_message(
                chat_id,
                f"‚ùå *–û–®–ò–ë–ö–ê –ê–ù–ê–õ–ò–ó–ê*\n\n"
                f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–º—É. –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:\n"
                f"‚Ä¢ –°–ª–∏—à–∫–æ–º –æ–±—â–∏–π –∏–ª–∏ –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω—ã–π –∑–∞–ø—Ä–æ—Å\n"
                f"‚Ä¢ –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º\n"
                f"‚Ä¢ –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤\n\n"
                f"–ü–æ–ø—Ä–æ–±—É–π—Ç–µ:\n"
                f"1. –ö–æ–Ω–∫—Ä–µ—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–º—É\n"
                f"2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞—É—á–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã\n"
                f"3. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–æ–∑–∂–µ\n\n"
                f"–û—à–∏–±–∫–∞: {str(e)[:100]}"
            )
    
    def _send_intelligent_conspect(self, chat_id, conspect, volume_choice):
        """–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞ –∫–æ–Ω—Å–ø–µ–∫—Ç–∞"""
        max_length = 3900  # –° –∑–∞–ø–∞—Å–æ–º –¥–ª—è Telegram
        
        if len(conspect) <= max_length:
            self._send_message(chat_id, conspect)
            return
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ª–æ–≥–∏—á–µ—Å–∫–∏–º —Ä–∞–∑–¥–µ–ª–∞–º
        sections = []
        
        # –ò—â–µ–º –∫—Ä—É–ø–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã (—Å === –∏–ª–∏ ‚îÅ‚îÅ‚îÅ‚îÅ)
        big_sections = re.split(r'(=+\n[^=]+\n=+|\n‚îÅ‚îÅ[‚îÅ]+\n)', conspect)
        
        current_section = ""
        for part in big_sections:
            if not part.strip():
                continue
            
            # –ï—Å–ª–∏ —ç—Ç–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ä–∞–∑–¥–µ–ª–∞
            if re.match(r'(=+\n[^=]+\n=+|\n‚îÅ‚îÅ[‚îÅ]+\n)', part):
                if current_section and len(current_section) > 1000:
                    sections.append(current_section.strip())
                    current_section = part + "\n\n"
                else:
                    current_section += part + "\n\n"
            else:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É
                if len(current_section + part) > max_length and current_section:
                    sections.append(current_section.strip())
                    current_section = part
                else:
                    current_section += part
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–∑–¥–µ–ª
        if current_section.strip():
            sections.append(current_section.strip())
        
        # –ï—Å–ª–∏ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å, —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∞–±–∑–∞—Ü–∞–º
        if not sections or (len(sections) == 1 and len(sections[0]) > max_length):
            paragraphs = conspect.split('\n\n')
            sections = []
            current = ""
            
            for para in paragraphs:
                if len(current + para) > max_length and current:
                    sections.append(current.strip())
                    current = para
                else:
                    if current:
                        current += "\n\n" + para
                    else:
                        current = para
            
            if current.strip():
                sections.append(current.strip())
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤—Å–µ —á–∞—Å—Ç–∏
        total_parts = len(sections)
        for i, section in enumerate(sections, 1):
            # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è (–∫—Ä–æ–º–µ –ø–µ—Ä–≤–æ–π —á–∞—Å—Ç–∏)
            if i > 1:
                header = f"üìñ *–ü–†–û–î–û–õ–ñ–ï–ù–ò–ï –ê–ù–ê–õ–ò–ó–ê ({i}/{total_parts})*\n\n"
                section = header + section
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É –µ—â–µ —Ä–∞–∑
            if len(section) > max_length:
                # –≠–∫—Å—Ç—Ä–µ–Ω–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
                sentences = re.split(r'[.!?]+', section)
                current_chunk = ""
                
                for sentence in sentences:
                    sentence = sentence.strip()
                    if not sentence:
                        continue
                    
                    if len(current_chunk + sentence) > max_length and current_chunk:
                        self._send_message(chat_id, current_chunk)
                        current_chunk = sentence + ". "
                        time.sleep(0.5)
                    else:
                        current_chunk += sentence + ". "
                
                if current_chunk.strip():
                    self._send_message(chat_id, current_chunk.strip())
            else:
                self._send_message(chat_id, section)
            
            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —á–∞—Å—Ç—è–º–∏
            if i < total_parts:
                time.sleep(0.7)
    
    def _update_stats(self, chat_id):
        user_id = str(chat_id)
        
        if user_id not in stats["user_states"]:
            stats["total_users"] += 1
            stats["user_states"][user_id] = {
                "first_seen": datetime.now().isoformat(),
                "message_count": 0
            }
        
        stats["user_states"][user_id]["last_seen"] = datetime.now().isoformat()
        stats["user_states"][user_id]["message_count"] += 1
        stats["total_messages"] += 1
    
    def _send_message(self, chat_id, text):
        try:
            response = requests.post(
                f"{self.bot_url}/sendMessage",
                json={
                    "chat_id": chat_id,
                    "text": text,
                    "parse_mode": "Markdown",
                    "disable_web_page_preview": True
                },
                timeout=15
            )
            return response.json().get("ok", False)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏—è: {e}")
            return False

# ==================== HTTP –°–ï–†–í–ï–† ====================
class BotHTTPServer(BaseHTTPRequestHandler):
    def do_GET(self):
        path = self.path.split('?')[0]
        
        if path == "/":
            self._send_html(INDEX_HTML)
        elif path == "/health":
            self._send_json({"status": "ok", "mode": "reliability_analysis", "time": datetime.now().isoformat()})
        elif path == "/stats":
            self._send_json(stats)
        else:
            self.send_response(404)
            self.end_headers()
    
    def do_POST(self):
        if self.path == "/webhook":
            content_length = int(self.headers.get('Content-Length', 0))
            if content_length:
                try:
                    data = self.rfile.read(content_length)
                    update = json.loads(data.decode('utf-8'))
                    
                    threading.Thread(
                        target=self._handle_update,
                        args=(update,),
                        daemon=True
                    ).start()
                    
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–µ–±—Ö—É–∫–∞: {e}")
            
            self.send_response(200)
            self.end_headers()
            self.wfile.write(b'OK')
        else:
            self.send_response(404)
            self.end_headers()
    
    def _handle_update(self, update):
        try:
            if "message" in update and "text" in update["message"]:
                chat_id = update["message"]["chat"]["id"]
                text = update["message"]["text"]
                
                bot = TelegramBot()
                bot.process_message(chat_id, text)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
    
    def _send_html(self, content):
        self.send_response(200)
        self.send_header('Content-Type', 'text/html; charset=utf-8')
        self.end_headers()
        self.wfile.write(content.encode('utf-8'))
    
    def _send_json(self, data):
        self.send_response(200)
        self.send_header('Content-Type', 'application/json; charset=utf-8')
        self.end_headers()
        self.wfile.write(json.dumps(data, ensure_ascii=False, indent=2).encode('utf-8'))
    
    def log_message(self, format, *args):
        pass

INDEX_HTML = """<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>ü§ñ –£–º–Ω—ã–π Konspekt Helper Bot</title>
    <style>
        body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 0; padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); min-height: 100vh; }
        .container { max-width: 900px; margin: 0 auto; background: white; padding: 40px; border-radius: 20px; box-shadow: 0 20px 60px rgba(0,0,0,0.3); }
        .header { text-align: center; margin-bottom: 40px; }
        .reliability-badges { display: flex; justify-content: center; gap: 20px; margin: 30px 0; flex-wrap: wrap; }
        .badge { padding: 15px; border-radius: 10px; color: white; font-weight: bold; min-width: 200px; text-align: center; }
        .badge-scientific { background: linear-gradient(to right, #10b981, #059669); }
        .badge-official { background: linear-gradient(to right, #f59e0b, #d97706); }
        .badge-blog { background: linear-gradient(to right, #f97316, #ea580c); }
        .badge-dubious { background: linear-gradient(to right, #ef4444, #dc2626); }
        .stats-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; margin: 30px 0; }
        .stat-card { background: #f8fafc; padding: 20px; border-radius: 10px; text-align: center; border-left: 5px solid #667eea; }
        .stat-value { font-size: 2em; font-weight: bold; color: #667eea; }
        .btn { display: inline-block; background: linear-gradient(to right, #667eea, #764ba2); color: white; padding: 12px 25px; border-radius: 8px; text-decoration: none; font-weight: bold; margin: 10px 5px; transition: transform 0.2s; }
        .btn:hover { transform: translateY(-2px); }
        .feature-list { background: #f0f9ff; padding: 25px; border-radius: 15px; margin: 30px 0; border-left: 5px solid #3b82f6; }
        .feature-item { margin: 10px 0; padding-left: 20px; position: relative; }
        .feature-item:before { content: "‚úì"; position: absolute; left: 0; color: #10b981; font-weight: bold; }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ü§ñ –£–º–Ω—ã–π Konspekt Helper Bot</h1>
            <p style="color: #666; font-size: 1.2em;">–ê–Ω–∞–ª–∏–∑ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ + –°–≤—è–∑—ã–≤–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ + –í—ã—è–≤–ª–µ–Ω–∏–µ –±—Ä–µ–¥–∞</p>
        </div>
        
        <div class="feature-list">
            <h3>üéØ –ö–ª—é—á–µ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã:</h3>
            <div class="feature-item">üî¨ <strong>–ê–Ω–∞–ª–∏–∑ –Ω–∞—É—á–Ω–æ—Å—Ç–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤</strong> - –æ—Ü–µ–Ω–∫–∞ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞</div>
            <div class="feature-item">üéØ <strong>–û—Ü–µ–Ω–∫–∞ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞</strong> - –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–∫—Ç–æ–≤ –∏ –¥–∞–Ω–Ω—ã—Ö</div>
            <div class="feature-item">üîó <strong>–°–≤—è–∑—ã–≤–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏</strong> - –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤</div>
            <div class="feature-item">‚ö†Ô∏è <strong>–í—ã—è–≤–ª–µ–Ω–∏–µ –±—Ä–µ–¥–∞</strong> - —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–µ–ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –∏ –Ω–µ–Ω–∞—É—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö</div>
            <div class="feature-item">üìä <strong>–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑</strong> - –ø–æ–∏—Å–∫ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏</div>
            <div class="feature-item">üß† <strong>–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞</strong> - —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø—Ä–æ–≤–µ—Ä–∫–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏</div>
        </div>
        
        <h3>üé® –°–∏—Å—Ç–µ–º–∞ –æ—Ü–µ–Ω–∫–∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏:</h3>
        <div class="reliability-badges">
            <div class="badge badge-scientific">
                üü¢ –ù–ê–£–ß–ù–´–ô<br>0.8-1.0
            </div>
            <div class="badge badge-official">
                üü° –û–§–ò–¶–ò–ê–õ–¨–ù–´–ô<br>0.6-0.8
            </div>
            <div class="badge badge-blog">
                üü† –ò–ù–§–û–†–ú–ê–¶–ò–û–ù–ù–´–ô<br>0.4-0.6
            </div>
            <div class="badge badge-dubious">
                üî¥ –°–û–ú–ù–ò–¢–ï–õ–¨–ù–´–ô<br>0.0-0.4
            </div>
        </div>
        
        <div class="stats-grid">
            <div class="stat-card">
                <div class="stat-value" id="users">0</div>
                <div>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π</div>
            </div>
            <div class="stat-card">
                <div class="stat-value" id="messages">0</div>
                <div>–°–æ–æ–±—â–µ–Ω–∏–π</div>
            </div>
            <div class="stat-card">
                <div class="stat-value" id="conspects">0</div>
                <div>–ö–æ–Ω—Å–ø–µ–∫—Ç–æ–≤</div>
            </div>
            <div class="stat-card">
                <div class="stat-value" id="searches">0</div>
                <div>–ü–æ–∏—Å–∫–æ–≤</div>
            </div>
        </div>
        
        <h3>üîó –ë—ã—Å—Ç—Ä—ã–µ —Å—Å—ã–ª–∫–∏:</h3>
        <div style="text-align: center;">
            <a href="https://t.me/Konspekt_help_bot" class="btn" target="_blank">ü§ñ –û—Ç–∫—Ä—ã—Ç—å –±–æ—Ç–∞ –≤ Telegram</a>
            <a href="/stats" class="btn">üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã</a>
            <a href="/health" class="btn">‚ù§Ô∏è –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è</a>
        </div>
        
        <h3>üéØ –ö–∞–∫ –ø–æ–ª—É—á–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑:</h3>
        <ol>
            <li>–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ, –Ω–∞—É—á–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã</li>
            <li>–ò–∑–±–µ–≥–∞–π—Ç–µ –æ–±—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫</li>
            <li>–£–∫–∞–∑—ã–≤–∞–π—Ç–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –∏ –ø–æ–Ω—è—Ç–∏—è</li>
            <li>–ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ –æ—Ü–µ–Ω–∫—É –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö</li>
            <li>–°—Ä–∞–≤–Ω–∏–≤–∞–π—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤</li>
        </ol>
        
        <div style="background: #fef3c7; padding: 15px; border-radius: 10px; margin-top: 30px; border-left: 5px solid #f59e0b;">
            <strong>‚ö†Ô∏è –í–∞–∂–Ω–æ–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ:</strong> –°–∏—Å—Ç–µ–º–∞ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –Ω–æ –Ω–µ –∑–∞–º–µ–Ω—è–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –º—ã—à–ª–µ–Ω–∏–µ. –í—Å–µ–≥–¥–∞ –ø—Ä–æ–≤–µ—Ä—è–π—Ç–µ –≤–∞–∂–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º.
        </div>
        
        <p style="text-align: center; color: #666; margin-top: 40px; font-size: 0.9em;">
            –°–∏—Å—Ç–µ–º–∞ –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏. –¢–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è: <span id="time"></span>
        </p>
    </div>
    
    <script>
        async function loadStats() {
            try {
                const response = await fetch('/stats');
                const data = await response.json();
                
                document.getElementById('users').textContent = data.total_users || 0;
                document.getElementById('messages').textContent = data.total_messages || 0;
                document.getElementById('conspects').textContent = data.conspects_created || 0;
                document.getElementById('searches').textContent = data.google_searches || 0;
                
                document.getElementById('time').textContent = new Date().toLocaleTimeString();
            } catch (error) {
                console.log('–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏:', error);
            }
        }
        
        loadStats();
        setInterval(loadStats, 3000);
    </script>
</body>
</html>
"""

# ==================== –ó–ê–ü–£–°–ö ====================
def main():
    logger.info("=" * 70)
    logger.info("üöÄ –ó–ê–ü–£–°–ö –£–ú–ù–û–ì–û KONSPEKT BOT –° –ê–ù–ê–õ–ò–ó–û–ú –î–û–°–¢–û–í–ï–†–ù–û–°–¢–ò")
    logger.info("=" * 70)
    logger.info(f"üåê –í–Ω–µ—à–Ω–∏–π URL: {RENDER_EXTERNAL_URL}")
    logger.info(f"üö™ –ü–æ—Ä—Ç: {PORT}")
    logger.info("‚úÖ –†–µ–∂–∏–º: –ê–Ω–∞–ª–∏–∑ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ + –°–≤—è–∑—ã–≤–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏")
    logger.info("‚úÖ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: –ù–∞—É—á–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ vs –ë—Ä–µ–¥")
    logger.info("‚úÖ –û–±—ä–µ–º—ã: –í—Å–µ —Ç—Ä–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
    logger.info("=" * 70)
    
    server = HTTPServer(('', PORT), BotHTTPServer)
    logger.info(f"‚úÖ HTTP —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω –Ω–∞ –ø–æ—Ä—Ç—É {PORT}")
    
    try:
        server.serve_forever()
    except KeyboardInterrupt:
        logger.info("‚èπÔ∏è –°–µ—Ä–≤–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    main()
